# Apprentice: Knowledge Distillation with Low-Precision Networks

[Paper](https://arxiv.org/pdf/1711.05852.pdf)

## Abstract

Low-precision numerics and model compression using knowledge distillation are popular techniques to lower both the compute requirements and memory footprint of these deployed model