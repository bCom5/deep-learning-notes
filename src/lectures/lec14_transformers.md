# Transformers and Pre-Training

## Review

![](https://i.imgur.com/BVdVEoi.png)

* Up and Down Attention

![](https://i.imgur.com/gGG4J6N.png)

* Transformers (Convolution of NLP)
    * Self Attention
    * Encoding/Decoding

## Transformers

![](https://i.imgur.com/mYSymyv.png)

![](https://i.imgur.com/5jvchbe.png)

* __Multi-Headed Attention__
    `h` block of scaled dot-product attention. Tensor V, K, Q

![](https://i.imgur.com/aqMQVVh.png)

* What is Encoder/Decoder?

### Transformer Encoder

![](https://i.imgur.com/Bd6mOCq.png)

* Has residuals of based inputs
* What is the Positional Encoding

![](https://i.imgur.com/FmYWjJH.png)

![](https://i.imgur.com/0dTtfWA.png)

* Why does it mix?
* From words and has positional encoding

### Multi-Headed Attention

![](https://i.imgur.com/LgGrwEe.png)

* Attention strength (sum) from different words

![](https://i.imgur.com/G490xoJ.png)

* Self Attention (?)
* What are the colors of the lines and on the words (?)

### Transformer Decoding

![](https://i.imgur.com/J7O9jOG.png)

* Decoding is to generate text
* Attention vs Self-Attention (?)
* What does the direction of the arrow mean?