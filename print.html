<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js ayu">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>CS 282: Deep Learning Notes Spring 2020</title>
        
        <meta name="robots" content="noindex" />
        

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">
        <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:500" rel="stylesheet" type="text/css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "ayu" : "ayu";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('ayu')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div id="sidebar-scrollbox" class="sidebar-scrollbox">
                <ol class="chapter"><li class="expanded "><a href="introduction.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="expanded "><a href="lectures/lectures.html"><strong aria-hidden="true">2.</strong> Lectures</a></li><li><ol class="section"><li class="expanded "><a href="lectures/lec09_rnn.html"><strong aria-hidden="true">2.1.</strong> Recurrent Networks, LSTMS, and Applications</a></li><li class="expanded "><a href="lectures/lec10_visual.html"><strong aria-hidden="true">2.2.</strong> Visualizing Deep Networks</a></li><li class="expanded "><a href="lectures/lec11_attention.html"><strong aria-hidden="true">2.3.</strong> Attention Networks</a></li><li class="expanded "><a href="lectures/lec12_text_semantics.html"><strong aria-hidden="true">2.4.</strong> Text Semantics</a></li></ol></li><li class="expanded "><a href="homework/homework.html"><strong aria-hidden="true">3.</strong> Homeworks</a></li><li><ol class="section"><li class="expanded "><a href="homework/hw2_lstm.html"><strong aria-hidden="true">3.1.</strong> Homework 2 LSTM</a></li></ol></li><li class="expanded "><a href="exam/exam.html"><strong aria-hidden="true">4.</strong> Exam Practice</a></li><li><ol class="section"><li class="expanded "><a href="exam/midterm1/midterm1.html"><strong aria-hidden="true">4.1.</strong> Midterm 1</a></li><li><ol class="section"><li class="expanded "><a href="exam/midterm1/mt1sp19.html"><strong aria-hidden="true">4.1.1.</strong> Midterm 1 Spring 2019</a></li><li class="expanded "><a href="exam/midterm1/mt1prac1sp18.html"><strong aria-hidden="true">4.1.2.</strong> Midterm 1 Practice 1 Spring 2018</a></li><li class="expanded "><a href="exam/midterm1/cheatsheet.html"><strong aria-hidden="true">4.1.3.</strong> Midterm 1 Cheat Sheet</a></li></ol></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar" class="menu-bar">
                    <div id="menu-bar-sticky-container">
                        <div class="left-buttons">
                            <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                                <i class="fa fa-bars"></i>
                            </button>
                            <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                                <i class="fa fa-paint-brush"></i>
                            </button>
                            <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                                <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu (default)</button></li>
                            </ul>
                            
                            <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                                <i class="fa fa-search"></i>
                            </button>
                            
                        </div>

                        <h1 class="menu-title">CS 282: Deep Learning Notes Spring 2020</h1>

                        <div class="right-buttons">
                            <a href="print.html" title="Print this book" aria-label="Print this book">
                                <i id="print-button" class="fa fa-print"></i>
                            </a>
                            
                        </div>
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1><a class="header" href="#introduction" id="introduction">Introduction</a></h1>
<p>CS 282 Deep Learning, Spring 2020 
Taught by John Canny</p>
<p><img src="https://i.imgur.com/0ztiyxz.png" alt="" /></p>
<ul>
<li><a href="https://bcourses.berkeley.edu/courses/1487769">Website</a></li>
<li><a href="https://www.youtube.com/playlist?list=PLkFD6_40KJIwaO6Eca8kzsEFBob0nFvwm">Webcast</a></li>
</ul>
<h1><a class="header" href="#lectures" id="lectures">Lectures</a></h1>
<h1><a class="header" href="#recurrent-networks-lstms-and-applications" id="recurrent-networks-lstms-and-applications">Recurrent Networks, LSTMS, and Applications</a></h1>
<h1><a class="header" href="#visualizing-deep-networks" id="visualizing-deep-networks">Visualizing Deep Networks</a></h1>
<h3><a class="header" href="#activation-maximization" id="activation-maximization">Activation Maximization</a></h3>
<p><img src="https://i.imgur.com/bKFfWQy.png" alt="" /></p>
<p>Generate a synthetic image I, normalize it L2 nrom, maxing a class. This is done through backpropogation. Class is one hot encoded <code>[0 0 1 0 0]</code>. Images are weird though.</p>
<h3><a class="header" href="#deconv-approaches" id="deconv-approaches">Deconv Approaches</a></h3>
<p><img src="https://i.imgur.com/L9W4AWf.png" alt="" /></p>
<p>To make generating synthetic images better:</p>
<ul>
<li>backprop zeros out negative values in the forward pass</li>
<li>decovnet zeros out negative gradients in the backward pass. Negative gradients are inhibitory activations (they were the wrong class)</li>
<li>guided backprop does both, zeros out both</li>
</ul>
<h3><a class="header" href="#neural-style-transfer" id="neural-style-transfer">Neural Style Transfer</a></h3>
<p><img src="https://i.imgur.com/xdURQhp.jpg" alt="" /></p>
<p>Review this!</p>
<h1><a class="header" href="#attention-networks" id="attention-networks">Attention Networks</a></h1>
<p><strong>The most important idea in deep networks this decade.</strong></p>
<h3><a class="header" href="#pilot-analogy" id="pilot-analogy">Pilot analogy</a></h3>
<p><img src="https://i.imgur.com/LU12T8h.jpg" alt="" /></p>
<p>Pilots move their focus on different things, gauges, buttons, switches</p>
<h3><a class="header" href="#papers" id="papers">Papers</a></h3>
<p>Attention is All You Need (2017) only stacked attention, previous used RNNs</p>
<h2><a class="header" href="#soft-vs-hard-attention-introduction" id="soft-vs-hard-attention-introduction">Soft vs Hard Attention Introduction</a></h2>
<p><img src="https://i.imgur.com/GwTuFpG.png" alt="" /></p>
<ul>
<li><strong>Hard Attention</strong> is what humans do. Not differentiable because attention map is 1 where focused, 0 elsewhere (discrete).</li>
<li><strong>Soft Attention</strong> - linear combination over inputs, can use backprop.
<ul>
<li>Trains network and attention to the right place!</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/3JN8VEn.png" alt="" />
<strong>Supervised Learning</strong> vs <strong>Reinforcement Learning</strong>, maximizing a reward (discrete)</p>
<p>Reinforcement Learning, the <strong>agent tries to maximize the sum of rewards over an epoch</strong></p>
<h2><a class="header" href="#attention-for-recognition" id="attention-for-recognition">Attention for Recognition</a></h2>
<p><img src="https://i.imgur.com/d2tfdX3.png" alt="" /></p>
<p>Has time stamps. Has location and the image. RNN always predicting gaze location for next time.</p>
<p><img src="https://i.imgur.com/BAmjvmm.png" alt="" /></p>
<p>Resolution when focusing on something (glimpse). Output is location and action (classify/don't classify). Inputs are on top, output is at the bottom.</p>
<p><img src="https://i.imgur.com/NnoPB1G.png" alt="" /></p>
<p>This shows glimpses. Focused in center. Blurred around. More blurred after, 3 levels. Green is location as it moves around.</p>
<h2><a class="header" href="#soft-attention-for-translation" id="soft-attention-for-translation">Soft Attention for Translation</a></h2>
<p><img src="https://i.imgur.com/ode4uKw.png" alt="" /></p>
<p>When the world is outputting <em>Me</em>, it has attention on <em>I</em>. Likewise with <em>coffee</em> and <em>cafe</em>.</p>
<p><img src="https://i.imgur.com/bWX5rXq.png" alt="" /></p>
<p>What word does each word correspond to? (Where is the attention located)?</p>
<h2><a class="header" href="#rnn-for-captioning" id="rnn-for-captioning">RNN for Captioning</a></h2>
<p><img src="https://i.imgur.com/O3QzfWP.png" alt="" /></p>
<p>Output <code>d0</code> goes into a softmax over the vocabulary. <em>Bird</em> is fed back into <code>x1</code>, next token. </p>
<p><img src="https://i.imgur.com/qA37EVl.png" alt="" /></p>
<p>Have \( L \times D \) features extracted. Our RNN outputs classification and weight location. Mask/summing (?) with the features give the weighted feature \( D \) fed back into the RNN.</p>
<h2><a class="header" href="#soft-vs-hard-attention-in-captioning" id="soft-vs-hard-attention-in-captioning">Soft vs. Hard Attention in Captioning</a></h2>
<p><img src="https://i.imgur.com/VNdA0yR.png" alt="" /></p>
<ul>
<li>Have two inputs, RNN gives probability distribution (see above) (also think about the result of softmax).</li>
<li><strong>Soft Attention</strong> sum all location and derivative is \( \frac{dz}{dp} \). Train with gradient descent.</li>
<li><strong>Hard Attention</strong> samples only one location. With argmax, \( \frac{dz}{dp} \) is almost zero everywhere. Can't use gradient descent.</li>
</ul>
<p><img src="https://i.imgur.com/RphCcoj.png" alt="" /></p>
<p>Soft is smooth around the area. Hard is discrete (1 or 0) around the image.</p>
<p><img src="https://i.imgur.com/wLcn23X.jpg" alt="" /></p>
<p><img src="https://i.imgur.com/YYT5G2u.jpg" alt="" /></p>
<p>Can see where it made mistakes!!!</p>
<h2><a class="header" href="#attention-mechanics" id="attention-mechanics">Attention Mechanics</a></h2>
<p><img src="https://i.imgur.com/0q6PVTx.png" alt="" /></p>
<ul>
<li>Image Feature output matrix \( L \times D \)</li>
<li>Attention weights output vector \( L \). Can \( L \) be an image \( H \times W \)?</li>
<li>Do a broadcast multiply - each vector in \( D \) multiplies by the Attention vector. Output is still \( L \times D \)</li>
<li>Sum the \( L \) axis (vector). Output is size \( D \).</li>
</ul>
<p><img src="https://i.imgur.com/NcPqXPf.png" alt="" /></p>
<ul>
<li>Need sum for the contributions. This is because \( B \) is broadcast multiplied to \( A \)</li>
</ul>
<h3><a class="header" href="#salience" id="salience">Salience</a></h3>
<p><img src="https://i.imgur.com/v4cwBwH.png" alt="" /></p>
<ul>
<li>Circle operation is product</li>
<li>What is salience (A *dot* )
<ul>
<li><strong>Salience:</strong> The total contribution and gradient to the output (the importance of an area)</li>
</ul>
</li>
</ul>
<h2><a class="header" href="#attention-and-lstm" id="attention-and-lstm">Attention and LSTM</a></h2>
<p><img src="https://i.imgur.com/EDcAoA5.png" alt="" /></p>
<ul>
<li>LSTMs also receive a <strong>salience gradient</strong>. There is multiplicative gating (kind of like attention)</li>
</ul>
<h2><a class="header" href="#soft-attention-for-video" id="soft-attention-for-video">Soft Attention for Video</a></h2>
<p><img src="https://i.imgur.com/wHL1iwj.png" alt="" /></p>
<p>Now temporal (time) can have attention. (Which frame is important?)</p>
<p><img src="https://i.imgur.com/lo2rgUy.png" alt="" /></p>
<ul>
<li>Can have spatial and temporal attention. Temporal in this network.</li>
<li>Create a vector of feature frames.</li>
</ul>
<p><img src="https://i.imgur.com/uvLw16A.png" alt="" /></p>
<p><img src="https://i.imgur.com/iNBaAPT.png" alt="" /></p>
<p>Perplexity is diversity of word options. Goal is smaller which means the model is more confident.</p>
<h2><a class="header" href="#find-the-region-again" id="find-the-region-again">Find the Region Again</a></h2>
<p><img src="https://i.imgur.com/gVzxWMc.png" alt="" /></p>
<ul>
<li>Classify - attention to regions in input</li>
<li>Generate - draw digit</li>
</ul>
<p>Has a parameter for scale</p>
<h2><a class="header" href="#takeaways" id="takeaways">Takeaways</a></h2>
<p><img src="https://i.imgur.com/vquu8LF.png" alt="" /></p>
<ul>
<li><strong>Salience:</strong> Emphasize imporant data.</li>
</ul>
<p><img src="https://i.imgur.com/ddEkm4X.png" alt="" /></p>
<h1><a class="header" href="#text-semantics" id="text-semantics">Text Semantics</a></h1>
<p><a href="https://youtu.be/1rzjaUp6NiQ">Webcast</a></p>
<p><img src="https://i.imgur.com/7LIXXrJ.png" alt="" /></p>
<ul>
<li><strong>Propositional</strong>: logic, formal, mathematical</li>
<li><strong>Vector</strong>: represent as numbers, high dimensional space.</li>
</ul>
<p><img src="https://i.imgur.com/br0ryjb.png" alt="" /></p>
<ul>
<li>Propositional uses class (usually noun) and predicates - operation (usually a verb). <strong>Reasoning</strong></li>
<li>Word and sentences represented as vectors. Commutative! That is a problem.</li>
</ul>
<h2><a class="header" href="#vector-embedding-of-words" id="vector-embedding-of-words">Vector Embedding of Words</a></h2>
<p><img src="https://i.imgur.com/CGwXg4v.png" alt="" /></p>
<ul>
<li><strong>Bag-of-Words</strong> - sentence</li>
</ul>
<h3><a class="header" href="#word-similarity" id="word-similarity">Word Similarity</a></h3>
<p><img src="https://i.imgur.com/Vd6TRq8.png" alt="" /></p>
<ul>
<li><strong>Similar word</strong> in <strong>similar contexts</strong>. Dog and canine can be replaced.</li>
</ul>
<p><img src="https://i.imgur.com/Nqc0KTP.png" alt="" /></p>
<h3><a class="header" href="#dimension-reduction" id="dimension-reduction">Dimension Reduction</a></h3>
<p><img src="https://i.imgur.com/L54iu21.png" alt="" /></p>
<ul>
<li>Could <strong>one-hot encode</strong> every word in vocab. That's expensive.</li>
<li>Alt: counts of word used in that context. &quot;Dog barks&quot;.</li>
</ul>
<h2><a class="header" href="#dimensionality-reduction" id="dimensionality-reduction">Dimensionality Reduction</a></h2>
<p><img src="https://i.imgur.com/sd6BKtO.png" alt="" /></p>
<h3><a class="header" href="#latent-semantic-analysis" id="latent-semantic-analysis">Latent Semantic Analysis</a></h3>
<p><img src="https://i.imgur.com/YogMF8j.png" alt="" /></p>
<ul>
<li>Encode documents <code>N</code></li>
<li><code>M</code> is word count.</li>
</ul>
<p><img src="https://i.imgur.com/hXnjAO7.png" alt="" /></p>
<p>Counts words</p>
<h3><a class="header" href="#latent-semantic-analysis-1" id="latent-semantic-analysis-1">Latent Semantic Analysis</a></h3>
<p><img src="https://i.imgur.com/QEJ4hZq.png" alt="" /></p>
<ul>
<li>Low dimensional approximation using SVD. (Review SVD!)</li>
<li>Embedding is V, factors encode document contexts</li>
</ul>
<h4><a class="header" href="#singular-value-decomposition" id="singular-value-decomposition">Singular Value Decomposition</a></h4>
<p><img src="https://i.imgur.com/8vySkwT.png" alt="" /></p>
<ul>
<li>\( U, V \) <strong>orthogonal, normal</strong>. \( S \) rectangular diagonal, singular values, right padded with 0s to fit</li>
</ul>
<h4><a class="header" href="#review-how-to-calculate-svd" id="review-how-to-calculate-svd">Review How to Calculate SVD</a></h4>
<p><img src="https://i.imgur.com/VHinLAL.png" alt="" /></p>
<ul>
<li>It can be computed from eigenvalues of \( T ~ T ^T \)</li>
<li>Insides cancel out, there's an \( S^2 \) term!!!</li>
</ul>
<p><img src="https://i.imgur.com/FB3xuv1.png" alt="" /></p>
<p>Both work!</p>
<p><img src="https://i.imgur.com/2ynYgmd.png" alt="" /></p>
<ul>
<li>Can have a smaller S, using only the largest singular values (S?) </li>
<li>Small K dimension</li>
<li>High dimensional T to low dimensional </li>
<li><strong>best possible reconstruction</strong> of documents from their embedding</li>
</ul>
<p><img src="https://i.imgur.com/mWKPVXo.png" alt="" /></p>
<ul>
<li>Documents are T, Encoding is Z</li>
</ul>
<p><img src="https://i.imgur.com/0IJVSaK.png" alt="" /></p>
<ul>
<li>Have a network learn SVD</li>
<li>V is vocab times latent dimensions</li>
<li>Scalable version of LSA/SVD</li>
<li>similar things will get mapped to similar places</li>
</ul>
<h2><a class="header" href="#t-sne-word-embeddings" id="t-sne-word-embeddings">t-SNE Word Embeddings</a></h2>
<p><img src="https://i.imgur.com/tF1VGAF.png" alt="" /></p>
<h2><a class="header" href="#word2vec" id="word2vec">Word2Vec</a></h2>
<p><img src="https://i.imgur.com/LmTf2z2.png" alt="" /></p>
<ul>
<li>neighborhood of a word instead of whole document, <strong>skip-gram</strong></li>
<li>nonlinearity</li>
</ul>
<p><img src="https://i.imgur.com/GLAWU4T.png" alt="" /></p>
<ul>
<li>Predict center words from context</li>
<li>order does not matter</li>
</ul>
<p><img src="https://i.imgur.com/ZrKymNE.png" alt="" /></p>
<ul>
<li>Predict context words from center word</li>
</ul>
<p><img src="https://i.imgur.com/9sTa45d.png" alt="" /></p>
<ul>
<li>Problem of SVD is it favors minimizing large distances (min squared error). Want to preserve <strong>close</strong> distance like t-SNE</li>
</ul>
<p><img src="https://i.imgur.com/E3M523Q.png" alt="" /></p>
<ul>
<li>Canny says it's a mess</li>
</ul>
<p><img src="https://i.imgur.com/29fFUrk.png" alt="" /></p>
<ul>
<li>Holds relations as vectors! Vector math!</li>
</ul>
<p><img src="https://i.imgur.com/0WFGMaA.png" alt="" /></p>
<ul>
<li>This model uses contexts</li>
<li>City pairs, opposites, comparatives (great, greater)</li>
</ul>
<p>Criticisms:</p>
<ul>
<li>Cross-entropy emphasizes small word combinations</li>
<li>Expensive to softmax</li>
</ul>
<p><img src="https://i.imgur.com/Er3Ogvq.png" alt="" /></p>
<ul>
<li>Maybe wrong data</li>
</ul>
<p><img src="https://i.imgur.com/CwVSFNj.png" alt="" /></p>
<ul>
<li>Now words times words! Better for contextual words!</li>
<li>Window size (prob exam problem), just try it and check</li>
</ul>
<h2><a class="header" href="#glove" id="glove">GloVe</a></h2>
<p><img src="https://i.imgur.com/OkSCRaK.png" alt="" /></p>
<ul>
<li>\( C_{ij} \) num times word j in context of word i. Query the matrix
<ul>
<li>For favoring the counts</li>
</ul>
</li>
<li>\( u_i \) is embedding, \( v_j \) is context word embedding
<ul>
<li>Inner product for similarity!!!</li>
</ul>
</li>
<li>\( f \) properties make it small for close words, and not too big for unlike words in context</li>
</ul>
<p><img src="https://i.imgur.com/CJ0MNrE.png" alt="" /></p>
<h2><a class="header" href="#compositional-semantics" id="compositional-semantics">Compositional Semantics</a></h2>
<p><img src="https://i.imgur.com/NsrUYj7.png" alt="" /></p>
<ul>
<li><strong>Compositional Semantics</strong> capture meaning in the structure and ordering </li>
</ul>
<p><img src="https://i.imgur.com/H4Iio7l.png" alt="" /></p>
<h3><a class="header" href="#skip-through-vectors" id="skip-through-vectors">Skip-Through Vectors</a></h3>
<p><img src="https://i.imgur.com/3TYFBkQ.png" alt="" /></p>
<ul>
<li>Predict the previous sentence and next sentences
<ul>
<li>Fed back in</li>
</ul>
</li>
<li>RNN, each state </li>
</ul>
<p><img src="https://i.imgur.com/ozMrn5g.png" alt="" /></p>
<ul>
<li>Doesn't require backprop?</li>
</ul>
<p><img src="https://i.imgur.com/VCd42TW.png" alt="" /></p>
<ul>
<li>Human evaluation.</li>
</ul>
<p><img src="https://i.imgur.com/LS5i6O1.png" alt="" /></p>
<ul>
<li>Why not just train/optimize for similarity
<ul>
<li>Minimize Manhattan distance</li>
</ul>
</li>
</ul>
<h3><a class="header" href="#semantic-entailment-evaluation" id="semantic-entailment-evaluation">Semantic Entailment Evaluation</a></h3>
<p><img src="https://i.imgur.com/pHYzAi9.png" alt="" /></p>
<ul>
<li>Tasks are Entailment, Contradiction, Neutral</li>
</ul>
<h1><a class="header" href="#homeworks" id="homeworks">Homeworks</a></h1>
<h1><a class="header" href="#homework-2-lstm" id="homework-2-lstm">Homework 2 LSTM</a></h1>
<p><a href="https://bcourses.berkeley.edu/courses/1487769/pages/assignment-2-description">Link</a></p>
<h2><a class="header" href="#setup" id="setup">Setup</a></h2>
<pre><code class="language-sh">conda install &quot;tensorflow&lt;2.0&quot;          # cpu version
conda install &quot;tensorflow-gpu&lt;2.0&quot;      # gpu version
# I think I will use PyTorch though

conda create -n cs182-assignment2  
conda activate cs182-assignment2
# to deactivate:  conda deactivate

pip3 install -r requirements.txt # @Brian changed to pip3
</code></pre>
<h3><a class="header" href="#gpus" id="gpus">GPUs</a></h3>
<p>GPUs are not required for this assignment, but will help to speed up training and processing time for questions 3-4.</p>
<h3><a class="header" href="#download-data" id="download-data">Download Data</a></h3>
<pre><code class="language-sh">cd deeplearning/datasets
./get_assignment2_data.sh
</code></pre>
<p>Now you can use Jupyter Notebook <code>jupyter serve</code>!</p>
<h2><a class="header" href="#q1-image-captioning-with-vanilla-rnns-30-points" id="q1-image-captioning-with-vanilla-rnns-30-points">Q1: Image Captioning with Vanilla RNNs (30 points)</a></h2>
<p><img src="https://i.imgur.com/dysQhPY.png" alt="" /></p>
<ul>
<li>RNN Equation</li>
</ul>
<p>b included before tanh:
<code>sum_together = dot_x + dot_h + b</code></p>
<h2><a class="header" href="#q2-image-captioning-with-lstms-30-points" id="q2-image-captioning-with-lstms-30-points">Q2: Image Captioning with LSTMs (30 points)</a></h2>
<h2><a class="header" href="#q3-network-visualization-saliency-maps-class-visualization-and-fooling-images-20-points" id="q3-network-visualization-saliency-maps-class-visualization-and-fooling-images-20-points">Q3: Network Visualization: Saliency maps, Class Visualization, and Fooling Images (20 points)</a></h2>
<h2><a class="header" href="#q4-style-transfer-20-points" id="q4-style-transfer-20-points">Q4: Style Transfer (20 points)</a></h2>
<h2><a class="header" href="#debugging" id="debugging">Debugging</a></h2>
<p>List Conda Enviornments:</p>
<pre><code>conda env list
</code></pre>
<p>Trying without conda env.</p>
<pre><code>pip install scipy==1.1.0
</code></pre>
<h2><a class="header" href="#tips-and-notes" id="tips-and-notes">Tips and Notes</a></h2>
<h3><a class="header" href="#numpy" id="numpy">Numpy</a></h3>
<pre><code>np.zeros_like

A.shape # (3, 4)
B.shape # (4, 4)
np.dot(A, B).shape # (3, 4)
</code></pre>
<h3><a class="header" href="#calculus" id="calculus">Calculus</a></h3>
<h3><a class="header" href="#chain-rule" id="chain-rule">Chain Rule:</a></h3>
<p>$$ \frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \frac{\partial y}{\partial x} $$</p>
<p>Same for \( \frac{\partial L}{\partial b} \) and \( \frac{\partial L}{\partial W} \).</p>
<p>We have \( \frac{\partial L}{\partial y} \) and we can solve for \( \frac{\partial y}{\partial y} \) locally. This may need values cached from the forward pass.</p>
<h1><a class="header" href="#exam-practice" id="exam-practice">Exam Practice</a></h1>
<h1><a class="header" href="#midterm-1" id="midterm-1">Midterm 1</a></h1>
<h1><a class="header" href="#midterm-1-spring-2019" id="midterm-1-spring-2019">Midterm 1 Spring 2019</a></h1>
<p><img src="https://i.imgur.com/0WqLprF.png" alt="" /></p>
<ol>
<li>
<p>a.</p>
<ul>
<li><strong>Ans:</strong> AlexNet is deeper. AlexNet is split on GPUs.</li>
<li><strong>Corrections:</strong> AlexNet uses ReLU, LeNet uses Sigmoid</li>
<li><strong>Review:</strong> <code>AlexNet and LeNet</code></li>
</ul>
<p>b. </p>
<ul>
<li><strong>Ans:</strong> Multi-task learning is you replace the end layers of a network including the classifier, and can connect it to other tasks like question and answering. This makes it more robust to learn representations in images.</li>
<li><strong>Corrections:</strong> Layers are shared in a network (shared representation). The network is applied to multiple tasks. Benefits are: the network has extra information to capture the essence of a task, and avoids overfitting by learning more robust features.</li>
<li><strong>Review:</strong> <code>multi-task learning</code></li>
</ul>
<p>c.</p>
<ul>
<li><strong>Ans:</strong> Generative Models try to generate the joint distribution \( P(B,A) \). Discriminative models try to model only the decision boundary.</li>
<li><strong>Corrections:</strong> generate is &quot;sample&quot; data from distribution. Can't say it models \( P(X, Y) \). Discriminative doesn't need assumptions aboutd data, can model more complex distributions.</li>
<li><strong>Review:</strong> <code>Generative vs Discriminative Models</code></li>
</ul>
</li>
</ol>
<p><img src="https://i.imgur.com/mZLgVpH.png" alt="" /></p>
<ol>
<li>
<p>d.</p>
<ul>
<li><strong>Ans:</strong> Cross Entropy is used for discrete values</li>
<li><strong>Corrections:</strong> Cross entropy loss (which is log loss in the binary case)</li>
<li><strong>Review:</strong> <code>Squared Error Loss and Cross Entropy Loss. Review Loss vs. Risk</code></li>
</ul>
<p>e.</p>
<ul>
<li><strong>Ans:</strong> (rip)</li>
<li><strong>Corrections:</strong> Loss gradients push the boundary to move points outside the margin. Close points have very large gradients.</li>
<li><strong>Review:</strong> <code>Logistic Regression, its training algorithm</code></li>
</ul>
</li>
</ol>
<p><img src="https://i.imgur.com/ZoKIsmv.png" alt="" /></p>
<ol>
<li>
<p>f.</p>
<ul>
<li><strong>Ans:</strong> Bias is how accurate the model is on learning the relationship between x and y. Variance is how accurate the model is to data it has not seen. Deep neural networks have high variance and low bias.</li>
<li><strong>Corrections:</strong> Complex models can fit data better (low bias) but are more sensitive to data (high variance) Regularization reduces variance at expense for bias. Deep neural networks are low-bias, high variance</li>
<li><strong>Review</strong>: <code>bias variance tradeoff, regularization, deep networks are low bias high variance</code></li>
</ul>
<p>g.</p>
<ul>
<li><strong>Ans:</strong> When each variable is independent. Logistic regression is more accurate when this is not the case and there is more data.</li>
<li><strong>Corrections:</strong> Naive Bayes assumes feature values are conditionally independent based on class label. Logistic regression doesn't make that assumption and is more accurate when that is not the case.</li>
<li><strong>Review</strong>: <code>Naive Bayes, Conditional Independence on class label, Logistic Regression.</code></li>
</ul>
</li>
</ol>
<p><img src="https://i.imgur.com/h77LjNU.png" alt="" /></p>
<ol>
<li>
<p>h.</p>
<ul>
<li><strong>Review:</strong> Gradients vanishes at local minima, maxima and saddle points</li>
<li><strong>Corrections:</strong></li>
<li><strong>Review:</strong> <code>Vanishing Gradients</code></li>
</ul>
<p>i.</p>
<ul>
<li><strong>Review:</strong> \( 128 \times 64 \times 64 \)</li>
<li><strong>Corrections:</strong></li>
<li><strong>Review:</strong> <code>Convolution Equation</code></li>
</ul>
<p>j.</p>
<ul>
<li><strong>Review:</strong> Split dataset into <code>k+1</code> subsets taking one out for testing. On round <code>i</code>, hold <code>i</code> for validation and and the rest of the <code>k</code> for training. Repeat with different holdout sets.</li>
<li><strong>Corrections:</strong> Check on fold <code>i</code> (validation data)</li>
<li><strong>Review:</strong> <code>Cross Validation</code></li>
</ul>
</li>
</ol>
<h1><a class="header" href="#midterm-1-practice-1-spring-2018" id="midterm-1-practice-1-spring-2018">Midterm 1 Practice 1 Spring 2018</a></h1>
<p><img src="https://i.imgur.com/v1k6bSW.png" alt="" /></p>
<ol>
<li>
<p>a. Objects can be seen from different perspectives. Lighting can be different. Objects can be blocked; object partially visible.</p>
<ul>
<li><strong>Review:</strong> <code>Lecture 1.</code></li>
</ul>
<p>b. When doing transfer learning, the network has already extracted high level like edges, mid level like shapes and lines, and low level like faces. These generalize and don't have to be updated too much in being fine tuned. <strong>The already trained low-dimensional features capture the essence of the data, can have faster fine-tuning.</strong></p>
<ul>
<li><strong>Review:</strong> <code>Lecture 1, Transfer Learning.</code></li>
</ul>
<p>c. Expected Risk is <strong>expectation</strong> over all datasets of model loss (prediction – actual result). Empirical risk is difference over a fixed dataset sample. Machine Learning minimizes empirical risk.</p>
<ul>
<li><strong>Review:</strong> <code>Expected Risk, Empirical Risk, Loss vs Risk.</code></li>
</ul>
</li>
</ol>
<p><img src="https://i.imgur.com/Z5g6JuI.png" alt="" /></p>
<ol>
<li>
<p>d. L2 is designed for loss of continuous value functions. Logistic regression is for binary classification (0 or 1). We use binary cross-entropy loss. (It encourages outputs to be 0 or 1?) The loss becomes a probability?</p>
<ul>
<li><strong>Review:</strong> <code>L2 Loss vs Cross-Entropy Loss</code></li>
<li>Why does it work for binary/discrete classification vs L2 for continuous? What is the relationship of it to probability?</li>
</ul>
<p>e. Newton's second-order method converge to local minima and saddle points. Where second derivative gradients (is this correct?) are zero</p>
<ul>
<li><strong>Review:</strong> <code>Netwon Second-Order Methods</code></li>
</ul>
<p>f. Using a max-margin classifier is the most robust (decreases overfitting) classifier to unseen data. It maximizes distance of the nearest points to the margin/decision boundary. Diagram shows decision boundary with max distance to nearest points</p>
<ul>
<li><strong>Review:</strong> <code>SVM, Max-Margin, Hinge Loss</code></li>
</ul>
</li>
</ol>
<p><img src="https://i.imgur.com/ray5suc.png" alt="" /></p>
<ol>
<li>g. for each data point \( (x_i, y_i) \), we have \( f_{y_i}(x_i) - f_j(x_i) \) (the difference) for all classes \( j \neq y \). The original SVM loss is \( max(0, 1 - yw^Tx) \). Max margins for all classes not \( j \) is \( max(0, 1 - f_{y_i}(x_i) + f_j(x_i) \). Loss is sum/averaged. OvA classifer? What the heck is this?
<ul>
<li><strong>Review:</strong> <code>SVM</code></li>
</ul>
</li>
</ol>
<p><img src="https://i.imgur.com/yCRut4F.png" alt="" /></p>
<ol>
<li>
<p>h. Multiclass logistic regression can learn the multiclass naive Bayes</p>
<ul>
<li><strong>Review:</strong> <code>Multiclass Logistic Regression, Multiclass Naive Bayes, their relationship</code></li>
</ul>
<p>i. Gradient decreases proportionally to accumulating value at \( \approx \frac{1}{\sqrt{t}}\). This is because the denominator of the update contains sum of squares of past gradients.</p>
</li>
</ol>
<p><img src="https://i.imgur.com/0rGE4yp.png" alt="" /></p>
<ol>
<li>
<p>j. </p>
<ul>
<li><strong>Ans:</strong> Local optima were usually saddle points. SGD performs well because it does not just follow the gradient, so it is likely to fall off a saddle point.</li>
<li><strong>Corrections:</strong> </li>
<li><strong>Review:</strong> <code>loss landscape and convexity, SGD</code> </li>
</ul>
<p>k.</p>
<ul>
<li><strong>Ans:</strong> Depends on the stride and padding. if stride 1 padding 0,  \( 194 \times 194 \times 1\)</li>
<li><strong>Corrections:</strong> </li>
<li><strong>Review:</strong> <code>Convolution reshape equation</code> </li>
</ul>
<p>l.</p>
<ul>
<li><strong>Ans:</strong> They extract features in images. These features can be used for more robust representations of images.</li>
<li><strong>Corrections:</strong> </li>
<li><strong>Review:</strong> <code>Convolutions, Feature Detection</code> </li>
</ul>
<p>m. </p>
<ul>
<li><strong>Ans:</strong> Expectation of dropout is <code>output * p</code>. We can use the expectation of dropout in inference then, but also just use the <code>output</code> by doing dropout at training time (with a mask where each index has probability <code>p</code> being 1 (retained) and dividing by <code>p</code>.</li>
<li><strong>Corrections:</strong> </li>
<li><strong>Review:</strong> <code>Dropout</code></li>
</ul>
<p>n. </p>
<ul>
<li><strong>Ans:</strong> Prediction Averaging reduces variance. Each model is robust to different relationships (learn different things). Parameter averaging canceling out the relationships each model learns. Snapshot parameter ensembling works because the relationships the model was learning are close to the same, while still reducing variance.</li>
<li><strong>Corrections:</strong> </li>
<li><strong>Review:</strong> <code>Prediction Averaging, Parameter Averaging, Snapshot Ensembling</code></li>
</ul>
<p>o. </p>
<ul>
<li><strong>Ans:</strong> x -&gt; [] (array with h wraps into block) -&gt; y</li>
<li><strong>Corrections:</strong> </li>
<li><strong>Review:</strong> <code>RNN</code></li>
</ul>
<p>p. </p>
<ul>
<li><strong>Ans:</strong> <code>y = A_yh * tanh(A_hx * x + A_hh * h)</code></li>
<li><strong>Corrections:</strong> </li>
<li><strong>Review:</strong> <code>RNN equation</code></li>
</ul>
<p>q. </p>
<ul>
<li><strong>Ans:</strong> Remember and Forget?</li>
<li><strong>Corrections:</strong> </li>
<li><strong>Review:</strong> <code>LSTM equation and interpretation</code></li>
</ul>
</li>
</ol>
<h1><a class="header" href="#midterm-1-cheat-sheet" id="midterm-1-cheat-sheet">Midterm 1 Cheat Sheet</a></h1>
<p><img src="https://i.imgur.com/5fKIV3n.jpg" alt="" /></p>
<p><img src="https://i.imgur.com/Ch56t9P.jpg" alt="" /></p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        

        
        
        
        <script type="text/javascript">
            window.playpen_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>
        
        

    </body>
</html>
