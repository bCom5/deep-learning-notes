<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js ayu">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>CS 282: Deep Learning Notes Spring 2020</title>
        
        <meta name="robots" content="noindex" />
        

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">
        <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:500" rel="stylesheet" type="text/css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "ayu" : "ayu";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('ayu')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div id="sidebar-scrollbox" class="sidebar-scrollbox">
                <ol class="chapter"><li class="expanded "><a href="introduction.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="expanded "><a href="lectures/lectures.html"><strong aria-hidden="true">2.</strong> Lectures</a></li><li><ol class="section"><li class="expanded "><a href="lectures/lec09_rnn.html"><strong aria-hidden="true">2.1.</strong> Recurrent Networks, LSTMS, and Applications</a></li><li class="expanded "><a href="lectures/lec10_visual.html"><strong aria-hidden="true">2.2.</strong> Visualizing Deep Networks</a></li><li class="expanded "><a href="lectures/lec11_attention.html"><strong aria-hidden="true">2.3.</strong> Attention Networks</a></li><li class="expanded "><a href="lectures/lec12_text_semantics.html"><strong aria-hidden="true">2.4.</strong> Text Semantics</a></li><li class="expanded "><a href="lectures/lec13_translation.html"><strong aria-hidden="true">2.5.</strong> Translation</a></li><li class="expanded "><a href="lectures/lec14_transformers.html"><strong aria-hidden="true">2.6.</strong> Transformers and Pre-Training</a></li></ol></li><li class="expanded "><a href="other_resources/other_resources.html"><strong aria-hidden="true">3.</strong> Other Resources</a></li><li><ol class="section"><li class="expanded "><a href="other_resources/viz_seq.html"><strong aria-hidden="true">3.1.</strong> Visualizing Seq2Seq with Attention</a></li><li class="expanded "><a href="other_resources/viz_transformers.html"><strong aria-hidden="true">3.2.</strong> Visualizing Transformers</a></li></ol></li><li class="expanded "><a href="homework/homework.html"><strong aria-hidden="true">4.</strong> Homeworks</a></li><li><ol class="section"><li class="expanded "><a href="homework/hw2_rnn_lstm.html"><strong aria-hidden="true">4.1.</strong> Homework 2 RNN and LSTM</a></li></ol></li><li class="expanded "><a href="exam/exam.html"><strong aria-hidden="true">5.</strong> Exam Practice</a></li><li><ol class="section"><li class="expanded "><a href="exam/midterm1/midterm1.html"><strong aria-hidden="true">5.1.</strong> Midterm 1</a></li><li><ol class="section"><li class="expanded "><a href="exam/midterm1/mt1sp19.html"><strong aria-hidden="true">5.1.1.</strong> Midterm 1 Spring 2019</a></li><li class="expanded "><a href="exam/midterm1/mt1prac1sp18.html"><strong aria-hidden="true">5.1.2.</strong> Midterm 1 Practice 1 Spring 2018</a></li><li class="expanded "><a href="exam/midterm1/cheatsheet.html"><strong aria-hidden="true">5.1.3.</strong> Midterm 1 Cheat Sheet</a></li></ol></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar" class="menu-bar">
                    <div id="menu-bar-sticky-container">
                        <div class="left-buttons">
                            <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                                <i class="fa fa-bars"></i>
                            </button>
                            <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                                <i class="fa fa-paint-brush"></i>
                            </button>
                            <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                                <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu (default)</button></li>
                            </ul>
                            
                            <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                                <i class="fa fa-search"></i>
                            </button>
                            
                        </div>

                        <h1 class="menu-title">CS 282: Deep Learning Notes Spring 2020</h1>

                        <div class="right-buttons">
                            <a href="print.html" title="Print this book" aria-label="Print this book">
                                <i id="print-button" class="fa fa-print"></i>
                            </a>
                            
                        </div>
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1><a class="header" href="#introduction" id="introduction">Introduction</a></h1>
<p>CS 282 Deep Learning, Spring 2020 
Taught by John Canny</p>
<p><img src="https://i.imgur.com/0ztiyxz.png" alt="" /></p>
<ul>
<li><a href="https://bcourses.berkeley.edu/courses/1487769">Website</a></li>
<li><a href="https://www.youtube.com/playlist?list=PLkFD6_40KJIwaO6Eca8kzsEFBob0nFvwm">Webcast</a></li>
</ul>
<h1><a class="header" href="#lectures" id="lectures">Lectures</a></h1>
<h1><a class="header" href="#recurrent-networks-lstms-and-applications" id="recurrent-networks-lstms-and-applications">Recurrent Networks, LSTMS, and Applications</a></h1>
<h1><a class="header" href="#visualizing-deep-networks" id="visualizing-deep-networks">Visualizing Deep Networks</a></h1>
<h3><a class="header" href="#activation-maximization" id="activation-maximization">Activation Maximization</a></h3>
<p><img src="https://i.imgur.com/bKFfWQy.png" alt="" /></p>
<p>Generate a synthetic image I, normalize it L2 nrom, maxing a class. This is done through backpropogation. Class is one hot encoded <code>[0 0 1 0 0]</code>. Images are weird though.</p>
<h3><a class="header" href="#deconv-approaches" id="deconv-approaches">Deconv Approaches</a></h3>
<p><img src="https://i.imgur.com/L9W4AWf.png" alt="" /></p>
<p>To make generating synthetic images better:</p>
<ul>
<li>backprop zeros out negative values in the forward pass</li>
<li>decovnet zeros out negative gradients in the backward pass. Negative gradients are inhibitory activations (they were the wrong class)</li>
<li>guided backprop does both, zeros out both</li>
</ul>
<h3><a class="header" href="#neural-style-transfer" id="neural-style-transfer">Neural Style Transfer</a></h3>
<p><img src="https://i.imgur.com/xdURQhp.jpg" alt="" /></p>
<p>Review this!</p>
<h1><a class="header" href="#attention-networks" id="attention-networks">Attention Networks</a></h1>
<p><strong>The most important idea in deep networks this decade.</strong></p>
<h3><a class="header" href="#pilot-analogy" id="pilot-analogy">Pilot analogy</a></h3>
<p><img src="https://i.imgur.com/LU12T8h.jpg" alt="" /></p>
<p>Pilots move their focus on different things, gauges, buttons, switches</p>
<h3><a class="header" href="#papers" id="papers">Papers</a></h3>
<p>Attention is All You Need (2017) only stacked attention, previous used RNNs</p>
<h2><a class="header" href="#soft-vs-hard-attention-introduction" id="soft-vs-hard-attention-introduction">Soft vs Hard Attention Introduction</a></h2>
<p><img src="https://i.imgur.com/GwTuFpG.png" alt="" /></p>
<ul>
<li><strong>Hard Attention</strong> is what humans do. Not differentiable because attention map is 1 where focused, 0 elsewhere (discrete).</li>
<li><strong>Soft Attention</strong> - linear combination over inputs, can use backprop.
<ul>
<li>Trains network and attention to the right place!</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/3JN8VEn.png" alt="" />
<strong>Supervised Learning</strong> vs <strong>Reinforcement Learning</strong>, maximizing a reward (discrete)</p>
<p>Reinforcement Learning, the <strong>agent tries to maximize the sum of rewards over an epoch</strong></p>
<h2><a class="header" href="#attention-for-recognition" id="attention-for-recognition">Attention for Recognition</a></h2>
<p><img src="https://i.imgur.com/d2tfdX3.png" alt="" /></p>
<p>Has time stamps. Has location and the image. RNN always predicting gaze location for next time.</p>
<p><img src="https://i.imgur.com/BAmjvmm.png" alt="" /></p>
<p>Resolution when focusing on something (glimpse). Output is location and action (classify/don't classify). Inputs are on top, output is at the bottom.</p>
<p><img src="https://i.imgur.com/NnoPB1G.png" alt="" /></p>
<p>This shows glimpses. Focused in center. Blurred around. More blurred after, 3 levels. Green is location as it moves around.</p>
<h2><a class="header" href="#soft-attention-for-translation" id="soft-attention-for-translation">Soft Attention for Translation</a></h2>
<p><img src="https://i.imgur.com/ode4uKw.png" alt="" /></p>
<p>When the world is outputting <em>Me</em>, it has attention on <em>I</em>. Likewise with <em>coffee</em> and <em>cafe</em>.</p>
<p><img src="https://i.imgur.com/bWX5rXq.png" alt="" /></p>
<p>What word does each word correspond to? (Where is the attention located)?</p>
<h2><a class="header" href="#rnn-for-captioning" id="rnn-for-captioning">RNN for Captioning</a></h2>
<p><img src="https://i.imgur.com/O3QzfWP.png" alt="" /></p>
<p>Output <code>d0</code> goes into a softmax over the vocabulary. <em>Bird</em> is fed back into <code>x1</code>, next token. </p>
<p><img src="https://i.imgur.com/qA37EVl.png" alt="" /></p>
<p>Have \( L \times D \) features extracted. Our RNN outputs classification and weight location. Mask/summing (?) with the features give the weighted feature \( D \) fed back into the RNN.</p>
<h2><a class="header" href="#soft-vs-hard-attention-in-captioning" id="soft-vs-hard-attention-in-captioning">Soft vs. Hard Attention in Captioning</a></h2>
<p><img src="https://i.imgur.com/VNdA0yR.png" alt="" /></p>
<ul>
<li>Have two inputs, RNN gives probability distribution (see above) (also think about the result of softmax).</li>
<li><strong>Soft Attention</strong> sum all location and derivative is \( \frac{dz}{dp} \). Train with gradient descent.</li>
<li><strong>Hard Attention</strong> samples only one location. With argmax, \( \frac{dz}{dp} \) is almost zero everywhere. Can't use gradient descent.</li>
</ul>
<p><img src="https://i.imgur.com/RphCcoj.png" alt="" /></p>
<p>Soft is smooth around the area. Hard is discrete (1 or 0) around the image.</p>
<p><img src="https://i.imgur.com/wLcn23X.jpg" alt="" /></p>
<p><img src="https://i.imgur.com/YYT5G2u.jpg" alt="" /></p>
<p>Can see where it made mistakes!!!</p>
<h2><a class="header" href="#attention-mechanics" id="attention-mechanics">Attention Mechanics</a></h2>
<p><img src="https://i.imgur.com/0q6PVTx.png" alt="" /></p>
<ul>
<li>Image Feature output matrix \( L \times D \)</li>
<li>Attention weights output vector \( L \). Can \( L \) be an image \( H \times W \)?</li>
<li>Do a broadcast multiply - each vector in \( D \) multiplies by the Attention vector. Output is still \( L \times D \)</li>
<li>Sum the \( L \) axis (vector). Output is size \( D \).</li>
</ul>
<p><img src="https://i.imgur.com/NcPqXPf.png" alt="" /></p>
<ul>
<li>Need sum for the contributions. This is because \( B \) is broadcast multiplied to \( A \)</li>
</ul>
<h3><a class="header" href="#salience" id="salience">Salience</a></h3>
<p><img src="https://i.imgur.com/v4cwBwH.png" alt="" /></p>
<ul>
<li>Circle operation is product</li>
<li>What is salience (A *dot* )
<ul>
<li><strong>Salience:</strong> The total contribution and gradient to the output (the importance of an area)</li>
</ul>
</li>
</ul>
<h2><a class="header" href="#attention-and-lstm" id="attention-and-lstm">Attention and LSTM</a></h2>
<p><img src="https://i.imgur.com/EDcAoA5.png" alt="" /></p>
<ul>
<li>LSTMs also receive a <strong>salience gradient</strong>. There is multiplicative gating (kind of like attention)</li>
</ul>
<h2><a class="header" href="#soft-attention-for-video" id="soft-attention-for-video">Soft Attention for Video</a></h2>
<p><img src="https://i.imgur.com/wHL1iwj.png" alt="" /></p>
<p>Now temporal (time) can have attention. (Which frame is important?)</p>
<p><img src="https://i.imgur.com/lo2rgUy.png" alt="" /></p>
<ul>
<li>Can have spatial and temporal attention. Temporal in this network.</li>
<li>Create a vector of feature frames.</li>
</ul>
<p><img src="https://i.imgur.com/uvLw16A.png" alt="" /></p>
<p><img src="https://i.imgur.com/iNBaAPT.png" alt="" /></p>
<p>Perplexity is diversity of word options. Goal is smaller which means the model is more confident.</p>
<h2><a class="header" href="#find-the-region-again" id="find-the-region-again">Find the Region Again</a></h2>
<p><img src="https://i.imgur.com/gVzxWMc.png" alt="" /></p>
<ul>
<li>Classify - attention to regions in input</li>
<li>Generate - draw digit</li>
</ul>
<p>Has a parameter for scale</p>
<h2><a class="header" href="#takeaways" id="takeaways">Takeaways</a></h2>
<p><img src="https://i.imgur.com/vquu8LF.png" alt="" /></p>
<ul>
<li><strong>Salience:</strong> Emphasize imporant data.</li>
</ul>
<p><img src="https://i.imgur.com/ddEkm4X.png" alt="" /></p>
<h1><a class="header" href="#text-semantics" id="text-semantics">Text Semantics</a></h1>
<p><a href="https://youtu.be/1rzjaUp6NiQ">Webcast</a></p>
<p><img src="https://i.imgur.com/7LIXXrJ.png" alt="" /></p>
<ul>
<li><strong>Propositional</strong>: logic, formal, mathematical</li>
<li><strong>Vector</strong>: represent as numbers, high dimensional space.</li>
</ul>
<p><img src="https://i.imgur.com/br0ryjb.png" alt="" /></p>
<ul>
<li>Propositional uses class (usually noun) and predicates - operation (usually a verb). <strong>Reasoning</strong></li>
<li>Word and sentences represented as vectors. Commutative! That is a problem.</li>
</ul>
<h2><a class="header" href="#vector-embedding-of-words" id="vector-embedding-of-words">Vector Embedding of Words</a></h2>
<p><img src="https://i.imgur.com/CGwXg4v.png" alt="" /></p>
<ul>
<li><strong>Bag-of-Words</strong> - sentence</li>
</ul>
<h3><a class="header" href="#word-similarity" id="word-similarity">Word Similarity</a></h3>
<p><img src="https://i.imgur.com/Vd6TRq8.png" alt="" /></p>
<ul>
<li><strong>Similar word</strong> in <strong>similar contexts</strong>. Dog and canine can be replaced.</li>
</ul>
<p><img src="https://i.imgur.com/Nqc0KTP.png" alt="" /></p>
<h3><a class="header" href="#dimension-reduction" id="dimension-reduction">Dimension Reduction</a></h3>
<p><img src="https://i.imgur.com/L54iu21.png" alt="" /></p>
<ul>
<li>Could <strong>one-hot encode</strong> every word in vocab. That's expensive.</li>
<li>Alt: counts of word used in that context. &quot;Dog barks&quot;.</li>
</ul>
<h2><a class="header" href="#dimensionality-reduction" id="dimensionality-reduction">Dimensionality Reduction</a></h2>
<p><img src="https://i.imgur.com/sd6BKtO.png" alt="" /></p>
<h3><a class="header" href="#latent-semantic-analysis" id="latent-semantic-analysis">Latent Semantic Analysis</a></h3>
<p><img src="https://i.imgur.com/YogMF8j.png" alt="" /></p>
<ul>
<li>Encode documents <code>N</code></li>
<li><code>M</code> is word count.</li>
</ul>
<p><img src="https://i.imgur.com/hXnjAO7.png" alt="" /></p>
<p>Counts words</p>
<h3><a class="header" href="#latent-semantic-analysis-1" id="latent-semantic-analysis-1">Latent Semantic Analysis</a></h3>
<p><img src="https://i.imgur.com/QEJ4hZq.png" alt="" /></p>
<ul>
<li>Low dimensional approximation using SVD. (Review SVD!)</li>
<li>Embedding is V, factors encode document contexts</li>
</ul>
<h4><a class="header" href="#singular-value-decomposition" id="singular-value-decomposition">Singular Value Decomposition</a></h4>
<p><img src="https://i.imgur.com/8vySkwT.png" alt="" /></p>
<ul>
<li>\( U, V \) <strong>orthogonal, normal</strong>. \( S \) rectangular diagonal, singular values, right padded with 0s to fit</li>
</ul>
<h4><a class="header" href="#review-how-to-calculate-svd" id="review-how-to-calculate-svd">Review How to Calculate SVD</a></h4>
<p><img src="https://i.imgur.com/VHinLAL.png" alt="" /></p>
<ul>
<li>It can be computed from eigenvalues of \( T ~ T ^T \)</li>
<li>Insides cancel out, there's an \( S^2 \) term!!!</li>
</ul>
<p><img src="https://i.imgur.com/FB3xuv1.png" alt="" /></p>
<p>Both work!</p>
<p><img src="https://i.imgur.com/2ynYgmd.png" alt="" /></p>
<ul>
<li>Can have a smaller S, using only the largest singular values (S?) </li>
<li>Small K dimension</li>
<li>High dimensional T to low dimensional </li>
<li><strong>best possible reconstruction</strong> of documents from their embedding</li>
</ul>
<p><img src="https://i.imgur.com/mWKPVXo.png" alt="" /></p>
<ul>
<li>Documents are T, Encoding is Z</li>
</ul>
<p><img src="https://i.imgur.com/0IJVSaK.png" alt="" /></p>
<ul>
<li>Have a network learn SVD</li>
<li>V is vocab times latent dimensions</li>
<li>Scalable version of LSA/SVD</li>
<li>similar things will get mapped to similar places</li>
</ul>
<h2><a class="header" href="#t-sne-word-embeddings" id="t-sne-word-embeddings">t-SNE Word Embeddings</a></h2>
<p><img src="https://i.imgur.com/tF1VGAF.png" alt="" /></p>
<h2><a class="header" href="#word2vec" id="word2vec">Word2Vec</a></h2>
<p><img src="https://i.imgur.com/LmTf2z2.png" alt="" /></p>
<ul>
<li>neighborhood of a word instead of whole document, <strong>skip-gram</strong></li>
<li>nonlinearity</li>
</ul>
<p><img src="https://i.imgur.com/GLAWU4T.png" alt="" /></p>
<ul>
<li>Predict center words from context</li>
<li>order does not matter</li>
</ul>
<p><img src="https://i.imgur.com/ZrKymNE.png" alt="" /></p>
<ul>
<li>Predict context words from center word</li>
</ul>
<p><img src="https://i.imgur.com/9sTa45d.png" alt="" /></p>
<ul>
<li>Problem of SVD is it favors minimizing large distances (min squared error). Want to preserve <strong>close</strong> distance like t-SNE</li>
</ul>
<p><img src="https://i.imgur.com/E3M523Q.png" alt="" /></p>
<ul>
<li>Canny says it's a mess</li>
</ul>
<p><img src="https://i.imgur.com/29fFUrk.png" alt="" /></p>
<ul>
<li>Holds relations as vectors! Vector math!</li>
</ul>
<p><img src="https://i.imgur.com/0WFGMaA.png" alt="" /></p>
<ul>
<li>This model uses contexts</li>
<li>City pairs, opposites, comparatives (great, greater)</li>
</ul>
<p>Criticisms:</p>
<ul>
<li>Cross-entropy emphasizes small word combinations</li>
<li>Expensive to softmax</li>
</ul>
<p><img src="https://i.imgur.com/Er3Ogvq.png" alt="" /></p>
<ul>
<li>Maybe wrong data</li>
</ul>
<p><img src="https://i.imgur.com/CwVSFNj.png" alt="" /></p>
<ul>
<li>Now words times words! Better for contextual words!</li>
<li>Window size (prob exam problem), just try it and check</li>
</ul>
<h2><a class="header" href="#glove" id="glove">GloVe</a></h2>
<p><img src="https://i.imgur.com/OkSCRaK.png" alt="" /></p>
<ul>
<li>\( C_{ij} \) num times word j in context of word i. Query the matrix
<ul>
<li>For favoring the counts</li>
</ul>
</li>
<li>\( u_i \) is embedding, \( v_j \) is context word embedding
<ul>
<li>Inner product for similarity!!!</li>
</ul>
</li>
<li>\( f \) properties make it small for close words, and not too big for unlike words in context</li>
</ul>
<p><img src="https://i.imgur.com/CJ0MNrE.png" alt="" /></p>
<h2><a class="header" href="#compositional-semantics" id="compositional-semantics">Compositional Semantics</a></h2>
<p><img src="https://i.imgur.com/NsrUYj7.png" alt="" /></p>
<ul>
<li><strong>Compositional Semantics</strong> capture meaning in the structure and ordering </li>
</ul>
<p><img src="https://i.imgur.com/H4Iio7l.png" alt="" /></p>
<h3><a class="header" href="#skip-through-vectors" id="skip-through-vectors">Skip-Through Vectors</a></h3>
<p><img src="https://i.imgur.com/3TYFBkQ.png" alt="" /></p>
<ul>
<li>Predict the previous sentence and next sentences
<ul>
<li>Fed back in</li>
</ul>
</li>
<li>RNN, each state </li>
</ul>
<p><img src="https://i.imgur.com/ozMrn5g.png" alt="" /></p>
<ul>
<li>Doesn't require backprop?</li>
</ul>
<p><img src="https://i.imgur.com/VCd42TW.png" alt="" /></p>
<ul>
<li>Human evaluation.</li>
</ul>
<p><img src="https://i.imgur.com/LS5i6O1.png" alt="" /></p>
<ul>
<li>Why not just train/optimize for similarity
<ul>
<li>Minimize Manhattan distance</li>
</ul>
</li>
</ul>
<h3><a class="header" href="#semantic-entailment-evaluation" id="semantic-entailment-evaluation">Semantic Entailment Evaluation</a></h3>
<p><img src="https://i.imgur.com/pHYzAi9.png" alt="" /></p>
<ul>
<li>Tasks are Entailment, Contradiction, Neutral</li>
</ul>
<h1><a class="header" href="#translation" id="translation">Translation</a></h1>
<ul>
<li><a href="https://briantliao.com/store/cs282-lectures-slides/lec13.pdf">Slides</a></li>
</ul>
<h2><a class="header" href="#review" id="review">Review</a></h2>
<p><img src="https://i.imgur.com/ytGyXuo.png" alt="" /></p>
<ul>
<li>Inner Product (or is it Dot Product?) is closeness</li>
</ul>
<p><img src="https://i.imgur.com/E9DMaxX.png" alt="" /></p>
<ul>
<li>Predict previous and next sentences, now ordering is included</li>
</ul>
<p><img src="https://i.imgur.com/r9NLuPf.png" alt="" /></p>
<ul>
<li>Train for your task! This is based on closeness.</li>
</ul>
<h2><a class="header" href="#translation-1" id="translation-1">Translation</a></h2>
<p><img src="https://i.imgur.com/myVDtam.png" alt="" /></p>
<h3><a class="header" href="#sequence-to-sequence" id="sequence-to-sequence">Sequence to Sequence</a></h3>
<ul>
<li><strong>Sequence-To-Sequence RNN</strong>, input fed into left, output comes out of the right</li>
</ul>
<p><img src="https://i.imgur.com/3HwFFEZ.png" alt="" /></p>
<ul>
<li>At each RNN node, the output is fed back in. Keep the n-best</li>
</ul>
<h3><a class="header" href="#bleu" id="bleu">Bleu</a></h3>
<p><img src="https://i.imgur.com/9G6bB4e.png" alt="" /></p>
<ul>
<li>Candidate is what our network gives us. References are from humans.</li>
<li>See where it matches.</li>
</ul>
<p><img src="https://i.imgur.com/1lMHytF.png" alt="" /></p>
<ul>
<li>Unigram matching - mapping</li>
</ul>
<p><img src="https://i.imgur.com/j2kCRM2.png" alt="" /></p>
<ul>
<li>Averaged over references</li>
</ul>
<p><img src="https://i.imgur.com/IyG2pdL.png" alt="" /></p>
<ul>
<li><strong>Unigram</strong> for <strong>adequacy</strong></li>
<li><strong>Ngram</strong> for <strong>fluency</strong>
<ul>
<li>Fluency is better (imo)</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/j4kbkLo.png" alt="" /></p>
<ul>
<li>Bigram (2 word length)</li>
</ul>
<p><img src="https://i.imgur.com/P9iowOf.png" alt="" /></p>
<ul>
<li>Geometric Avg ( (( w_n log p_n \) )
<ul>
<li>BP: penalty shorter than r</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/4xTSym1.png" alt="" /></p>
<ul>
<li>Ensemble does well</li>
</ul>
<p><img src="https://i.imgur.com/4dQNLlc.png" alt="" /></p>
<ul>
<li>Try going backwards!
<ul>
<li>coffee love I</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/EC4HORT.png" alt="" /></p>
<ul>
<li>Really small BEAM search</li>
</ul>
<p><img src="https://i.imgur.com/tJE0USb.png" alt="" /></p>
<ul>
<li>Problem! There's a bottleneck for information</li>
</ul>
<p><img src="https://i.imgur.com/qERSjqN.png" alt="" /></p>
<ul>
<li>In soft-attention, Coffee is related to cafe
<ul>
<li>But bottlenecked!</li>
</ul>
</li>
</ul>
<h2><a class="header" href="#soft-attention-for-translation-1" id="soft-attention-for-translation-1">Soft Attention for Translation</a></h2>
<p><img src="https://i.imgur.com/ZETrFIt.png" alt="" /></p>
<ul>
<li><strong>Context vector</strong> is the sum of all weighed h, which are hidden states</li>
<li>Weights are <strong>Mixture weights</strong>, softmax over alignment scores</li>
<li><strong>Alignment scores</strong> input words and output words?</li>
</ul>
<p><img src="https://i.imgur.com/YPalgse.png" alt="" /></p>
<ul>
<li>Wow this is amazing</li>
<li>Bi-directional RNN Encoder</li>
</ul>
<p><img src="https://i.imgur.com/Z1r7qjJ.png" alt="" /></p>
<ul>
<li>Decoder is a RNN, sample word fed back in, get next word out</li>
<li>You have a <strong>Recurrent State</strong> in the <strong>Decoder RNN</strong></li>
<li>and a <strong>Attention Vectors</strong> hidden state in the <strong>Bidirection Encoder RNN</strong></li>
</ul>
<p><img src="https://i.imgur.com/RFYS1Xt.png" alt="" /></p>
<p><img src="https://i.imgur.com/nl7EEHM.png" alt="" /></p>
<ul>
<li>English to French, correctly picks up reversal!</li>
</ul>
<p><img src="https://i.imgur.com/EF8asQs.png" alt="" /></p>
<ul>
<li><strong>Neural Machine Translation!</strong>
<ul>
<li>Complicated model, what the heck</li>
</ul>
</li>
</ul>
<h3><a class="header" href="#stanford-manning-update-attention-neural-machine-translation" id="stanford-manning-update-attention-neural-machine-translation">Stanford Manning Update Attention Neural Machine Translation</a></h3>
<p><img src="https://i.imgur.com/LU9UKYM.png" alt="" /></p>
<ul>
<li>Stacked LSTM</li>
</ul>
<p><img src="https://i.imgur.com/GyKZ5bK.png" alt="" /></p>
<ul>
<li><strong>Global Attention</strong>, Attention is not Recurrent, align weights are now global</li>
</ul>
<p><img src="https://i.imgur.com/4AXBT7p.png" alt="" /></p>
<ul>
<li>Alignment (?)</li>
</ul>
<h3><a class="header" href="#translation-and-parsing" id="translation-and-parsing">Translation and Parsing</a></h3>
<p><img src="https://i.imgur.com/EqGiSWf.png" alt="" /></p>
<ul>
<li>Can generate like nested/tree code!</li>
</ul>
<p><img src="https://i.imgur.com/I2NfCBU.png" alt="" /></p>
<ul>
<li>Build the parse tree (Nouns, parts, etc.)
<ul>
<li>Trees are like LISP, can be nested in parenthesis</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/u5gQqQI.png" alt="" /></p>
<ul>
<li>Sequence-To-Sequence Parse Tree</li>
</ul>
<p><img src="https://i.imgur.com/SBY9QQq.png" alt="" /></p>
<ul>
<li>Training, on the three-bank not well</li>
<li>Then train on the Berkeley Parser</li>
<li>Add attention and retrain on human data
<ul>
<li>It's like Model Distillation?</li>
<li>Something about overfitting</li>
</ul>
</li>
</ul>
<h2><a class="header" href="#attention-only-translation" id="attention-only-translation">Attention-only Translation</a></h2>
<p><img src="https://i.imgur.com/wNITJAH.png" alt="" /></p>
<ul>
<li>Time grows in proportion to sentence length</li>
<li>Long-range hard</li>
<li>Hierarchal deeper structure hard</li>
</ul>
<h3><a class="header" href="#transformer" id="transformer">Transformer</a></h3>
<p><img src="https://i.imgur.com/YJD3LBN.png" alt="" /></p>
<ul>
<li>Query-Key-Value</li>
</ul>
<p><img src="https://i.imgur.com/ki9Z54a.png" alt="" /></p>
<ul>
<li>Try my query to other keys</li>
</ul>
<p><img src="https://i.imgur.com/ctviBRe.png" alt="" /></p>
<ul>
<li>Score is local, Softmax x Value then Sum is global</li>
</ul>
<p><img src="https://i.imgur.com/079hOLX.png" alt="" /></p>
<ul>
<li>Input x, compute inner products with their matrices
<ul>
<li>\( W^V, W^K, W^Q \)</li>
<li>Get \( V_1, K_1, Q_1 \)</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/3Xd2tFj.png" alt="" /></p>
<p><img src="https://i.imgur.com/SUfx7a8.png" alt="" /></p>
<ul>
<li>Q, K, V are matrices now</li>
</ul>
<p><img src="https://i.imgur.com/qzELmTE.png" alt="" /></p>
<ul>
<li><strong>Multi-Headed Attention</strong></li>
</ul>
<p><img src="https://i.imgur.com/14NfcZi.png" alt="" /></p>
<ul>
<li>multiple heads
<ul>
<li>recognized more difficult is coupled</li>
<li>blue, green, red</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/sPO4eMU.png" alt="" /></p>
<ul>
<li><strong>Transformer Encoder</strong>
<ul>
<li>Encoded as a single matrix</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/jTD6Qo6.png" alt="" /></p>
<h4><a class="header" href="#transformer-encoderdecoder" id="transformer-encoderdecoder">Transformer Encoder/Decoder</a></h4>
<p><img src="https://i.imgur.com/IKZ7vfD.png" alt="" /></p>
<ul>
<li>Value Key fed in</li>
</ul>
<p><img src="https://i.imgur.com/wZmiJ9e.png" alt="" /></p>
<h1><a class="header" href="#transformers-and-pre-training" id="transformers-and-pre-training">Transformers and Pre-Training</a></h1>
<p><a href="https://towardsdatascience.com/visual-attention-model-in-deep-learning-708813c2912c">Visual Attention</a></p>
<p><a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Visualizing NMT</a></p>
<p><a href="http://jalammar.github.io/illustrated-transformer/">Visualizing Transformers</a></p>
<h2><a class="header" href="#review-1" id="review-1">Review</a></h2>
<p><img src="https://i.imgur.com/BVdVEoi.png" alt="" /></p>
<ul>
<li>Up and Down Attention</li>
</ul>
<p><img src="https://i.imgur.com/gGG4J6N.png" alt="" /></p>
<ul>
<li>Transformers (Convolution of NLP)
<ul>
<li>Self Attention</li>
<li>Encoding/Decoding</li>
</ul>
</li>
</ul>
<h2><a class="header" href="#transformers" id="transformers">Transformers</a></h2>
<p><img src="https://i.imgur.com/mYSymyv.png" alt="" /></p>
<p><img src="https://i.imgur.com/5jvchbe.png" alt="" /></p>
<ul>
<li><strong>Multi-Headed Attention</strong>
<code>h</code> block of scaled dot-product attention. Tensor V, K, Q</li>
</ul>
<p><img src="https://i.imgur.com/aqMQVVh.png" alt="" /></p>
<ul>
<li>What is Encoder/Decoder?</li>
</ul>
<h3><a class="header" href="#transformer-encoder" id="transformer-encoder">Transformer Encoder</a></h3>
<p><img src="https://i.imgur.com/Bd6mOCq.png" alt="" /></p>
<ul>
<li>Has residuals of based inputs</li>
<li>What is the Positional Encoding</li>
</ul>
<p><img src="https://i.imgur.com/FmYWjJH.png" alt="" /></p>
<p><img src="https://i.imgur.com/0dTtfWA.png" alt="" /></p>
<ul>
<li>Why does it mix?</li>
<li>From words and has positional encoding</li>
</ul>
<h3><a class="header" href="#multi-headed-attention" id="multi-headed-attention">Multi-Headed Attention</a></h3>
<p><img src="https://i.imgur.com/LgGrwEe.png" alt="" /></p>
<ul>
<li>Attention strength (sum) from different words</li>
</ul>
<p><img src="https://i.imgur.com/G490xoJ.png" alt="" /></p>
<ul>
<li>Self Attention (?)</li>
<li>What are the colors of the lines and on the words (?)</li>
</ul>
<h3><a class="header" href="#transformer-decoding" id="transformer-decoding">Transformer Decoding</a></h3>
<p><img src="https://i.imgur.com/J7O9jOG.png" alt="" /></p>
<ul>
<li>Decoding is to generate text</li>
<li>Attention vs Self-Attention (?)</li>
<li>What does the direction of the arrow mean?</li>
</ul>
<p><img src="https://i.imgur.com/IVhfqGJ.png" alt="" /></p>
<ul>
<li>Green is (causal/masked) self-attention</li>
</ul>
<p><img src="https://i.imgur.com/NAR824z.png" alt="" /></p>
<ul>
<li>At training time can do in parallel</li>
</ul>
<p><img src="https://i.imgur.com/7byCbh7.png" alt="" /></p>
<ul>
<li>N-Best Transformers, k copy of your output word
<ul>
<li>renard with 0.3 confidence</li>
<li>canard with 0.1 confidence</li>
</ul>
</li>
</ul>
<h3><a class="header" href="#transformer-position-encoding" id="transformer-position-encoding">Transformer Position Encoding</a></h3>
<p><img src="https://i.imgur.com/vfl5qkP.png" alt="" /></p>
<ul>
<li>The encoding doesn't have any ordering</li>
<li>Position is a sinusoid?</li>
</ul>
<p><img src="https://i.imgur.com/AguYmZQ.png" alt="" /></p>
<ul>
<li>If your vector is even or odd
<ul>
<li>Learnable shifting relative displacement</li>
<li>The inner product measures relative displacement</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/l2zCU1J.png" alt="" /></p>
<p><img src="https://i.imgur.com/oWqORx5.png" alt="" /></p>
<ul>
<li>linear combination that is strongest at position (idk)</li>
</ul>
<h2><a class="header" href="#tokenization-challenges" id="tokenization-challenges">Tokenization Challenges</a></h2>
<p><img src="https://i.imgur.com/zItfCEk.png" alt="" /></p>
<ul>
<li>new word like Starliner? can use <code>UNK</code> char</li>
</ul>
<p><img src="https://i.imgur.com/twmby69.png" alt="" /></p>
<ul>
<li>Break word down <code>##liner</code> from <code>starliner</code></li>
</ul>
<p><img src="https://i.imgur.com/OFg3afu.png" alt="" /></p>
<ul>
<li>Small vocab</li>
<li>No <code>UNK</code> tokens</li>
</ul>
<p><img src="https://i.imgur.com/Qz0dYAU.png" alt="" /></p>
<ul>
<li>Summarization, quadratic terms</li>
</ul>
<p><img src="https://i.imgur.com/oauz9sr.png" alt="" /></p>
<ul>
<li>He mixed up M and N. Don't need <code>N^2</code> term, small <code>M^2</code> term</li>
</ul>
<h2><a class="header" href="#bert-bidirectional-encoder-representations-from-transformers" id="bert-bidirectional-encoder-representations-from-transformers">Bert (Bidirectional Encoder Representations from Transformers)</a></h2>
<p><img src="https://i.imgur.com/Lkc4KuK.png" alt="" /></p>
<ul>
<li>Language model. Text is like a label, so it is like self labeling! Predict next word, previous word, etc.</li>
<li>Bert is a encoder model, bi-directional</li>
<li>GPT is a decoder model</li>
</ul>
<p><img src="https://i.imgur.com/oYIMVEN.png" alt="" /></p>
<ul>
<li>No encoder, no cross attention (?)</li>
</ul>
<p><img src="https://i.imgur.com/aNnUldP.png" alt="" /></p>
<ul>
<li>Minimize language modeling loss</li>
</ul>
<p><img src="https://i.imgur.com/0mgUX62.png" alt="" /></p>
<ul>
<li>Apply GPT to:
<ul>
<li>classification (what kind of speech)</li>
<li>Entailment (a implies b, contraction, independent)</li>
<li>Similarity has both orders</li>
<li>Multiple Choice, run it multiple times!</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/qVblTh7.png" alt="" /></p>
<ul>
<li>More data, larger models keep getting better on performance!</li>
</ul>
<h2><a class="header" href="#summary" id="summary">Summary</a></h2>
<p><img src="https://i.imgur.com/gyL8pqy.png" alt="" /></p>
<h1><a class="header" href="#other-resources" id="other-resources">Other Resources</a></h1>
<h1><a class="header" href="#visualizing-seq2seq-with-attention" id="visualizing-seq2seq-with-attention">Visualizing Seq2Seq with Attention</a></h1>
<p><a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Link</a></p>
<h2><a class="header" href="#review-of-visual-attention" id="review-of-visual-attention">Review of Visual Attention</a></h2>
<p><img src="https://i.imgur.com/GpIFcPF.png" alt="" /></p>
<ul>
<li>Have Image (top) and location <code>l_t-1</code> fed into <code>f_g</code> glimpse network.</li>
<li>This is fed into a recurrent network <code>f_h</code></li>
<li>The output hidden state is fed into two networks
<ul>
<li><code>f_a</code> activation network which does the prediction</li>
<li><code>f_l</code> location network which predicts the next location <code>l_t</code>. In Soft Attention this is done with a saliency map.</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/3lZj6UK.png" alt="" /></p>
<ul>
<li>The model has an <strong>encoder</strong> and <strong>decoder</strong></li>
<li><strong>Context</strong> is transfered from the encoder to the decoder
<ul>
<li>Context is a vector of floats. It is a hidden state in a RNN</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/IVCpWGs.png" alt="" /></p>
<ul>
<li>The words are embedded as vectors using a <strong>word embedding</strong> algorithm.</li>
</ul>
<h3><a class="header" href="#at-attention" id="at-attention">At Attention!</a></h3>
<ul>
<li><strong>Attention</strong> is the idea some words are more important than others when getting the translation
<ul>
<li>Café to coffee</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/VviBRGt.png" alt="" /></p>
<ul>
<li>Instead of one hidden state, pass all the hidden states!</li>
</ul>
<p><img src="https://i.imgur.com/R0V8kgM.png" alt="" /></p>
<ul>
<li>Use the decoder hidden state to score encoder hidden states. This is  <strong>attention</strong>.</li>
</ul>
<p><img src="https://i.imgur.com/a012seQ.png" alt="" /></p>
<ul>
<li><code>h_4</code> : hidden state and <code>c_4</code> : context state are concated and the output word comes out</li>
<li>Where does the scoring happen though?</li>
</ul>
<h2><a class="header" href="#from-lecture" id="from-lecture">From Lecture</a></h2>
<p><img src="https://i.imgur.com/ZETrFIt.png" alt="" /></p>
<ul>
<li>In lecture, it is weighed alignment scores</li>
<li>alignment score: \( \text{score}(\boldsymbol{s}_t, \boldsymbol{h}_i) = \mathbf{v}_a^\top \tanh(\mathbf{W}_a[\boldsymbol{s}_t; \boldsymbol{h}_i]) \)
<ul>
<li>\( \mathbf{v}_a \) and \( \mathbf{W}_a \) are weights that can be learned</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/RFYS1Xt.png" alt="" /></p>
<ul>
<li>Combine current recurrent state and all input states into attention weight (soft-attention) of input states. The attention weight adds up to 1, and is done with a softmax. </li>
</ul>
<h1><a class="header" href="#visualizing-transformers" id="visualizing-transformers">Visualizing Transformers</a></h1>
<p><a href="http://jalammar.github.io/illustrated-transformer/">Link</a></p>
<h2><a class="header" href="#high-level" id="high-level">High Level</a></h2>
<p><img src="https://i.imgur.com/EjK8hNc.png" alt="" /></p>
<ul>
<li>In the transformer black box we have an <strong>encoder</strong> and <strong>decoder</strong></li>
</ul>
<p><img src="https://i.imgur.com/DN6ElwE.png" alt="" /></p>
<ul>
<li>The encoders are stacked on top of each other.</li>
</ul>
<h3><a class="header" href="#encoders" id="encoders">Encoders</a></h3>
<p><img src="https://i.imgur.com/NZ8mlKa.png" alt="" /></p>
<ul>
<li>The encoder block is made of a <strong>self-attention</strong> layer and <strong>feed-forward network</strong>
<ul>
<li>self-attention helps the encoder look at words in the input sentence as it encodes a specific word</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/7W5GDqK.png" alt="" /></p>
<ul>
<li>In NLP, each word is embedded as a vector</li>
<li>list of word vectors is a tensor</li>
</ul>
<p><img src="https://i.imgur.com/E6136r3.png" alt="" /></p>
<p><img src="https://i.imgur.com/8jThN0h.png" alt="" /></p>
<h3><a class="header" href="#self-attention" id="self-attention">Self-Attention</a></h3>
<ul>
<li>The feed-forward layer is actually one network that is reused by each \( Z_i \) encoded vector. That is like a convolution</li>
</ul>
<p><img src="https://i.imgur.com/Vqn2VSb.png" alt="" /></p>
<ul>
<li>Say you have the sentence &quot;The animal didn't cross the street because it was too tired.&quot; What does <strong>it</strong> refer to? In this case, &quot;The animal.&quot; A self-attention layer lets the network learn this representation.</li>
</ul>
<p><img src="https://i.imgur.com/unpaPfS.png" alt="" /></p>
<ul>
<li>We have the embedded words (green \( X_i \))</li>
<li>For each word, we want to get out a <strong>queries</strong> vector, a <strong>keys</strong> vector, and a <strong>values</strong> vector. We have learnable weights \( W^Q \), \( W^K \), and \( W^V \).</li>
<li>The matrix multiplication of the green embedded word and the queries matrix, keys matrix, and values matrix create the queries vector, keys vector, and values vector for each word.</li>
</ul>
<p><img src="https://i.imgur.com/jsas4BF.png" alt="" /></p>
<ul>
<li>For each word, we calculate a score using the queries vector and keys vector.</li>
</ul>
<p><img src="https://i.imgur.com/LpzehcO.png" alt="" /></p>
<ul>
<li>We divide by 8 (sqrt of dim of keys vector for stable gradients) and take the softmax over all words (in a batch? in the corpus?)
<ul>
<li>(self-attention seems to be over a batch)</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/o2gTn7S.png" alt="" /></p>
<ul>
<li>We multiply the softmax by the value and sum up all the weighted value vector \( V_i \)
<ul>
<li>(Does this mean all \( Z_i \) are equal though?)</li>
</ul>
</li>
</ul>
<h3><a class="header" href="#self-attention-as-matrices" id="self-attention-as-matrices">Self-Attention as Matrices</a></h3>
<p><img src="https://i.imgur.com/CmSRI5r.png" alt="" /></p>
<ul>
<li>Batch of two words, embedded in a vector of size 2.</li>
<li>(Each weight matrix is the same size though?)</li>
</ul>
<p><img src="https://i.imgur.com/eVmtrMH.png" alt="" /></p>
<ul>
<li>Q, K, and V are calculated from X (They are NOT the weight matrices!)</li>
<li>Their operation can be condensed into one formula</li>
</ul>
<h3><a class="header" href="#multi-head-attention" id="multi-head-attention">Multi-head Attention</a></h3>
<p><img src="https://i.imgur.com/63q1zFr.png" alt="" /></p>
<ul>
<li>Instead of just using one Q, K, and V weight we can use multiple!</li>
<li>This is like multiple filters in a convolution.</li>
</ul>
<p><img src="https://i.imgur.com/vaVBv2C.png" alt="" /></p>
<ul>
<li>To combine the multiple heads into a single head for the feed-forward network, we can concat all the heads \( Z_i \) and multiply it by a weight matrix \( W^O \)</li>
</ul>
<p><img src="https://i.imgur.com/vyxHEn7.png" alt="" /></p>
<ul>
<li>Each step shown together
<ul>
<li>The input to self-attention does not have to be a batch of words (green matrix \( X \)) but it can be the output of an encoder below (blue matrix \( R \)). Encoders are stacked.</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/7RDKhZK.png" alt="" /></p>
<ul>
<li>At the top is a bunch of colors. They represent heads. The blue head for the word &quot;it_&quot; has attention at &quot;street&quot;, &quot;'<em>&quot;, and &quot;because&quot;. The green head has attention at &quot;tire&quot;, &quot;d</em>&quot;, and &quot;was_&quot;.</li>
</ul>
<h3><a class="header" href="#positional-encoding" id="positional-encoding">Positional Encoding</a></h3>
<p><img src="https://i.imgur.com/6nL9QBu.png" alt="" /></p>
<ul>
<li>To encode position, we add positional encoding with our embedded word vectors</li>
</ul>
<p><img src="https://i.imgur.com/KJy5mug.png" alt="" /></p>
<ul>
<li>The encoding is done with sin and cosine. Not much detail is added (@TODO go back to this in the lecture)</li>
</ul>
<h3><a class="header" href="#residuals-in-the-encoder" id="residuals-in-the-encoder">Residuals in the Encoder</a></h3>
<p><img src="https://i.imgur.com/ObpOAHt.png" alt="" /></p>
<ul>
<li>Like ResNet, Residuals are useful in Transformers.</li>
</ul>
<p><img src="https://i.imgur.com/856SQJe.png" alt="" /></p>
<ul>
<li>The pre-attention vectors and post-attention vectors are added together and normalized like batch norm</li>
</ul>
<h2><a class="header" href="#decoder" id="decoder">Decoder</a></h2>
<p><img src="https://i.imgur.com/ZttEnYU.png" alt="" /></p>
<ul>
<li>The output vectors of the encoder are transformed into Key and Value matrices (not weights?) in the Decoder sequence</li>
<li>The Decoder has to create it's own Queries matrix</li>
<li>They are used by each Decoder (why?)</li>
</ul>
<p><img src="https://i.imgur.com/4O6WuNr.png" alt="" /></p>
<ul>
<li>The previous input is fed into the decoder. The positional encoding is added again to this output</li>
</ul>
<p><img src="https://i.imgur.com/51kvlQj.png" alt="" /></p>
<ul>
<li>The output embedded vector is matrix mutliplied to the vocab_size. It goes through a softmax, where the large index is the chosen word.</li>
</ul>
<h2><a class="header" href="#training" id="training">Training</a></h2>
<p><img src="https://i.imgur.com/vlzyFKr.png" alt="" /></p>
<ul>
<li>The data is matrix of batch size times one hot encoding and the true value is the same.</li>
<li>In training we can keep the top k best predictions at each word. And feed best next words into the decoder. This is <strong>beam search</strong></li>
</ul>
<h1><a class="header" href="#homeworks" id="homeworks">Homeworks</a></h1>
<h1><a class="header" href="#homework-2-rnn-and-lstm" id="homework-2-rnn-and-lstm">Homework 2 RNN and LSTM</a></h1>
<ul>
<li><a href="https://bcourses.berkeley.edu/courses/1487769/pages/assignment-2-description">Link</a></li>
<li><a href="https://briantliao.com/store/cs282-lectures-slides/lec09.pdf">RNN and LSTM lecture slides</a></li>
</ul>
<h2><a class="header" href="#setup" id="setup">Setup</a></h2>
<pre><code class="language-sh">conda install &quot;tensorflow&lt;2.0&quot;          # cpu version
conda install &quot;tensorflow-gpu&lt;2.0&quot;      # gpu version
# I think I will use PyTorch though

conda create -n cs182-assignment2  
conda activate cs182-assignment2
# to deactivate:  conda deactivate

pip3 install -r requirements.txt # @Brian changed to pip3
</code></pre>
<h3><a class="header" href="#gpus" id="gpus">GPUs</a></h3>
<p>GPUs are not required for this assignment, but will help to speed up training and processing time for questions 3-4.</p>
<h3><a class="header" href="#download-data" id="download-data">Download Data</a></h3>
<pre><code class="language-sh">cd deeplearning/datasets
./get_assignment2_data.sh
</code></pre>
<p>Now you can use Jupyter Notebook <code>jupyter serve</code>!</p>
<h2><a class="header" href="#q1-image-captioning-with-vanilla-rnns-30-points" id="q1-image-captioning-with-vanilla-rnns-30-points">Q1: Image Captioning with Vanilla RNNs (30 points)</a></h2>
<p><img src="https://i.imgur.com/dysQhPY.png" alt="" /></p>
<ul>
<li>RNN Equation</li>
</ul>
<p>b included before tanh:
<code>sum_together = dot_x + dot_h + b</code></p>
<h2><a class="header" href="#q2-image-captioning-with-lstms-30-points" id="q2-image-captioning-with-lstms-30-points">Q2: Image Captioning with LSTMs (30 points)</a></h2>
<ul>
<li>\( \odot \) is the elementwise product of vectors.</li>
<li><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTMs</a></li>
</ul>
<p><img src="https://i.imgur.com/5LFX2M2.png" alt="" /></p>
<ul>
<li>RNN</li>
</ul>
<p><img src="https://i.imgur.com/OHAy8uM.png" alt="" /></p>
<ul>
<li>LSTM</li>
</ul>
<p>LSTM backwards</p>
<h2><a class="header" href="#q3-network-visualization-saliency-maps-class-visualization-and-fooling-images-20-points" id="q3-network-visualization-saliency-maps-class-visualization-and-fooling-images-20-points">Q3: Network Visualization: Saliency maps, Class Visualization, and Fooling Images (20 points)</a></h2>
<h3><a class="header" href="#saliency" id="saliency">Saliency</a></h3>
<p><img src="https://i.imgur.com/agqsFrM.png" alt="" /></p>
<ul>
<li>Which pixel has the most effect on the input</li>
</ul>
<p><img src="https://i.imgur.com/a1XejED.png" alt="" /></p>
<ul>
<li>
<p>A <strong>saliency map</strong> tells us the degree to which each pixel in the image affects the classification score for that image. To compute it, we compute the gradient of the unnormalized score (?) corresponding to the correct class (which is a scalar) (why is this a scalar?) with respect to the pixels of the image.</p>
</li>
<li>
<p>If the image has shape (3, H, W) then this gradient will also have shape (3, H, W); for each pixel in the image, this gradient tells us the amount by which the classification score will change if the pixel changes by a small amount. </p>
</li>
<li>
<p>To compute the saliency map, we take the absolute value of this gradient, then take the maximum value over the 3 input channels; the final saliency map thus has shape (H, W) and all entries are nonnegative.</p>
</li>
<li>
<p>@Brian: gradient of what? Max of what?</p>
</li>
<li>
<p><a href="http://pytorch.org/docs/torch.html#torch.gather">Pytorch Gather</a></p>
</li>
<li>
<p><code>s.gather(1, y.view(-1, 1)).squeeze()</code></p>
<ul>
<li>turns out to be like a loss. Cross entropy <code>mean()</code></li>
</ul>
</li>
</ul>
<h4><a class="header" href="#fooling-network" id="fooling-network">Fooling Network</a></h4>
<ul>
<li><code>torch.Tensor.data</code> and <code>torch.Tensor.grad.data</code></li>
<li>do not update <code>torch.Tensor += torch.Tensor</code> when we are returning a copy, <code>torch.Tensor.copy()</code></li>
<li>gradient ascent <code>torch.Tensor += torch.Tensor</code> wild</li>
<li>we calculate our own loss????</li>
</ul>
<h2><a class="header" href="#q4-style-transfer-20-points" id="q4-style-transfer-20-points">Q4: Style Transfer (20 points)</a></h2>
<h2><a class="header" href="#debugging" id="debugging">Debugging</a></h2>
<p>List Conda Enviornments:</p>
<pre><code>conda env list
</code></pre>
<p>Trying without conda env.</p>
<pre><code>pip install scipy==1.1.0
</code></pre>
<h2><a class="header" href="#tips-and-notes" id="tips-and-notes">Tips and Notes</a></h2>
<h3><a class="header" href="#numpy" id="numpy">Numpy</a></h3>
<pre><code class="language-python">np.zeros_like( another numpy array )

A.shape # (3, 4)
B.shape # (4, 4)
np.dot(A, B).shape # (3, 4)

np.zeros((dim1, dim2, dim3))

 x_pad[n,:,y_padded:y_padded+HH,x_padded:x_padded+WW]

 x[0,0,0] # indexes a single value
 x[0:5, 1:4, 6:8] # slices a tensor

x[-1] == x[len(x) - 1] 

# let x in 10 elements, index 0 to 9
for i in range(x - 1, -1, -1):
    print(i)
# 9, 8, 7 ... 2, 1, 0

</code></pre>
<h3><a class="header" href="#calculus" id="calculus">Calculus</a></h3>
<h3><a class="header" href="#chain-rule" id="chain-rule">Chain Rule:</a></h3>
<p>$$ \frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \frac{\partial y}{\partial x} $$</p>
<p>Same for \( \frac{\partial L}{\partial b} \) and \( \frac{\partial L}{\partial W} \).</p>
<p>We have \( \frac{\partial L}{\partial y} \) and we can solve for \( \frac{\partial y}{\partial y} \) locally. This may need values cached from the forward pass.</p>
<h1><a class="header" href="#exam-practice" id="exam-practice">Exam Practice</a></h1>
<h1><a class="header" href="#midterm-1" id="midterm-1">Midterm 1</a></h1>
<h1><a class="header" href="#midterm-1-spring-2019" id="midterm-1-spring-2019">Midterm 1 Spring 2019</a></h1>
<p><img src="https://i.imgur.com/0WqLprF.png" alt="" /></p>
<ol>
<li>
<p>a.</p>
<ul>
<li><strong>Ans:</strong> AlexNet is deeper. AlexNet is split on GPUs.</li>
<li><strong>Corrections:</strong> AlexNet uses ReLU, LeNet uses Sigmoid</li>
<li><strong>Review:</strong> <code>AlexNet and LeNet</code></li>
</ul>
<p>b. </p>
<ul>
<li><strong>Ans:</strong> Multi-task learning is you replace the end layers of a network including the classifier, and can connect it to other tasks like question and answering. This makes it more robust to learn representations in images.</li>
<li><strong>Corrections:</strong> Layers are shared in a network (shared representation). The network is applied to multiple tasks. Benefits are: the network has extra information to capture the essence of a task, and avoids overfitting by learning more robust features.</li>
<li><strong>Review:</strong> <code>multi-task learning</code></li>
</ul>
<p>c.</p>
<ul>
<li><strong>Ans:</strong> Generative Models try to generate the joint distribution \( P(B,A) \). Discriminative models try to model only the decision boundary.</li>
<li><strong>Corrections:</strong> generate is &quot;sample&quot; data from distribution. Can't say it models \( P(X, Y) \). Discriminative doesn't need assumptions aboutd data, can model more complex distributions.</li>
<li><strong>Review:</strong> <code>Generative vs Discriminative Models</code></li>
</ul>
</li>
</ol>
<p><img src="https://i.imgur.com/mZLgVpH.png" alt="" /></p>
<ol>
<li>
<p>d.</p>
<ul>
<li><strong>Ans:</strong> Cross Entropy is used for discrete values</li>
<li><strong>Corrections:</strong> Cross entropy loss (which is log loss in the binary case)</li>
<li><strong>Review:</strong> <code>Squared Error Loss and Cross Entropy Loss. Review Loss vs. Risk</code></li>
</ul>
<p>e.</p>
<ul>
<li><strong>Ans:</strong> (rip)</li>
<li><strong>Corrections:</strong> Loss gradients push the boundary to move points outside the margin. Close points have very large gradients.</li>
<li><strong>Review:</strong> <code>Logistic Regression, its training algorithm</code></li>
</ul>
</li>
</ol>
<p><img src="https://i.imgur.com/ZoKIsmv.png" alt="" /></p>
<ol>
<li>
<p>f.</p>
<ul>
<li><strong>Ans:</strong> Bias is how accurate the model is on learning the relationship between x and y. Variance is how accurate the model is to data it has not seen. Deep neural networks have high variance and low bias.</li>
<li><strong>Corrections:</strong> Complex models can fit data better (low bias) but are more sensitive to data (high variance) Regularization reduces variance at expense for bias. Deep neural networks are low-bias, high variance</li>
<li><strong>Review</strong>: <code>bias variance tradeoff, regularization, deep networks are low bias high variance</code></li>
</ul>
<p>g.</p>
<ul>
<li><strong>Ans:</strong> When each variable is independent. Logistic regression is more accurate when this is not the case and there is more data.</li>
<li><strong>Corrections:</strong> Naive Bayes assumes feature values are conditionally independent based on class label. Logistic regression doesn't make that assumption and is more accurate when that is not the case.</li>
<li><strong>Review</strong>: <code>Naive Bayes, Conditional Independence on class label, Logistic Regression.</code></li>
</ul>
</li>
</ol>
<p><img src="https://i.imgur.com/h77LjNU.png" alt="" /></p>
<ol>
<li>
<p>h.</p>
<ul>
<li><strong>Review:</strong> Gradients vanishes at local minima, maxima and saddle points</li>
<li><strong>Corrections:</strong></li>
<li><strong>Review:</strong> <code>Vanishing Gradients</code></li>
</ul>
<p>i.</p>
<ul>
<li><strong>Review:</strong> \( 128 \times 64 \times 64 \)</li>
<li><strong>Corrections:</strong></li>
<li><strong>Review:</strong> <code>Convolution Equation</code></li>
</ul>
<p>j.</p>
<ul>
<li><strong>Review:</strong> Split dataset into <code>k+1</code> subsets taking one out for testing. On round <code>i</code>, hold <code>i</code> for validation and and the rest of the <code>k</code> for training. Repeat with different holdout sets.</li>
<li><strong>Corrections:</strong> Check on fold <code>i</code> (validation data)</li>
<li><strong>Review:</strong> <code>Cross Validation</code></li>
</ul>
</li>
</ol>
<h1><a class="header" href="#midterm-1-practice-1-spring-2018" id="midterm-1-practice-1-spring-2018">Midterm 1 Practice 1 Spring 2018</a></h1>
<p><img src="https://i.imgur.com/v1k6bSW.png" alt="" /></p>
<ol>
<li>
<p>a. Objects can be seen from different perspectives. Lighting can be different. Objects can be blocked; object partially visible.</p>
<ul>
<li><strong>Review:</strong> <code>Lecture 1.</code></li>
</ul>
<p>b. When doing transfer learning, the network has already extracted high level like edges, mid level like shapes and lines, and low level like faces. These generalize and don't have to be updated too much in being fine tuned. <strong>The already trained low-dimensional features capture the essence of the data, can have faster fine-tuning.</strong></p>
<ul>
<li><strong>Review:</strong> <code>Lecture 1, Transfer Learning.</code></li>
</ul>
<p>c. Expected Risk is <strong>expectation</strong> over all datasets of model loss (prediction – actual result). Empirical risk is difference over a fixed dataset sample. Machine Learning minimizes empirical risk.</p>
<ul>
<li><strong>Review:</strong> <code>Expected Risk, Empirical Risk, Loss vs Risk.</code></li>
</ul>
</li>
</ol>
<p><img src="https://i.imgur.com/Z5g6JuI.png" alt="" /></p>
<ol>
<li>
<p>d. L2 is designed for loss of continuous value functions. Logistic regression is for binary classification (0 or 1). We use binary cross-entropy loss. (It encourages outputs to be 0 or 1?) The loss becomes a probability?</p>
<ul>
<li><strong>Review:</strong> <code>L2 Loss vs Cross-Entropy Loss</code></li>
<li>Why does it work for binary/discrete classification vs L2 for continuous? What is the relationship of it to probability?</li>
</ul>
<p>e. Newton's second-order method converge to local minima and saddle points. Where second derivative gradients (is this correct?) are zero</p>
<ul>
<li><strong>Review:</strong> <code>Netwon Second-Order Methods</code></li>
</ul>
<p>f. Using a max-margin classifier is the most robust (decreases overfitting) classifier to unseen data. It maximizes distance of the nearest points to the margin/decision boundary. Diagram shows decision boundary with max distance to nearest points</p>
<ul>
<li><strong>Review:</strong> <code>SVM, Max-Margin, Hinge Loss</code></li>
</ul>
</li>
</ol>
<p><img src="https://i.imgur.com/ray5suc.png" alt="" /></p>
<ol>
<li>g. for each data point \( (x_i, y_i) \), we have \( f_{y_i}(x_i) - f_j(x_i) \) (the difference) for all classes \( j \neq y \). The original SVM loss is \( max(0, 1 - yw^Tx) \). Max margins for all classes not \( j \) is \( max(0, 1 - f_{y_i}(x_i) + f_j(x_i) \). Loss is sum/averaged. OvA classifer? What the heck is this?
<ul>
<li><strong>Review:</strong> <code>SVM</code></li>
</ul>
</li>
</ol>
<p><img src="https://i.imgur.com/yCRut4F.png" alt="" /></p>
<ol>
<li>
<p>h. Multiclass logistic regression can learn the multiclass naive Bayes</p>
<ul>
<li><strong>Review:</strong> <code>Multiclass Logistic Regression, Multiclass Naive Bayes, their relationship</code></li>
</ul>
<p>i. Gradient decreases proportionally to accumulating value at \( \approx \frac{1}{\sqrt{t}}\). This is because the denominator of the update contains sum of squares of past gradients.</p>
</li>
</ol>
<p><img src="https://i.imgur.com/0rGE4yp.png" alt="" /></p>
<ol>
<li>
<p>j. </p>
<ul>
<li><strong>Ans:</strong> Local optima were usually saddle points. SGD performs well because it does not just follow the gradient, so it is likely to fall off a saddle point.</li>
<li><strong>Corrections:</strong> </li>
<li><strong>Review:</strong> <code>loss landscape and convexity, SGD</code> </li>
</ul>
<p>k.</p>
<ul>
<li><strong>Ans:</strong> Depends on the stride and padding. if stride 1 padding 0,  \( 194 \times 194 \times 1\)</li>
<li><strong>Corrections:</strong> </li>
<li><strong>Review:</strong> <code>Convolution reshape equation</code> </li>
</ul>
<p>l.</p>
<ul>
<li><strong>Ans:</strong> They extract features in images. These features can be used for more robust representations of images.</li>
<li><strong>Corrections:</strong> </li>
<li><strong>Review:</strong> <code>Convolutions, Feature Detection</code> </li>
</ul>
<p>m. </p>
<ul>
<li><strong>Ans:</strong> Expectation of dropout is <code>output * p</code>. We can use the expectation of dropout in inference then, but also just use the <code>output</code> by doing dropout at training time (with a mask where each index has probability <code>p</code> being 1 (retained) and dividing by <code>p</code>.</li>
<li><strong>Corrections:</strong> </li>
<li><strong>Review:</strong> <code>Dropout</code></li>
</ul>
<p>n. </p>
<ul>
<li><strong>Ans:</strong> Prediction Averaging reduces variance. Each model is robust to different relationships (learn different things). Parameter averaging canceling out the relationships each model learns. Snapshot parameter ensembling works because the relationships the model was learning are close to the same, while still reducing variance.</li>
<li><strong>Corrections:</strong> </li>
<li><strong>Review:</strong> <code>Prediction Averaging, Parameter Averaging, Snapshot Ensembling</code></li>
</ul>
<p>o. </p>
<ul>
<li><strong>Ans:</strong> x -&gt; [] (array with h wraps into block) -&gt; y</li>
<li><strong>Corrections:</strong> </li>
<li><strong>Review:</strong> <code>RNN</code></li>
</ul>
<p>p. </p>
<ul>
<li><strong>Ans:</strong> <code>y = A_yh * tanh(A_hx * x + A_hh * h)</code></li>
<li><strong>Corrections:</strong> </li>
<li><strong>Review:</strong> <code>RNN equation</code></li>
</ul>
<p>q. </p>
<ul>
<li><strong>Ans:</strong> Remember and Forget?</li>
<li><strong>Corrections:</strong> </li>
<li><strong>Review:</strong> <code>LSTM equation and interpretation</code></li>
</ul>
</li>
</ol>
<h1><a class="header" href="#midterm-1-cheat-sheet" id="midterm-1-cheat-sheet">Midterm 1 Cheat Sheet</a></h1>
<p><img src="https://i.imgur.com/5fKIV3n.jpg" alt="" /></p>
<p><img src="https://i.imgur.com/Ch56t9P.jpg" alt="" /></p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        

        
        
        
        <script type="text/javascript">
            window.playpen_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>
        
        

    </body>
</html>
