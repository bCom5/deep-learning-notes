<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js ayu">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>CS 282: Deep Learning Notes Spring 2020</title>
        
        <meta name="robots" content="noindex" />
        

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">
        <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:500" rel="stylesheet" type="text/css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "ayu" : "ayu";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('ayu')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div id="sidebar-scrollbox" class="sidebar-scrollbox">
                <ol class="chapter"><li class="expanded "><a href="introduction.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="expanded "><a href="lectures/lectures.html"><strong aria-hidden="true">2.</strong> Lectures</a></li><li><ol class="section"><li class="expanded "><a href="lectures/lec09_rnn.html"><strong aria-hidden="true">2.1.</strong> Recurrent Networks, LSTMS, and Applications</a></li><li class="expanded "><a href="lectures/lec10_visual.html"><strong aria-hidden="true">2.2.</strong> Visualizing Deep Networks</a></li><li class="expanded "><a href="lectures/lec11_attention.html"><strong aria-hidden="true">2.3.</strong> Attention Networks</a></li><li class="expanded "><a href="lectures/lec12_text_semantics.html"><strong aria-hidden="true">2.4.</strong> Text Semantics</a></li><li class="expanded "><a href="lectures/lec13_translation.html"><strong aria-hidden="true">2.5.</strong> Translation</a></li><li class="expanded "><a href="lectures/lec14_transformers.html"><strong aria-hidden="true">2.6.</strong> Transformers and Pre-Training</a></li><li class="expanded "><a href="lectures/lec15_nlp_applications.html"><strong aria-hidden="true">2.7.</strong> NLP Applications</a></li><li class="expanded "><a href="lectures/lec16_generative_models.html"><strong aria-hidden="true">2.8.</strong> Generative Models</a></li><li class="expanded "><a href="lectures/lec17_gan.html"><strong aria-hidden="true">2.9.</strong> Generative Adversarial Networks</a></li></ol></li><li class="expanded "><a href="other_resources/other_resources.html"><strong aria-hidden="true">3.</strong> Other Resources</a></li><li><ol class="section"><li class="expanded "><a href="other_resources/viz_seq.html"><strong aria-hidden="true">3.1.</strong> Visualizing Seq2Seq with Attention</a></li><li class="expanded "><a href="other_resources/viz_transformers.html"><strong aria-hidden="true">3.2.</strong> Visualizing Transformers</a></li><li class="expanded "><a href="other_resources/viz_bert_elmo_gpt.html"><strong aria-hidden="true">3.3.</strong> Visualizing BERT, ELMo, and GPT</a></li></ol></li><li class="expanded "><a href="homework/homework.html"><strong aria-hidden="true">4.</strong> Homeworks</a></li><li><ol class="section"><li class="expanded "><a href="homework/hw2_rnn_lstm.html"><strong aria-hidden="true">4.1.</strong> Homework 2 RNN and LSTM</a></li><li class="expanded "><a href="homework/hw3_nlp.html"><strong aria-hidden="true">4.2.</strong> Homework 3 Natural Language Processing</a></li></ol></li><li class="expanded "><a href="project/project.html"><strong aria-hidden="true">5.</strong> Final Project: Model Distillation Low Precision Neural Networks</a></li><li><ol class="section"><li class="expanded "><a href="project/xnor-net.html"><strong aria-hidden="true">5.1.</strong> XNOR-net</a></li><li class="expanded "><a href="project/knowledge_distill.html"><strong aria-hidden="true">5.2.</strong> Knowledge Distillation</a></li><li class="expanded "><a href="project/dorefa_net.html"><strong aria-hidden="true">5.3.</strong> DoReFa-Net</a></li><li class="expanded "><a href="project/apprentice_knowledge_distill.html"><strong aria-hidden="true">5.4.</strong> Apprentice: Knowledge Distillation with Low-Precision Networks</a></li></ol></li><li class="expanded "><a href="exam/exam.html"><strong aria-hidden="true">6.</strong> Exam Practice</a></li><li><ol class="section"><li class="expanded "><a href="exam/midterm1/midterm1.html"><strong aria-hidden="true">6.1.</strong> Midterm 1</a></li><li><ol class="section"><li class="expanded "><a href="exam/midterm1/mt1sp19.html"><strong aria-hidden="true">6.1.1.</strong> Midterm 1 Spring 2019</a></li><li class="expanded "><a href="exam/midterm1/mt1prac1sp18.html"><strong aria-hidden="true">6.1.2.</strong> Midterm 1 Practice 1 Spring 2018</a></li><li class="expanded "><a href="exam/midterm1/cheatsheet.html"><strong aria-hidden="true">6.1.3.</strong> Midterm 1 Cheat Sheet</a></li></ol></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar" class="menu-bar">
                    <div id="menu-bar-sticky-container">
                        <div class="left-buttons">
                            <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                                <i class="fa fa-bars"></i>
                            </button>
                            <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                                <i class="fa fa-paint-brush"></i>
                            </button>
                            <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                                <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu (default)</button></li>
                            </ul>
                            
                            <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                                <i class="fa fa-search"></i>
                            </button>
                            
                        </div>

                        <h1 class="menu-title">CS 282: Deep Learning Notes Spring 2020</h1>

                        <div class="right-buttons">
                            <a href="print.html" title="Print this book" aria-label="Print this book">
                                <i id="print-button" class="fa fa-print"></i>
                            </a>
                            
                        </div>
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1><a class="header" href="#introduction" id="introduction">Introduction</a></h1>
<p>CS 282 Deep Learning, Spring 2020 
Taught by John Canny</p>
<p><img src="https://i.imgur.com/0ztiyxz.png" alt="" /></p>
<ul>
<li><a href="https://bcourses.berkeley.edu/courses/1487769">Website</a></li>
<li><a href="https://www.youtube.com/playlist?list=PLkFD6_40KJIwaO6Eca8kzsEFBob0nFvwm">Webcast</a></li>
</ul>
<h1><a class="header" href="#lectures" id="lectures">Lectures</a></h1>
<h1><a class="header" href="#recurrent-networks-lstms-and-applications" id="recurrent-networks-lstms-and-applications">Recurrent Networks, LSTMS, and Applications</a></h1>
<h1><a class="header" href="#visualizing-deep-networks" id="visualizing-deep-networks">Visualizing Deep Networks</a></h1>
<h3><a class="header" href="#activation-maximization" id="activation-maximization">Activation Maximization</a></h3>
<p><img src="https://i.imgur.com/bKFfWQy.png" alt="" /></p>
<p>Generate a synthetic image I, normalize it L2 nrom, maxing a class. This is done through backpropogation. Class is one hot encoded <code>[0 0 1 0 0]</code>. Images are weird though.</p>
<h3><a class="header" href="#deconv-approaches" id="deconv-approaches">Deconv Approaches</a></h3>
<p><img src="https://i.imgur.com/L9W4AWf.png" alt="" /></p>
<p>To make generating synthetic images better:</p>
<ul>
<li>backprop zeros out negative values in the forward pass</li>
<li>decovnet zeros out negative gradients in the backward pass. Negative gradients are inhibitory activations (they were the wrong class)</li>
<li>guided backprop does both, zeros out both</li>
</ul>
<h3><a class="header" href="#neural-style-transfer" id="neural-style-transfer">Neural Style Transfer</a></h3>
<p><img src="https://i.imgur.com/xdURQhp.jpg" alt="" /></p>
<p>Review this!</p>
<h1><a class="header" href="#attention-networks" id="attention-networks">Attention Networks</a></h1>
<p><strong>The most important idea in deep networks this decade.</strong></p>
<h3><a class="header" href="#pilot-analogy" id="pilot-analogy">Pilot analogy</a></h3>
<p><img src="https://i.imgur.com/LU12T8h.jpg" alt="" /></p>
<p>Pilots move their focus on different things, gauges, buttons, switches</p>
<h3><a class="header" href="#papers" id="papers">Papers</a></h3>
<p>Attention is All You Need (2017) only stacked attention, previous used RNNs</p>
<h2><a class="header" href="#soft-vs-hard-attention-introduction" id="soft-vs-hard-attention-introduction">Soft vs Hard Attention Introduction</a></h2>
<p><img src="https://i.imgur.com/GwTuFpG.png" alt="" /></p>
<ul>
<li><strong>Hard Attention</strong> is what humans do. Not differentiable because attention map is 1 where focused, 0 elsewhere (discrete).</li>
<li><strong>Soft Attention</strong> - linear combination over inputs, can use backprop.
<ul>
<li>Trains network and attention to the right place!</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/3JN8VEn.png" alt="" />
<strong>Supervised Learning</strong> vs <strong>Reinforcement Learning</strong>, maximizing a reward (discrete)</p>
<p>Reinforcement Learning, the <strong>agent tries to maximize the sum of rewards over an epoch</strong></p>
<h2><a class="header" href="#attention-for-recognition" id="attention-for-recognition">Attention for Recognition</a></h2>
<p><img src="https://i.imgur.com/d2tfdX3.png" alt="" /></p>
<p>Has time stamps. Has location and the image. RNN always predicting gaze location for next time.</p>
<p><img src="https://i.imgur.com/BAmjvmm.png" alt="" /></p>
<p>Resolution when focusing on something (glimpse). Output is location and action (classify/don't classify). Inputs are on top, output is at the bottom.</p>
<p><img src="https://i.imgur.com/NnoPB1G.png" alt="" /></p>
<p>This shows glimpses. Focused in center. Blurred around. More blurred after, 3 levels. Green is location as it moves around.</p>
<h2><a class="header" href="#soft-attention-for-translation" id="soft-attention-for-translation">Soft Attention for Translation</a></h2>
<p><img src="https://i.imgur.com/ode4uKw.png" alt="" /></p>
<p>When the world is outputting <em>Me</em>, it has attention on <em>I</em>. Likewise with <em>coffee</em> and <em>cafe</em>.</p>
<p><img src="https://i.imgur.com/bWX5rXq.png" alt="" /></p>
<p>What word does each word correspond to? (Where is the attention located)?</p>
<h2><a class="header" href="#rnn-for-captioning" id="rnn-for-captioning">RNN for Captioning</a></h2>
<p><img src="https://i.imgur.com/O3QzfWP.png" alt="" /></p>
<p>Output <code>d0</code> goes into a softmax over the vocabulary. <em>Bird</em> is fed back into <code>x1</code>, next token. </p>
<p><img src="https://i.imgur.com/qA37EVl.png" alt="" /></p>
<p>Have \( L \times D \) features extracted. Our RNN outputs classification and weight location. Mask/summing (?) with the features give the weighted feature \( D \) fed back into the RNN.</p>
<h2><a class="header" href="#soft-vs-hard-attention-in-captioning" id="soft-vs-hard-attention-in-captioning">Soft vs. Hard Attention in Captioning</a></h2>
<p><img src="https://i.imgur.com/VNdA0yR.png" alt="" /></p>
<ul>
<li>Have two inputs, RNN gives probability distribution (see above) (also think about the result of softmax).</li>
<li><strong>Soft Attention</strong> sum all location and derivative is \( \frac{dz}{dp} \). Train with gradient descent.</li>
<li><strong>Hard Attention</strong> samples only one location. With argmax, \( \frac{dz}{dp} \) is almost zero everywhere. Can't use gradient descent.</li>
</ul>
<p><img src="https://i.imgur.com/RphCcoj.png" alt="" /></p>
<p>Soft is smooth around the area. Hard is discrete (1 or 0) around the image.</p>
<p><img src="https://i.imgur.com/wLcn23X.jpg" alt="" /></p>
<p><img src="https://i.imgur.com/YYT5G2u.jpg" alt="" /></p>
<p>Can see where it made mistakes!!!</p>
<h2><a class="header" href="#attention-mechanics" id="attention-mechanics">Attention Mechanics</a></h2>
<p><img src="https://i.imgur.com/0q6PVTx.png" alt="" /></p>
<ul>
<li>Image Feature output matrix \( L \times D \)</li>
<li>Attention weights output vector \( L \). Can \( L \) be an image \( H \times W \)?</li>
<li>Do a broadcast multiply - each vector in \( D \) multiplies by the Attention vector. Output is still \( L \times D \)</li>
<li>Sum the \( L \) axis (vector). Output is size \( D \).</li>
</ul>
<p><img src="https://i.imgur.com/NcPqXPf.png" alt="" /></p>
<ul>
<li>Need sum for the contributions. This is because \( B \) is broadcast multiplied to \( A \)</li>
</ul>
<h3><a class="header" href="#salience" id="salience">Salience</a></h3>
<p><img src="https://i.imgur.com/v4cwBwH.png" alt="" /></p>
<ul>
<li>Circle operation is product</li>
<li>What is salience (A *dot* )
<ul>
<li><strong>Salience:</strong> The total contribution and gradient to the output (the importance of an area)</li>
</ul>
</li>
</ul>
<h2><a class="header" href="#attention-and-lstm" id="attention-and-lstm">Attention and LSTM</a></h2>
<p><img src="https://i.imgur.com/EDcAoA5.png" alt="" /></p>
<ul>
<li>LSTMs also receive a <strong>salience gradient</strong>. There is multiplicative gating (kind of like attention)</li>
</ul>
<h2><a class="header" href="#soft-attention-for-video" id="soft-attention-for-video">Soft Attention for Video</a></h2>
<p><img src="https://i.imgur.com/wHL1iwj.png" alt="" /></p>
<p>Now temporal (time) can have attention. (Which frame is important?)</p>
<p><img src="https://i.imgur.com/lo2rgUy.png" alt="" /></p>
<ul>
<li>Can have spatial and temporal attention. Temporal in this network.</li>
<li>Create a vector of feature frames.</li>
</ul>
<p><img src="https://i.imgur.com/uvLw16A.png" alt="" /></p>
<p><img src="https://i.imgur.com/iNBaAPT.png" alt="" /></p>
<p>Perplexity is diversity of word options. Goal is smaller which means the model is more confident.</p>
<h2><a class="header" href="#find-the-region-again" id="find-the-region-again">Find the Region Again</a></h2>
<p><img src="https://i.imgur.com/gVzxWMc.png" alt="" /></p>
<ul>
<li>Classify - attention to regions in input</li>
<li>Generate - draw digit</li>
</ul>
<p>Has a parameter for scale</p>
<h2><a class="header" href="#takeaways" id="takeaways">Takeaways</a></h2>
<p><img src="https://i.imgur.com/vquu8LF.png" alt="" /></p>
<ul>
<li><strong>Salience:</strong> Emphasize imporant data.</li>
</ul>
<p><img src="https://i.imgur.com/ddEkm4X.png" alt="" /></p>
<h1><a class="header" href="#text-semantics" id="text-semantics">Text Semantics</a></h1>
<p><a href="https://youtu.be/1rzjaUp6NiQ">Webcast</a></p>
<p><img src="https://i.imgur.com/7LIXXrJ.png" alt="" /></p>
<ul>
<li><strong>Propositional</strong>: logic, formal, mathematical</li>
<li><strong>Vector</strong>: represent as numbers, high dimensional space.</li>
</ul>
<p><img src="https://i.imgur.com/br0ryjb.png" alt="" /></p>
<ul>
<li>Propositional uses class (usually noun) and predicates - operation (usually a verb). <strong>Reasoning</strong></li>
<li>Word and sentences represented as vectors. Commutative! That is a problem.</li>
</ul>
<h2><a class="header" href="#vector-embedding-of-words" id="vector-embedding-of-words">Vector Embedding of Words</a></h2>
<p><img src="https://i.imgur.com/CGwXg4v.png" alt="" /></p>
<ul>
<li><strong>Bag-of-Words</strong> - sentence</li>
</ul>
<h3><a class="header" href="#word-similarity" id="word-similarity">Word Similarity</a></h3>
<p><img src="https://i.imgur.com/Vd6TRq8.png" alt="" /></p>
<ul>
<li><strong>Similar word</strong> in <strong>similar contexts</strong>. Dog and canine can be replaced.</li>
</ul>
<p><img src="https://i.imgur.com/Nqc0KTP.png" alt="" /></p>
<h3><a class="header" href="#dimension-reduction" id="dimension-reduction">Dimension Reduction</a></h3>
<p><img src="https://i.imgur.com/L54iu21.png" alt="" /></p>
<ul>
<li>Could <strong>one-hot encode</strong> every word in vocab. That's expensive.</li>
<li>Alt: counts of word used in that context. &quot;Dog barks&quot;.</li>
</ul>
<h2><a class="header" href="#dimensionality-reduction" id="dimensionality-reduction">Dimensionality Reduction</a></h2>
<p><img src="https://i.imgur.com/sd6BKtO.png" alt="" /></p>
<h3><a class="header" href="#latent-semantic-analysis" id="latent-semantic-analysis">Latent Semantic Analysis</a></h3>
<p><img src="https://i.imgur.com/YogMF8j.png" alt="" /></p>
<ul>
<li>Encode documents <code>N</code></li>
<li><code>M</code> is word count.</li>
</ul>
<p><img src="https://i.imgur.com/hXnjAO7.png" alt="" /></p>
<p>Counts words</p>
<h3><a class="header" href="#latent-semantic-analysis-1" id="latent-semantic-analysis-1">Latent Semantic Analysis</a></h3>
<p><img src="https://i.imgur.com/QEJ4hZq.png" alt="" /></p>
<ul>
<li>Low dimensional approximation using SVD. (Review SVD!)</li>
<li>Embedding is V, factors encode document contexts</li>
</ul>
<h4><a class="header" href="#singular-value-decomposition" id="singular-value-decomposition">Singular Value Decomposition</a></h4>
<p><img src="https://i.imgur.com/8vySkwT.png" alt="" /></p>
<ul>
<li>\( U, V \) <strong>orthogonal, normal</strong>. \( S \) rectangular diagonal, singular values, right padded with 0s to fit</li>
</ul>
<h4><a class="header" href="#review-how-to-calculate-svd" id="review-how-to-calculate-svd">Review How to Calculate SVD</a></h4>
<p><img src="https://i.imgur.com/VHinLAL.png" alt="" /></p>
<ul>
<li>It can be computed from eigenvalues of \( T ~ T ^T \)</li>
<li>Insides cancel out, there's an \( S^2 \) term!!!</li>
</ul>
<p><img src="https://i.imgur.com/FB3xuv1.png" alt="" /></p>
<p>Both work!</p>
<p><img src="https://i.imgur.com/2ynYgmd.png" alt="" /></p>
<ul>
<li>Can have a smaller S, using only the largest singular values (S?) </li>
<li>Small K dimension</li>
<li>High dimensional T to low dimensional </li>
<li><strong>best possible reconstruction</strong> of documents from their embedding</li>
</ul>
<p><img src="https://i.imgur.com/mWKPVXo.png" alt="" /></p>
<ul>
<li>Documents are T, Encoding is Z</li>
</ul>
<p><img src="https://i.imgur.com/0IJVSaK.png" alt="" /></p>
<ul>
<li>Have a network learn SVD</li>
<li>V is vocab times latent dimensions</li>
<li>Scalable version of LSA/SVD</li>
<li>similar things will get mapped to similar places</li>
</ul>
<h2><a class="header" href="#t-sne-word-embeddings" id="t-sne-word-embeddings">t-SNE Word Embeddings</a></h2>
<p><img src="https://i.imgur.com/tF1VGAF.png" alt="" /></p>
<h2><a class="header" href="#word2vec" id="word2vec">Word2Vec</a></h2>
<p><img src="https://i.imgur.com/LmTf2z2.png" alt="" /></p>
<ul>
<li>neighborhood of a word instead of whole document, <strong>skip-gram</strong></li>
<li>nonlinearity</li>
</ul>
<p><img src="https://i.imgur.com/GLAWU4T.png" alt="" /></p>
<ul>
<li>Predict center words from context</li>
<li>order does not matter</li>
</ul>
<p><img src="https://i.imgur.com/ZrKymNE.png" alt="" /></p>
<ul>
<li>Predict context words from center word</li>
</ul>
<p><img src="https://i.imgur.com/9sTa45d.png" alt="" /></p>
<ul>
<li>Problem of SVD is it favors minimizing large distances (min squared error). Want to preserve <strong>close</strong> distance like t-SNE</li>
</ul>
<p><img src="https://i.imgur.com/E3M523Q.png" alt="" /></p>
<ul>
<li>Canny says it's a mess</li>
</ul>
<p><img src="https://i.imgur.com/29fFUrk.png" alt="" /></p>
<ul>
<li>Holds relations as vectors! Vector math!</li>
</ul>
<p><img src="https://i.imgur.com/0WFGMaA.png" alt="" /></p>
<ul>
<li>This model uses contexts</li>
<li>City pairs, opposites, comparatives (great, greater)</li>
</ul>
<p>Criticisms:</p>
<ul>
<li>Cross-entropy emphasizes small word combinations</li>
<li>Expensive to softmax</li>
</ul>
<p><img src="https://i.imgur.com/Er3Ogvq.png" alt="" /></p>
<ul>
<li>Maybe wrong data</li>
</ul>
<p><img src="https://i.imgur.com/CwVSFNj.png" alt="" /></p>
<ul>
<li>Now words times words! Better for contextual words!</li>
<li>Window size (prob exam problem), just try it and check</li>
</ul>
<h2><a class="header" href="#glove" id="glove">GloVe</a></h2>
<p><img src="https://i.imgur.com/OkSCRaK.png" alt="" /></p>
<ul>
<li>\( C_{ij} \) num times word j in context of word i. Query the matrix
<ul>
<li>For favoring the counts</li>
</ul>
</li>
<li>\( u_i \) is embedding, \( v_j \) is context word embedding
<ul>
<li>Inner product for similarity!!!</li>
</ul>
</li>
<li>\( f \) properties make it small for close words, and not too big for unlike words in context</li>
</ul>
<p><img src="https://i.imgur.com/CJ0MNrE.png" alt="" /></p>
<h2><a class="header" href="#compositional-semantics" id="compositional-semantics">Compositional Semantics</a></h2>
<p><img src="https://i.imgur.com/NsrUYj7.png" alt="" /></p>
<ul>
<li><strong>Compositional Semantics</strong> capture meaning in the structure and ordering </li>
</ul>
<p><img src="https://i.imgur.com/H4Iio7l.png" alt="" /></p>
<h3><a class="header" href="#skip-through-vectors" id="skip-through-vectors">Skip-Through Vectors</a></h3>
<p><img src="https://i.imgur.com/3TYFBkQ.png" alt="" /></p>
<ul>
<li>Predict the previous sentence and next sentences
<ul>
<li>Fed back in</li>
</ul>
</li>
<li>RNN, each state </li>
</ul>
<p><img src="https://i.imgur.com/ozMrn5g.png" alt="" /></p>
<ul>
<li>Doesn't require backprop?</li>
</ul>
<p><img src="https://i.imgur.com/VCd42TW.png" alt="" /></p>
<ul>
<li>Human evaluation.</li>
</ul>
<p><img src="https://i.imgur.com/LS5i6O1.png" alt="" /></p>
<ul>
<li>Why not just train/optimize for similarity
<ul>
<li>Minimize Manhattan distance</li>
</ul>
</li>
</ul>
<h3><a class="header" href="#semantic-entailment-evaluation" id="semantic-entailment-evaluation">Semantic Entailment Evaluation</a></h3>
<p><img src="https://i.imgur.com/pHYzAi9.png" alt="" /></p>
<ul>
<li>Tasks are Entailment, Contradiction, Neutral</li>
</ul>
<h1><a class="header" href="#translation" id="translation">Translation</a></h1>
<ul>
<li><a href="https://briantliao.com/store/cs282-lectures-slides/lec13.pdf">Slides</a></li>
</ul>
<h2><a class="header" href="#review" id="review">Review</a></h2>
<p><img src="https://i.imgur.com/ytGyXuo.png" alt="" /></p>
<ul>
<li>Inner Product (or is it Dot Product?) is closeness</li>
</ul>
<p><img src="https://i.imgur.com/E9DMaxX.png" alt="" /></p>
<ul>
<li>Predict previous and next sentences, now ordering is included</li>
</ul>
<p><img src="https://i.imgur.com/r9NLuPf.png" alt="" /></p>
<ul>
<li>Train for your task! This is based on closeness.</li>
</ul>
<h2><a class="header" href="#translation-1" id="translation-1">Translation</a></h2>
<p><img src="https://i.imgur.com/myVDtam.png" alt="" /></p>
<h3><a class="header" href="#sequence-to-sequence" id="sequence-to-sequence">Sequence to Sequence</a></h3>
<ul>
<li><strong>Sequence-To-Sequence RNN</strong>, input fed into left, output comes out of the right</li>
</ul>
<p><img src="https://i.imgur.com/3HwFFEZ.png" alt="" /></p>
<ul>
<li>At each RNN node, the output is fed back in. Keep the n-best</li>
</ul>
<h3><a class="header" href="#bleu" id="bleu">Bleu</a></h3>
<p><img src="https://i.imgur.com/9G6bB4e.png" alt="" /></p>
<ul>
<li>Candidate is what our network gives us. References are from humans.</li>
<li>See where it matches.</li>
</ul>
<p><img src="https://i.imgur.com/1lMHytF.png" alt="" /></p>
<ul>
<li>Unigram matching - mapping</li>
</ul>
<p><img src="https://i.imgur.com/j2kCRM2.png" alt="" /></p>
<ul>
<li>Averaged over references</li>
</ul>
<p><img src="https://i.imgur.com/IyG2pdL.png" alt="" /></p>
<ul>
<li><strong>Unigram</strong> for <strong>adequacy</strong></li>
<li><strong>Ngram</strong> for <strong>fluency</strong>
<ul>
<li>Fluency is better (imo)</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/j4kbkLo.png" alt="" /></p>
<ul>
<li>Bigram (2 word length)</li>
</ul>
<p><img src="https://i.imgur.com/P9iowOf.png" alt="" /></p>
<ul>
<li>Geometric Avg ( (( w_n log p_n \) )
<ul>
<li>BP: penalty shorter than r</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/4xTSym1.png" alt="" /></p>
<ul>
<li>Ensemble does well</li>
</ul>
<p><img src="https://i.imgur.com/4dQNLlc.png" alt="" /></p>
<ul>
<li>Try going backwards!
<ul>
<li>coffee love I</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/EC4HORT.png" alt="" /></p>
<ul>
<li>Really small BEAM search</li>
</ul>
<p><img src="https://i.imgur.com/tJE0USb.png" alt="" /></p>
<ul>
<li>Problem! There's a bottleneck for information</li>
</ul>
<p><img src="https://i.imgur.com/qERSjqN.png" alt="" /></p>
<ul>
<li>In soft-attention, Coffee is related to cafe
<ul>
<li>But bottlenecked!</li>
</ul>
</li>
</ul>
<h2><a class="header" href="#soft-attention-for-translation-1" id="soft-attention-for-translation-1">Soft Attention for Translation</a></h2>
<p><img src="https://i.imgur.com/ZETrFIt.png" alt="" /></p>
<ul>
<li><strong>Context vector</strong> is the sum of all weighed h, which are hidden states</li>
<li>Weights are <strong>Mixture weights</strong>, softmax over alignment scores</li>
<li><strong>Alignment scores</strong> input words and output words?</li>
</ul>
<p><img src="https://i.imgur.com/YPalgse.png" alt="" /></p>
<ul>
<li>Wow this is amazing</li>
<li>Bi-directional RNN Encoder</li>
</ul>
<p><img src="https://i.imgur.com/Z1r7qjJ.png" alt="" /></p>
<ul>
<li>Decoder is a RNN, sample word fed back in, get next word out</li>
<li>You have a <strong>Recurrent State</strong> in the <strong>Decoder RNN</strong></li>
<li>and a <strong>Attention Vectors</strong> hidden state in the <strong>Bidirection Encoder RNN</strong></li>
</ul>
<p><img src="https://i.imgur.com/RFYS1Xt.png" alt="" /></p>
<p><img src="https://i.imgur.com/nl7EEHM.png" alt="" /></p>
<ul>
<li>English to French, correctly picks up reversal!</li>
</ul>
<p><img src="https://i.imgur.com/EF8asQs.png" alt="" /></p>
<ul>
<li><strong>Neural Machine Translation!</strong>
<ul>
<li>Complicated model, what the heck</li>
</ul>
</li>
</ul>
<h3><a class="header" href="#stanford-manning-update-attention-neural-machine-translation" id="stanford-manning-update-attention-neural-machine-translation">Stanford Manning Update Attention Neural Machine Translation</a></h3>
<p><img src="https://i.imgur.com/LU9UKYM.png" alt="" /></p>
<ul>
<li>Stacked LSTM</li>
</ul>
<p><img src="https://i.imgur.com/GyKZ5bK.png" alt="" /></p>
<ul>
<li><strong>Global Attention</strong>, Attention is not Recurrent, align weights are now global</li>
</ul>
<p><img src="https://i.imgur.com/4AXBT7p.png" alt="" /></p>
<ul>
<li>Alignment (?)</li>
</ul>
<h3><a class="header" href="#translation-and-parsing" id="translation-and-parsing">Translation and Parsing</a></h3>
<p><img src="https://i.imgur.com/EqGiSWf.png" alt="" /></p>
<ul>
<li>Can generate like nested/tree code!</li>
</ul>
<p><img src="https://i.imgur.com/I2NfCBU.png" alt="" /></p>
<ul>
<li>Build the parse tree (Nouns, parts, etc.)
<ul>
<li>Trees are like LISP, can be nested in parenthesis</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/u5gQqQI.png" alt="" /></p>
<ul>
<li>Sequence-To-Sequence Parse Tree</li>
</ul>
<p><img src="https://i.imgur.com/SBY9QQq.png" alt="" /></p>
<ul>
<li>Training, on the three-bank not well</li>
<li>Then train on the Berkeley Parser</li>
<li>Add attention and retrain on human data
<ul>
<li>It's like Model Distillation?</li>
<li>Something about overfitting</li>
</ul>
</li>
</ul>
<h2><a class="header" href="#attention-only-translation" id="attention-only-translation">Attention-only Translation</a></h2>
<p><img src="https://i.imgur.com/wNITJAH.png" alt="" /></p>
<ul>
<li>Time grows in proportion to sentence length</li>
<li>Long-range hard</li>
<li>Hierarchal deeper structure hard</li>
</ul>
<h3><a class="header" href="#transformer" id="transformer">Transformer</a></h3>
<p><img src="https://i.imgur.com/YJD3LBN.png" alt="" /></p>
<ul>
<li>Query-Key-Value</li>
</ul>
<p><img src="https://i.imgur.com/ki9Z54a.png" alt="" /></p>
<ul>
<li>Try my query to other keys</li>
</ul>
<p><img src="https://i.imgur.com/ctviBRe.png" alt="" /></p>
<ul>
<li>Score is local, Softmax x Value then Sum is global</li>
</ul>
<p><img src="https://i.imgur.com/079hOLX.png" alt="" /></p>
<ul>
<li>Input x, compute inner products with their matrices
<ul>
<li>\( W^V, W^K, W^Q \)</li>
<li>Get \( V_1, K_1, Q_1 \)</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/3Xd2tFj.png" alt="" /></p>
<p><img src="https://i.imgur.com/SUfx7a8.png" alt="" /></p>
<ul>
<li>Q, K, V are matrices now</li>
</ul>
<p><img src="https://i.imgur.com/qzELmTE.png" alt="" /></p>
<ul>
<li><strong>Multi-Headed Attention</strong></li>
</ul>
<p><img src="https://i.imgur.com/14NfcZi.png" alt="" /></p>
<ul>
<li>multiple heads
<ul>
<li>recognized more difficult is coupled</li>
<li>blue, green, red</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/sPO4eMU.png" alt="" /></p>
<ul>
<li><strong>Transformer Encoder</strong>
<ul>
<li>Encoded as a single matrix</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/jTD6Qo6.png" alt="" /></p>
<h4><a class="header" href="#transformer-encoderdecoder" id="transformer-encoderdecoder">Transformer Encoder/Decoder</a></h4>
<p><img src="https://i.imgur.com/IKZ7vfD.png" alt="" /></p>
<ul>
<li>Value Key fed in</li>
</ul>
<p><img src="https://i.imgur.com/wZmiJ9e.png" alt="" /></p>
<h1><a class="header" href="#transformers-and-pre-training" id="transformers-and-pre-training">Transformers and Pre-Training</a></h1>
<p><a href="https://towardsdatascience.com/visual-attention-model-in-deep-learning-708813c2912c">Visual Attention</a></p>
<p><a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Visualizing NMT</a></p>
<p><a href="http://jalammar.github.io/illustrated-transformer/">Visualizing Transformers</a></p>
<h2><a class="header" href="#review-1" id="review-1">Review</a></h2>
<p><img src="https://i.imgur.com/BVdVEoi.png" alt="" /></p>
<ul>
<li>Up and Down Attention</li>
</ul>
<p><img src="https://i.imgur.com/gGG4J6N.png" alt="" /></p>
<ul>
<li>Transformers (Convolution of NLP)
<ul>
<li>Self Attention</li>
<li>Encoding/Decoding</li>
</ul>
</li>
</ul>
<h2><a class="header" href="#transformers" id="transformers">Transformers</a></h2>
<p><img src="https://i.imgur.com/mYSymyv.png" alt="" /></p>
<p><img src="https://i.imgur.com/5jvchbe.png" alt="" /></p>
<ul>
<li><strong>Multi-Headed Attention</strong>
<code>h</code> block of scaled dot-product attention. Tensor V, K, Q</li>
</ul>
<p><img src="https://i.imgur.com/aqMQVVh.png" alt="" /></p>
<ul>
<li>What is Encoder/Decoder?</li>
</ul>
<h3><a class="header" href="#transformer-encoder" id="transformer-encoder">Transformer Encoder</a></h3>
<p><img src="https://i.imgur.com/Bd6mOCq.png" alt="" /></p>
<ul>
<li>Has residuals of based inputs</li>
<li>What is the Positional Encoding</li>
</ul>
<p><img src="https://i.imgur.com/FmYWjJH.png" alt="" /></p>
<p><img src="https://i.imgur.com/0dTtfWA.png" alt="" /></p>
<ul>
<li>Why does it mix?</li>
<li>From words and has positional encoding</li>
</ul>
<h3><a class="header" href="#multi-headed-attention" id="multi-headed-attention">Multi-Headed Attention</a></h3>
<p><img src="https://i.imgur.com/LgGrwEe.png" alt="" /></p>
<ul>
<li>Attention strength (sum) from different words</li>
</ul>
<p><img src="https://i.imgur.com/G490xoJ.png" alt="" /></p>
<ul>
<li>Self Attention (?)</li>
<li>What are the colors of the lines and on the words (?)</li>
</ul>
<h3><a class="header" href="#transformer-decoding" id="transformer-decoding">Transformer Decoding</a></h3>
<p><img src="https://i.imgur.com/J7O9jOG.png" alt="" /></p>
<ul>
<li>Decoding is to generate text</li>
<li>Attention vs Self-Attention (?)</li>
<li>What does the direction of the arrow mean?</li>
</ul>
<p><img src="https://i.imgur.com/IVhfqGJ.png" alt="" /></p>
<ul>
<li>Green is (causal/masked) self-attention</li>
</ul>
<p><img src="https://i.imgur.com/NAR824z.png" alt="" /></p>
<ul>
<li>At training time can do in parallel</li>
</ul>
<p><img src="https://i.imgur.com/7byCbh7.png" alt="" /></p>
<ul>
<li>N-Best Transformers, k copy of your output word
<ul>
<li>renard with 0.3 confidence</li>
<li>canard with 0.1 confidence</li>
</ul>
</li>
</ul>
<h3><a class="header" href="#transformer-position-encoding" id="transformer-position-encoding">Transformer Position Encoding</a></h3>
<p><img src="https://i.imgur.com/vfl5qkP.png" alt="" /></p>
<ul>
<li>The encoding doesn't have any ordering</li>
<li>Position is a sinusoid?</li>
</ul>
<p><img src="https://i.imgur.com/AguYmZQ.png" alt="" /></p>
<ul>
<li>If your vector is even or odd
<ul>
<li>Learnable shifting relative displacement</li>
<li>The inner product measures relative displacement</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/l2zCU1J.png" alt="" /></p>
<p><img src="https://i.imgur.com/oWqORx5.png" alt="" /></p>
<ul>
<li>linear combination that is strongest at position (idk)</li>
</ul>
<h2><a class="header" href="#tokenization-challenges" id="tokenization-challenges">Tokenization Challenges</a></h2>
<p><img src="https://i.imgur.com/zItfCEk.png" alt="" /></p>
<ul>
<li>new word like Starliner? can use <code>UNK</code> char</li>
</ul>
<p><img src="https://i.imgur.com/twmby69.png" alt="" /></p>
<ul>
<li>Break word down <code>##liner</code> from <code>starliner</code></li>
</ul>
<p><img src="https://i.imgur.com/OFg3afu.png" alt="" /></p>
<ul>
<li>Small vocab</li>
<li>No <code>UNK</code> tokens</li>
</ul>
<p><img src="https://i.imgur.com/Qz0dYAU.png" alt="" /></p>
<ul>
<li>Summarization, quadratic terms</li>
</ul>
<p><img src="https://i.imgur.com/oauz9sr.png" alt="" /></p>
<ul>
<li>He mixed up M and N. Don't need <code>N^2</code> term, small <code>M^2</code> term</li>
</ul>
<h2><a class="header" href="#bert-bidirectional-encoder-representations-from-transformers" id="bert-bidirectional-encoder-representations-from-transformers">Bert (Bidirectional Encoder Representations from Transformers)</a></h2>
<p><img src="https://i.imgur.com/Lkc4KuK.png" alt="" /></p>
<ul>
<li>Language model. Text is like a label, so it is like self labeling! Predict next word, previous word, etc.</li>
<li>Bert is a encoder model, bi-directional</li>
<li>GPT is a decoder model</li>
</ul>
<p><img src="https://i.imgur.com/oYIMVEN.png" alt="" /></p>
<ul>
<li>No encoder, no cross attention (?)</li>
</ul>
<p><img src="https://i.imgur.com/aNnUldP.png" alt="" /></p>
<ul>
<li>Minimize language modeling loss</li>
</ul>
<p><img src="https://i.imgur.com/0mgUX62.png" alt="" /></p>
<ul>
<li>Apply GPT to:
<ul>
<li>classification (what kind of speech)</li>
<li>Entailment (a implies b, contraction, independent)</li>
<li>Similarity has both orders</li>
<li>Multiple Choice, run it multiple times!</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/qVblTh7.png" alt="" /></p>
<ul>
<li>More data, larger models keep getting better on performance!</li>
</ul>
<h2><a class="header" href="#summary" id="summary">Summary</a></h2>
<p><img src="https://i.imgur.com/gyL8pqy.png" alt="" /></p>
<h1><a class="header" href="#nlp-applications" id="nlp-applications">NLP Applications</a></h1>
<h2><a class="header" href="#review-2" id="review-2">Review</a></h2>
<p><img src="https://i.imgur.com/5mGuBFY.png" alt="" /></p>
<ul>
<li>There is self-attention</li>
<li>Masked attention, for decoder, can only see in the past</li>
</ul>
<h2><a class="header" href="#generation-vs-understanding" id="generation-vs-understanding">Generation vs Understanding</a></h2>
<ul>
<li>NLG generate sequence as words</li>
<li>NLU build a representation to be used for a task</li>
<li>Summarization can be seen as both
<ul>
<li>Abstract Summarization</li>
<li>Extractive Summarization (3 most important, not generating text in this case!)</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/Rn21lmM.png" alt="" /></p>
<ul>
<li>Pretraining: Language Modeling and Masked Language Modeling</li>
</ul>
<h2><a class="header" href="#bert" id="bert">BERT</a></h2>
<p><img src="https://i.imgur.com/GEp1YXT.png" alt="" /></p>
<ul>
<li><strong>Bidirectional</strong></li>
</ul>
<p><img src="https://i.imgur.com/5h9Shz5.png" alt="" /></p>
<ul>
<li>Can't do Language Modeling, cause we already have the word! The model would cheat</li>
<li>Instead mask some words. Guess the word!</li>
<li><strong>Next sentence prediction</strong>
<ul>
<li>Give Two sentences, does sentence two make sense to follow sentence one?</li>
<li>Learns to represent beyond the sentence</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/BycdfAl.png" alt="" /></p>
<ul>
<li>Mask word and next sentence prediction</li>
</ul>
<h3><a class="header" href="#masked-vs-regular-language-modeling" id="masked-vs-regular-language-modeling">Masked vs. Regular Language Modeling</a></h3>
<p><img src="https://i.imgur.com/xuqlfIZ.png" alt="" /></p>
<ul>
<li>bi-directionality is important 50% to 65%. Having masks allows you to do bi-directionality</li>
</ul>
<h3><a class="header" href="#contextual-word-embedding" id="contextual-word-embedding">Contextual Word Embedding</a></h3>
<p><img src="https://i.imgur.com/IFYurkY.png" alt="" /></p>
<ul>
<li><code>Run</code> has many meanings!</li>
<li>Unlike word2vec, we use these contexts by going through stacked layers of self-attention and transformation</li>
</ul>
<h2><a class="header" href="#bert-task-specialization" id="bert-task-specialization">BERT Task Specialization</a></h2>
<p><img src="https://i.imgur.com/GmFkv06.png" alt="" /></p>
<ul>
<li>Segment Embeddings?
<ul>
<li>Dif Sentences</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/qMllq9L.png" alt="" /></p>
<ul>
<li>Sentence Pair Classification</li>
<li>Single Sentence like sentiment classification</li>
<li>You can fine tune, it actually seems to work!
<ul>
<li>The residual layers can redirect to the relevant layers</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/85yLA3T.png" alt="" /></p>
<ul>
<li>BERT did really well on GLUE.</li>
</ul>
<h2><a class="header" href="#tasks" id="tasks">Tasks</a></h2>
<p><img src="https://i.imgur.com/LhCeK3Q.png" alt="" /></p>
<ul>
<li>entailment, contracts, neutral
<ul>
<li>Kids play in the garden, vs no one goes to the garden</li>
<li>Can be ambiguous - &quot;Do kids like playing Soccer?&quot; </li>
</ul>
</li>
<li>MultiGenre NLI
<ul>
<li>From Flickr and other datasets!</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/X2E7WAs.png" alt="" /></p>
<ul>
<li>SQuAD, not part of GLUE but common question answering</li>
<li>Big bias because the dataset always had an answer</li>
</ul>
<p><img src="https://i.imgur.com/S9TaXaF.png" alt="" /></p>
<ul>
<li>SQuAD 2.0 (adversarial questions)</li>
</ul>
<h3><a class="header" href="#transfer-learning" id="transfer-learning">Transfer Learning</a></h3>
<p><img src="https://i.imgur.com/pJ2sBgD.png" alt="" /></p>
<ul>
<li>Transformer -&gt; Pretrain on General Text -&gt; In-domain text -&gt; Fine-tune classification</li>
</ul>
<h3><a class="header" href="#evaluating-language-models---perplexity" id="evaluating-language-models---perplexity">Evaluating Language Models - Perplexity</a></h3>
<p><img src="https://i.imgur.com/jtMBbLd.png" alt="" /></p>
<ul>
<li>Uncertainty - normalize by length</li>
</ul>
<p><img src="https://i.imgur.com/sqQiATF.png" alt="" /></p>
<ul>
<li>GPT-2 beat models <strong>without finetuning</strong>, low perplexity</li>
</ul>
<h3><a class="header" href="#gpt-2-generation" id="gpt-2-generation">GPT-2 Generation</a></h3>
<p><img src="https://i.imgur.com/Bq29qiH.png" alt="" /></p>
<h3><a class="header" href="#multiple-languages" id="multiple-languages">Multiple Languages</a></h3>
<p><img src="https://i.imgur.com/MIh8YiK.png" alt="" /></p>
<ul>
<li>FAIR XLM did it with 15 languages</li>
</ul>
<p><img src="https://i.imgur.com/9dgRDQh.png" alt="" /></p>
<ul>
<li>Translation Language Modeling
<ul>
<li>Has some English, some French, when masked can learn to use the other lang</li>
</ul>
</li>
</ul>
<h3><a class="header" href="#conversational-qa" id="conversational-qa">Conversational Q&amp;A</a></h3>
<p><img src="https://i.imgur.com/MoUX4AG.png" alt="" /></p>
<ul>
<li>Incomplete information</li>
</ul>
<h1><a class="header" href="#generative-models" id="generative-models">Generative Models</a></h1>
<h2><a class="header" href="#review-3" id="review-3">Review</a></h2>
<p><img src="https://i.imgur.com/xRqRDYu.png" alt="" /></p>
<h2><a class="header" href="#generative-models-1" id="generative-models-1">Generative Models</a></h2>
<ul>
<li>Variational Auto-Encoder (VAE)</li>
<li>Auto-Regressive Models</li>
<li>Transformers</li>
<li>Generative Adversarial Networks (next time)</li>
</ul>
<h2><a class="header" href="#generative-models-2" id="generative-models-2">Generative Models</a></h2>
<p><img src="https://i.imgur.com/jsJTn7h.png" alt="" /></p>
<ul>
<li><strong>Classifier</strong> vs <strong>Generator</strong>
<ul>
<li>Deep Learning can't be efficient to create full joint probability</li>
</ul>
</li>
</ul>
<h2><a class="header" href="#auto-encoder" id="auto-encoder">Auto-Encoder</a></h2>
<p><img src="https://i.imgur.com/0uw6wI7.png" alt="" /></p>
<ul>
<li><strong>Encoder:</strong> Data Compressor</li>
<li><strong>Code:</strong> low dimension representation with information bottleneck</li>
<li><strong>Decoder:</strong> generative</li>
</ul>
<p><img src="https://i.imgur.com/dInFxyq.png" alt="" /></p>
<ul>
<li>One option is to invert the function
<ul>
<li>z -&gt; x' is non-differentiable so it requires reinforcement learning</li>
</ul>
</li>
</ul>
<h3><a class="header" href="#implicit-auto-encoders" id="implicit-auto-encoders">Implicit Auto-Encoders</a></h3>
<p><img src="https://i.imgur.com/tZgN5Iz.png" alt="" /></p>
<ul>
<li>From Latent code <code>z</code> to output <code>x</code>, same \( \theta \)
<ul>
<li>VAE: approximation Instead you get a distribution, not a single <code>x</code></li>
</ul>
</li>
</ul>
<h3><a class="header" href="#variational-auto-encoders" id="variational-auto-encoders">Variational Auto-Encoders</a></h3>
<p><img src="https://i.imgur.com/CByY1dP.png" alt="" /></p>
<ul>
<li>\( q_{\phi}(z|x) \): going up approximate inverse
<ul>
<li>\( p_{\theta}(z|x) \)</li>
</ul>
</li>
<li>What are these equations?
<ul>
<li>Theta vs phi</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/cl6xJLd.png" alt="" /></p>
<ul>
<li>Therefore there are two sets of parameters instead</li>
</ul>
<p><img src="https://i.imgur.com/N96QvIi.png" alt="" /></p>
<ul>
<li>\( \epsilon \) is noise
<ul>
<li>g can be a normal network</li>
<li>Both parts can be learn by backprop</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/FAKPt7t.png" alt="" /></p>
<ul>
<li>Max the probability of x given
<ul>
<li>marginal likelihood so we have un-marginalize it</li>
<li>written as an expectation (Expectation from \( E_{z~q_{\phi}(z|x^i)} \))</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/llP1nTk.png" alt="" /></p>
<ul>
<li>It's like the MLE trick</li>
</ul>
<p><img src="https://i.imgur.com/bQUYyf3.png" alt="" /></p>
<ul>
<li>Since it's concave, there's a <strong>tangent line</strong>
<ul>
<li>always less than or equal to that point</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/jdAxioF.png" alt="" /></p>
<p><img src="https://i.imgur.com/dMF1SMB.png" alt="" /></p>
<ul>
<li>?</li>
</ul>
<h2><a class="header" href="#optimizing-reparametrized-models" id="optimizing-reparametrized-models">Optimizing Reparametrized Models</a></h2>
<p><img src="https://i.imgur.com/MhtvPk9.png" alt="" /></p>
<ul>
<li></li>
</ul>
<p><img src="https://i.imgur.com/3q6PAJ5.png" alt="" /></p>
<ul>
<li>LMAO I'm confused plz help</li>
<li>Expected value: Integral</li>
<li>repraram the network with epsilon noise</li>
<li>the final equation is like SGD?
<ul>
<li>I think they meant it's like sampling </li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/VdhyTvM.png" alt="" /></p>
<ul>
<li>We can approximate things</li>
</ul>
<p><img src="https://i.imgur.com/I9m3zRo.png" alt="" /></p>
<ul>
<li>Let's be honest this is black magic at this point.
<ul>
<li>High variance from samples?</li>
<li>sample from equiv distributions instead</li>
<li>Wait this actually makes sense</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/d1vWYxe.png" alt="" /></p>
<p><img src="https://i.imgur.com/HsV8Cey.png" alt="" /></p>
<ul>
<li>Can kind of generate faces</li>
</ul>
<h2><a class="header" href="#autoregressive-models" id="autoregressive-models">Autoregressive Models</a></h2>
<p><img src="https://i.imgur.com/GuEsiT8.png" alt="" /></p>
<ul>
<li>Pixels are correlated</li>
<li>\( p(x_i | x_{i-1}, x_{i-2}, ...) \) conditioned on previous inputs</li>
</ul>
<p><img src="https://i.imgur.com/vtmFje2.png" alt="" /></p>
<p><img src="https://i.imgur.com/1MrQGev.png" alt="" /></p>
<ul>
<li>CNN is an empty image</li>
</ul>
<p><img src="https://i.imgur.com/tYVH9Ia.png" alt="" /></p>
<ul>
<li>and feed it back in</li>
</ul>
<p><img src="https://i.imgur.com/jg7dJ2K.png" alt="" /></p>
<ul>
<li>slow generation, fast training! it can be done in parallel!</li>
</ul>
<p><img src="https://i.imgur.com/E0U6ytC.png" alt="" /></p>
<ul>
<li>Use LSTM from row above, therefore can generate row in parallel!</li>
</ul>
<p><img src="https://i.imgur.com/E0U6ytC.png" alt="" /></p>
<ul>
<li>bam!</li>
</ul>
<p><img src="https://i.imgur.com/adhQ1Xj.png" alt="" /></p>
<ul>
<li>You can use previous rows, or all seen before!</li>
</ul>
<h3><a class="header" href="#examples" id="examples">Examples</a></h3>
<p><img src="https://i.imgur.com/6TbDfEO.jpg" alt="" /></p>
<ul>
<li>Good but with distortions</li>
</ul>
<p><img src="https://i.imgur.com/RbhuYF5.jpg" alt="" /></p>
<ul>
<li>Ok</li>
</ul>
<h2><a class="header" href="#image-transformer" id="image-transformer">Image Transformer</a></h2>
<p><img src="https://i.imgur.com/WJOARPJ.png" alt="" /></p>
<ul>
<li>Like generating text
<ul>
<li>multi-head attention</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/f5LrKTS.png" alt="" /></p>
<ul>
<li>2D Memory Block, left is a transformer</li>
</ul>
<h1><a class="header" href="#generative-adversarial-networks" id="generative-adversarial-networks">Generative Adversarial Networks</a></h1>
<h2><a class="header" href="#last-time" id="last-time">Last Time</a></h2>
<p><img src="https://i.imgur.com/LCa14jA.png" alt="" /></p>
<ul>
<li>network \( p_{\theta} \)</li>
</ul>
<p><img src="https://i.imgur.com/ADhTUfX.png" alt="" /></p>
<ul>
<li>PixelCNN</li>
</ul>
<p><img src="https://i.imgur.com/jD6YkiT.png" alt="" /></p>
<ul>
<li>Autoregressive conditioned on the input</li>
<li>Posterior distribution</li>
<li>Model human discriminating</li>
</ul>
<h2><a class="header" href="#problems-of-variational-auto-encoders" id="problems-of-variational-auto-encoders">Problems of Variational Auto-Encoders</a></h2>
<p><img src="https://i.imgur.com/0ZfYPLQ.png" alt="" /></p>
<ul>
<li>Compares densities in <strong>latent z space</strong></li>
</ul>
<p><img src="https://i.imgur.com/hSX7tTl.png" alt="" /></p>
<p><img src="https://i.imgur.com/srG2XiI.png" alt="" /></p>
<ul>
<li>Avoid approximations and density estimates</li>
</ul>
<p><img src="https://i.imgur.com/srG2XiI.png" alt="" /></p>
<ul>
<li>Real and Synthetic Image distribution \( p_{re}(x) \)</li>
<li>Has KL divergence and approximate expected values by sampling</li>
</ul>
<p><img src="https://i.imgur.com/VoRvTK5.png" alt="" /></p>
<ul>
<li>Wait what?</li>
<li>\( d_{\phi} \) is softmax - so it is a classifier</li>
</ul>
<p><img src="https://i.imgur.com/q3iiMMI.png" alt="" /></p>
<ul>
<li>First equation is plus
<ul>
<li>We are minimizing the divergence</li>
</ul>
</li>
<li>Second equation is minus.
<ul>
<li>We are optimizing the discriminator</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/miQHrAw.png" alt="" /></p>
<ul>
<li>Proof of classifier</li>
</ul>
<p><img src="https://i.imgur.com/G0tU2ga.png" alt="" /></p>
<ul>
<li>Written as one formula</li>
<li>Now we add a generator \( g_{\theta} \) being maximized</li>
</ul>
<p><img src="https://i.imgur.com/rwJe94j.png" alt="" /></p>
<ul>
<li>Two player game</li>
</ul>
<p><img src="https://i.imgur.com/DOJaVTh.png" alt="" /></p>
<ul>
<li>Generate from Latent space to image</li>
</ul>
<p><img src="https://i.imgur.com/SOxLluY.png" alt="" /></p>
<ul>
<li>Classifies realness. </li>
<li>Maximizes (or minimize) divergence? what does that mean</li>
</ul>
<p><img src="https://i.imgur.com/zMuCciB.png" alt="" /></p>
<ul>
<li>Can backprop all the way! End-to-end</li>
</ul>
<p><img src="https://i.imgur.com/GTWN3FI.png" alt="" /></p>
<ul>
<li></li>
</ul>
<p><img src="https://i.imgur.com/Nn5x37g.png" alt="" /></p>
<ul>
<li>Step 1 update discriminator</li>
<li>Step 2 update generator</li>
</ul>
<p><img src="https://i.imgur.com/8x6Zh9J.png" alt="" /></p>
<ul>
<li>Big gradients when outputs are bad.</li>
</ul>
<p><img src="https://i.imgur.com/jcBeurj.png" alt="" /></p>
<ul>
<li>Does pretty well! (2014)</li>
</ul>
<h2><a class="header" href="#gan-improvements" id="gan-improvements">GAN Improvements</a></h2>
<p><a href="https://github.com/soumith/ganhacks">GAN Hacks</a></p>
<p><img src="https://i.imgur.com/TvpPXWB.png" alt="" /></p>
<ul>
<li>Progressively going up in</li>
<li>Fractional Stride Convolution</li>
</ul>
<p><img src="https://i.imgur.com/yivjQGN.png" alt="" /></p>
<ul>
<li>Can do arithmetic on this! Man with glasses! without!</li>
</ul>
<p><img src="https://i.imgur.com/ME1p60T.png" alt="" /></p>
<ul>
<li>Add a class label</li>
</ul>
<p><img src="https://i.imgur.com/ME1p60T.png" alt="" /></p>
<ul>
<li>Text to Image synthesis to both generator and discriminator
<ul>
<li><code>This flower has small round violet petals with a dark purple center.</code></li>
</ul>
</li>
</ul>
<h2><a class="header" href="#gan-problems" id="gan-problems">GAN Problems</a></h2>
<p><img src="https://i.imgur.com/2OnYLJR.png" alt="" /></p>
<ul>
<li>only learns to generate South Pole</li>
<li>But then starts to generate Alice Springs</li>
<li>Unstable</li>
</ul>
<p><img src="https://i.imgur.com/aXGncYz.png" alt="" /></p>
<ul>
<li>Experience replay - old discriminator and generator.</li>
</ul>
<p><img src="https://i.imgur.com/0m2YB88.png" alt="" /></p>
<ul>
<li>Wasserstein Distance</li>
</ul>
<p><img src="https://i.imgur.com/UIgAppt.png" alt="" /></p>
<ul>
<li>Critic function diffs approximate the Wasserstein GAN</li>
</ul>
<p><img src="https://i.imgur.com/s780qW2.png" alt="" /></p>
<ul>
<li>Provably avoids mode collapse - train to optimality</li>
</ul>
<p><img src="https://i.imgur.com/yyXoOeQ.png" alt="" /></p>
<p><img src="https://i.imgur.com/a55wcBI.png" alt="" /></p>
<ul>
<li>Scale up, don't need progressive GAN</li>
</ul>
<p><img src="https://i.imgur.com/IvEgb8x.png" alt="" /></p>
<ul>
<li>Upsampling - generator</li>
</ul>
<h2><a class="header" href="#evaluating-gans" id="evaluating-gans">Evaluating GANs</a></h2>
<p><img src="https://i.imgur.com/oawobnd.png" alt="" /></p>
<ul>
<li>Inception Score</li>
<li>Using Inception v3 Network</li>
<li>Multi-resolution convolutions
<ul>
<li>1x1, 3x3, 5x5 blocks</li>
<li>like humans</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/OL9fRyo.png" alt="" /></p>
<ul>
<li>Another improvement on evaluation</li>
</ul>
<h2><a class="header" href="#unpaired-conditional-image-generation" id="unpaired-conditional-image-generation">Unpaired Conditional Image Generation</a></h2>
<p><img src="https://i.imgur.com/ZfgRSf0.png" alt="" /></p>
<ul>
<li>not paired. Use a corresponding image and generate a synthetic image</li>
</ul>
<p><img src="https://i.imgur.com/ZWXiMn2.png" alt="" /></p>
<ul>
<li><strong>CycleGAN</strong> create a cycle 2 generators and 2 discriminators
<ul>
<li>cycle-consistency loss L1 distance</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/ZWXiMn2.png" alt="" /></p>
<ul>
<li>Really remarkable</li>
<li>Compared to neural style
<ul>
<li>Neural style uses statistics</li>
<li>Harder to discriminate</li>
</ul>
</li>
</ul>
<h1><a class="header" href="#other-resources" id="other-resources">Other Resources</a></h1>
<h1><a class="header" href="#visualizing-seq2seq-with-attention" id="visualizing-seq2seq-with-attention">Visualizing Seq2Seq with Attention</a></h1>
<p><a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Link</a></p>
<h2><a class="header" href="#review-of-visual-attention" id="review-of-visual-attention">Review of Visual Attention</a></h2>
<p><img src="https://i.imgur.com/GpIFcPF.png" alt="" /></p>
<ul>
<li>Have Image (top) and location <code>l_t-1</code> fed into <code>f_g</code> glimpse network.</li>
<li>This is fed into a recurrent network <code>f_h</code></li>
<li>The output hidden state is fed into two networks
<ul>
<li><code>f_a</code> activation network which does the prediction</li>
<li><code>f_l</code> location network which predicts the next location <code>l_t</code>. In Soft Attention this is done with a saliency map.</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/3lZj6UK.png" alt="" /></p>
<ul>
<li>The model has an <strong>encoder</strong> and <strong>decoder</strong></li>
<li><strong>Context</strong> is transfered from the encoder to the decoder
<ul>
<li>Context is a vector of floats. It is a hidden state in a RNN</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/IVCpWGs.png" alt="" /></p>
<ul>
<li>The words are embedded as vectors using a <strong>word embedding</strong> algorithm.</li>
</ul>
<h3><a class="header" href="#at-attention" id="at-attention">At Attention!</a></h3>
<ul>
<li><strong>Attention</strong> is the idea some words are more important than others when getting the translation
<ul>
<li>Café to coffee</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/VviBRGt.png" alt="" /></p>
<ul>
<li>Instead of one hidden state, pass all the hidden states!</li>
</ul>
<p><img src="https://i.imgur.com/R0V8kgM.png" alt="" /></p>
<ul>
<li>Use the decoder hidden state to score encoder hidden states. This is  <strong>attention</strong>.</li>
</ul>
<p><img src="https://i.imgur.com/a012seQ.png" alt="" /></p>
<ul>
<li><code>h_4</code> : hidden state and <code>c_4</code> : context state are concated and the output word comes out</li>
<li>Where does the scoring happen though?</li>
</ul>
<h2><a class="header" href="#from-lecture" id="from-lecture">From Lecture</a></h2>
<p><img src="https://i.imgur.com/ZETrFIt.png" alt="" /></p>
<ul>
<li>In lecture, it is weighed alignment scores</li>
<li>alignment score: \( \text{score}(\boldsymbol{s}_t, \boldsymbol{h}_i) = \mathbf{v}_a^\top \tanh(\mathbf{W}_a[\boldsymbol{s}_t; \boldsymbol{h}_i]) \)
<ul>
<li>\( \mathbf{v}_a \) and \( \mathbf{W}_a \) are weights that can be learned</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/RFYS1Xt.png" alt="" /></p>
<ul>
<li>Combine current recurrent state and all input states into attention weight (soft-attention) of input states. The attention weight adds up to 1, and is done with a softmax. </li>
</ul>
<h1><a class="header" href="#visualizing-transformers" id="visualizing-transformers">Visualizing Transformers</a></h1>
<p><a href="http://jalammar.github.io/illustrated-transformer/">Link</a></p>
<h2><a class="header" href="#high-level" id="high-level">High Level</a></h2>
<p><img src="https://i.imgur.com/EjK8hNc.png" alt="" /></p>
<ul>
<li>In the transformer black box we have an <strong>encoder</strong> and <strong>decoder</strong></li>
</ul>
<p><img src="https://i.imgur.com/DN6ElwE.png" alt="" /></p>
<ul>
<li>The encoders are stacked on top of each other.</li>
</ul>
<h3><a class="header" href="#encoders" id="encoders">Encoders</a></h3>
<p><img src="https://i.imgur.com/NZ8mlKa.png" alt="" /></p>
<ul>
<li>The encoder block is made of a <strong>self-attention</strong> layer and <strong>feed-forward network</strong>
<ul>
<li>self-attention helps the encoder look at words in the input sentence as it encodes a specific word</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/7W5GDqK.png" alt="" /></p>
<ul>
<li>In NLP, each word is embedded as a vector</li>
<li>list of word vectors is a tensor</li>
</ul>
<p><img src="https://i.imgur.com/E6136r3.png" alt="" /></p>
<p><img src="https://i.imgur.com/8jThN0h.png" alt="" /></p>
<h3><a class="header" href="#self-attention" id="self-attention">Self-Attention</a></h3>
<ul>
<li>The feed-forward layer is actually one network that is reused by each \( Z_i \) encoded vector. That is like a convolution</li>
</ul>
<p><img src="https://i.imgur.com/Vqn2VSb.png" alt="" /></p>
<ul>
<li>Say you have the sentence &quot;The animal didn't cross the street because it was too tired.&quot; What does <strong>it</strong> refer to? In this case, &quot;The animal.&quot; A self-attention layer lets the network learn this representation.</li>
</ul>
<p><img src="https://i.imgur.com/unpaPfS.png" alt="" /></p>
<ul>
<li>We have the embedded words (green \( X_i \))</li>
<li>For each word, we want to get out a <strong>queries</strong> vector, a <strong>keys</strong> vector, and a <strong>values</strong> vector. We have learnable weights \( W^Q \), \( W^K \), and \( W^V \).</li>
<li>The matrix multiplication of the green embedded word and the queries matrix, keys matrix, and values matrix create the queries vector, keys vector, and values vector for each word.</li>
</ul>
<p><img src="https://i.imgur.com/jsas4BF.png" alt="" /></p>
<ul>
<li>For each word, we calculate a score using the queries vector and keys vector.</li>
</ul>
<p><img src="https://i.imgur.com/LpzehcO.png" alt="" /></p>
<ul>
<li>We divide by 8 (sqrt of dim of keys vector for stable gradients) and take the softmax over all words (in a batch? in the corpus?)
<ul>
<li>(self-attention seems to be over a batch)</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/o2gTn7S.png" alt="" /></p>
<ul>
<li>We multiply the softmax by the value and sum up all the weighted value vector \( V_i \)
<ul>
<li>(Does this mean all \( Z_i \) are equal though?)</li>
</ul>
</li>
</ul>
<h3><a class="header" href="#self-attention-as-matrices" id="self-attention-as-matrices">Self-Attention as Matrices</a></h3>
<p><img src="https://i.imgur.com/CmSRI5r.png" alt="" /></p>
<ul>
<li>Batch of two words, embedded in a vector of size 2.</li>
<li>(Each weight matrix is the same size though?)</li>
</ul>
<p><img src="https://i.imgur.com/eVmtrMH.png" alt="" /></p>
<ul>
<li>Q, K, and V are calculated from X (They are NOT the weight matrices!)</li>
<li>Their operation can be condensed into one formula</li>
</ul>
<h3><a class="header" href="#multi-head-attention" id="multi-head-attention">Multi-head Attention</a></h3>
<p><img src="https://i.imgur.com/63q1zFr.png" alt="" /></p>
<ul>
<li>Instead of just using one Q, K, and V weight we can use multiple!</li>
<li>This is like multiple filters in a convolution.</li>
</ul>
<p><img src="https://i.imgur.com/vaVBv2C.png" alt="" /></p>
<ul>
<li>To combine the multiple heads into a single head for the feed-forward network, we can concat all the heads \( Z_i \) and multiply it by a weight matrix \( W^O \)</li>
</ul>
<p><img src="https://i.imgur.com/vyxHEn7.png" alt="" /></p>
<ul>
<li>Each step shown together
<ul>
<li>The input to self-attention does not have to be a batch of words (green matrix \( X \)) but it can be the output of an encoder below (blue matrix \( R \)). Encoders are stacked.</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/7RDKhZK.png" alt="" /></p>
<ul>
<li>At the top is a bunch of colors. They represent heads. The blue head for the word &quot;it_&quot; has attention at &quot;street&quot;, &quot;'<em>&quot;, and &quot;because&quot;. The green head has attention at &quot;tire&quot;, &quot;d</em>&quot;, and &quot;was_&quot;.</li>
</ul>
<h3><a class="header" href="#positional-encoding" id="positional-encoding">Positional Encoding</a></h3>
<p><img src="https://i.imgur.com/6nL9QBu.png" alt="" /></p>
<ul>
<li>To encode position, we add positional encoding with our embedded word vectors</li>
</ul>
<p><img src="https://i.imgur.com/KJy5mug.png" alt="" /></p>
<ul>
<li>The encoding is done with sin and cosine. Not much detail is added (@TODO go back to this in the lecture)</li>
</ul>
<h3><a class="header" href="#residuals-in-the-encoder" id="residuals-in-the-encoder">Residuals in the Encoder</a></h3>
<p><img src="https://i.imgur.com/ObpOAHt.png" alt="" /></p>
<ul>
<li>Like ResNet, Residuals are useful in Transformers.</li>
</ul>
<p><img src="https://i.imgur.com/856SQJe.png" alt="" /></p>
<ul>
<li>The pre-attention vectors and post-attention vectors are added together and normalized like batch norm</li>
</ul>
<h2><a class="header" href="#decoder" id="decoder">Decoder</a></h2>
<p><img src="https://i.imgur.com/ZttEnYU.png" alt="" /></p>
<ul>
<li>The output vectors of the encoder are transformed into Key and Value matrices (not weights?) in the Decoder sequence</li>
<li>The Decoder has to create it's own Queries matrix</li>
<li>They are used by each Decoder (why?)</li>
</ul>
<p><img src="https://i.imgur.com/4O6WuNr.png" alt="" /></p>
<ul>
<li>The previous input is fed into the decoder. The positional encoding is added again to this output</li>
</ul>
<p><img src="https://i.imgur.com/51kvlQj.png" alt="" /></p>
<ul>
<li>The output embedded vector is matrix mutliplied to the vocab_size. It goes through a softmax, where the large index is the chosen word.</li>
</ul>
<h2><a class="header" href="#training" id="training">Training</a></h2>
<p><img src="https://i.imgur.com/vlzyFKr.png" alt="" /></p>
<ul>
<li>The data is matrix of batch size times one hot encoding and the true value is the same.</li>
<li>In training we can keep the top k best predictions at each word. And feed best next words into the decoder. This is <strong>beam search</strong></li>
</ul>
<h1><a class="header" href="#visualizing-bert-elmo-and-gpt" id="visualizing-bert-elmo-and-gpt">Visualizing BERT, ELMo, and GPT</a></h1>
<p><a href="http://jalammar.github.io/illustrated-bert/">Link</a></p>
<p><img src="https://i.imgur.com/vaB3saH.png" alt="" /></p>
<ul>
<li>Step 1 is to semi-supervise on large amounts of task. Usually predicting masked words.</li>
<li>Step 2 is to fine grain train on a dataset</li>
</ul>
<p><img src="https://i.imgur.com/ICfXukJ.png" alt="" /></p>
<ul>
<li>BERT then with a fine grain spam classifier</li>
</ul>
<p><img src="https://i.imgur.com/R5YLWxl.png" alt="" /></p>
<ul>
<li>BERT is a encoder transformer stack</li>
</ul>
<h2><a class="header" href="#word-embeddings-and-elmo" id="word-embeddings-and-elmo">Word Embeddings and ELMo</a></h2>
<p><img src="https://i.imgur.com/wNFFOZc.jpg" alt="" /></p>
<ul>
<li>ELMo uses contextual (where in the sentence) to embedded.</li>
</ul>
<p><img src="https://i.imgur.com/Ynzilsc.png" alt="" /></p>
<ul>
<li>ELMo predicts the next likely word</li>
</ul>
<p><img src="https://i.imgur.com/ZNU7ol2.png" alt="" /></p>
<ul>
<li>ELMo also includes information of the next words in a sentence/batch. The forward and backward LSTM are concatenated together</li>
</ul>
<h2><a class="header" href="#openai-transformer" id="openai-transformer">OpenAI Transformer</a></h2>
<ul>
<li>It uses only the transformer decoder</li>
</ul>
<p><img src="https://i.imgur.com/7feXECY.png" alt="" /></p>
<ul>
<li>Predict the next word</li>
</ul>
<p><img src="https://i.imgur.com/ddFdSrJ.png" alt="" /></p>
<ul>
<li>Transfer Learn!</li>
</ul>
<p><img src="https://i.imgur.com/bUgDTmk.png" alt="" /></p>
<ul>
<li>More Transfer Learning. Often with tasks, the structure of inputs is important</li>
</ul>
<h2><a class="header" href="#bert-1" id="bert-1">BERT</a></h2>
<p><img src="https://i.imgur.com/MtcxjTT.png" alt="" /></p>
<ul>
<li>15% of words are masked. BERT tries to predict the masked word</li>
</ul>
<p><img src="https://i.imgur.com/NzChyLx.png" alt="" /></p>
<ul>
<li>An additional task was to predict if a given sentence was followed by another given sentence</li>
</ul>
<p><img src="https://i.imgur.com/UN4nCpS.png" alt="" /></p>
<ul>
<li>BERT transfer learning</li>
</ul>
<p><img src="https://i.imgur.com/4kqLRhK.png" alt="" /></p>
<ul>
<li>BERT can do word embeddings</li>
</ul>
<p><img src="https://i.imgur.com/qB1f52C.png" alt="" /></p>
<ul>
<li>F1 score decreases the deeper the network. Deeper should give better embeddings.</li>
</ul>
<h1><a class="header" href="#homeworks" id="homeworks">Homeworks</a></h1>
<h1><a class="header" href="#homework-2-rnn-and-lstm" id="homework-2-rnn-and-lstm">Homework 2 RNN and LSTM</a></h1>
<ul>
<li><a href="https://bcourses.berkeley.edu/courses/1487769/pages/assignment-2-description">Link</a></li>
<li><a href="https://briantliao.com/store/cs282-lectures-slides/lec09.pdf">RNN and LSTM lecture slides</a></li>
</ul>
<h2><a class="header" href="#setup" id="setup">Setup</a></h2>
<pre><code class="language-sh">conda install &quot;tensorflow&lt;2.0&quot;          # cpu version
conda install &quot;tensorflow-gpu&lt;2.0&quot;      # gpu version
# I think I will use PyTorch though

conda create -n cs182-assignment2  
conda activate cs182-assignment2
# to deactivate:  conda deactivate

pip3 install -r requirements.txt # @Brian changed to pip3
</code></pre>
<h3><a class="header" href="#gpus" id="gpus">GPUs</a></h3>
<p>GPUs are not required for this assignment, but will help to speed up training and processing time for questions 3-4.</p>
<h3><a class="header" href="#download-data" id="download-data">Download Data</a></h3>
<pre><code class="language-sh">cd deeplearning/datasets
./get_assignment2_data.sh
</code></pre>
<p>Now you can use Jupyter Notebook <code>jupyter serve</code>!</p>
<h2><a class="header" href="#q1-image-captioning-with-vanilla-rnns-30-points" id="q1-image-captioning-with-vanilla-rnns-30-points">Q1: Image Captioning with Vanilla RNNs (30 points)</a></h2>
<p><img src="https://i.imgur.com/dysQhPY.png" alt="" /></p>
<ul>
<li>RNN Equation</li>
</ul>
<p>b included before tanh:
<code>sum_together = dot_x + dot_h + b</code></p>
<h2><a class="header" href="#q2-image-captioning-with-lstms-30-points" id="q2-image-captioning-with-lstms-30-points">Q2: Image Captioning with LSTMs (30 points)</a></h2>
<ul>
<li>\( \odot \) is the elementwise product of vectors.</li>
<li><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTMs</a></li>
</ul>
<p><img src="https://i.imgur.com/5LFX2M2.png" alt="" /></p>
<ul>
<li>RNN</li>
</ul>
<p><img src="https://i.imgur.com/OHAy8uM.png" alt="" /></p>
<ul>
<li>LSTM</li>
</ul>
<p>LSTM backwards</p>
<h2><a class="header" href="#q3-network-visualization-saliency-maps-class-visualization-and-fooling-images-20-points" id="q3-network-visualization-saliency-maps-class-visualization-and-fooling-images-20-points">Q3: Network Visualization: Saliency maps, Class Visualization, and Fooling Images (20 points)</a></h2>
<h3><a class="header" href="#saliency" id="saliency">Saliency</a></h3>
<p><img src="https://i.imgur.com/agqsFrM.png" alt="" /></p>
<ul>
<li>Which pixel has the most effect on the input</li>
</ul>
<p><img src="https://i.imgur.com/a1XejED.png" alt="" /></p>
<ul>
<li>
<p>A <strong>saliency map</strong> tells us the degree to which each pixel in the image affects the classification score for that image. To compute it, we compute the gradient of the unnormalized score (?) corresponding to the correct class (which is a scalar) (why is this a scalar?) with respect to the pixels of the image.</p>
</li>
<li>
<p>If the image has shape (3, H, W) then this gradient will also have shape (3, H, W); for each pixel in the image, this gradient tells us the amount by which the classification score will change if the pixel changes by a small amount. </p>
</li>
<li>
<p>To compute the saliency map, we take the absolute value of this gradient, then take the maximum value over the 3 input channels; the final saliency map thus has shape (H, W) and all entries are nonnegative.</p>
</li>
<li>
<p>@Brian: gradient of what? Max of what?</p>
</li>
<li>
<p><a href="http://pytorch.org/docs/torch.html#torch.gather">Pytorch Gather</a></p>
</li>
<li>
<p><code>s.gather(1, y.view(-1, 1)).squeeze()</code></p>
<ul>
<li>turns out to be like a loss. Cross entropy <code>mean()</code></li>
</ul>
</li>
</ul>
<h4><a class="header" href="#fooling-network" id="fooling-network">Fooling Network</a></h4>
<ul>
<li><code>torch.Tensor.data</code> and <code>torch.Tensor.grad.data</code></li>
<li>do not update <code>torch.Tensor += torch.Tensor</code> when we are returning a copy, <code>torch.Tensor.copy()</code></li>
<li>gradient ascent <code>torch.Tensor += torch.Tensor</code> wild</li>
<li>we calculate our own loss????</li>
</ul>
<h2><a class="header" href="#q4-style-transfer-20-points" id="q4-style-transfer-20-points">Q4: Style Transfer (20 points)</a></h2>
<h2><a class="header" href="#debugging" id="debugging">Debugging</a></h2>
<p>List Conda Enviornments:</p>
<pre><code>conda env list
</code></pre>
<p>Trying without conda env.</p>
<pre><code>pip install scipy==1.1.0
</code></pre>
<h2><a class="header" href="#tips-and-notes" id="tips-and-notes">Tips and Notes</a></h2>
<h3><a class="header" href="#numpy" id="numpy">Numpy</a></h3>
<pre><code class="language-python">np.zeros_like( another numpy array )

A.shape # (3, 4)
B.shape # (4, 4)
np.dot(A, B).shape # (3, 4)

np.zeros((dim1, dim2, dim3))

 x_pad[n,:,y_padded:y_padded+HH,x_padded:x_padded+WW]

 x[0,0,0] # indexes a single value
 x[0:5, 1:4, 6:8] # slices a tensor

x[-1] == x[len(x) - 1] 

# let x in 10 elements, index 0 to 9
for i in range(x - 1, -1, -1):
    print(i)
# 9, 8, 7 ... 2, 1, 0

</code></pre>
<h3><a class="header" href="#calculus" id="calculus">Calculus</a></h3>
<h3><a class="header" href="#chain-rule" id="chain-rule">Chain Rule:</a></h3>
<p>$$ \frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \frac{\partial y}{\partial x} $$</p>
<p>Same for \( \frac{\partial L}{\partial b} \) and \( \frac{\partial L}{\partial W} \).</p>
<p>We have \( \frac{\partial L}{\partial y} \) and we can solve for \( \frac{\partial y}{\partial y} \) locally. This may need values cached from the forward pass.</p>
<h1><a class="header" href="#homework-3-natural-language-processing" id="homework-3-natural-language-processing">Homework 3 Natural Language Processing</a></h1>
<p><a href="https://drive.google.com/open?id=1TNhUy9ldZ5mv_GLNNmCBFnLfT3DXwntF">Add to Google Colab</a></p>
<p><img src="https://i.imgur.com/5Rz14EN.png" alt="" /></p>
<ul>
<li>Attention</li>
</ul>
<p><img src="https://i.imgur.com/imbD4vw.png" alt="" /></p>
<ul>
<li>MultiHead</li>
</ul>
<h2><a class="header" href="#transformer-1" id="transformer-1">Transformer</a></h2>
<p><code>transformer_attention.py</code>:</p>
<pre><code class="language-python">from typing import Optional, Callable, Tuple

import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.layers import Layer

from transformer_layers import WeightNormDense as Dense, LayerNorm, ApplyAttentionMask

class AttentionQKV(Model):
    &quot;&quot;&quot;
    Computes attention based on provided similarity metric.
    &quot;&quot;&quot;

    def __init__(self) -&gt; None:
        super().__init__()
        self.apply_mask = ApplyAttentionMask()

    def call(self, queries, keys, values, mask=None):
        &quot;&quot;&quot;Fast scaled dot product attention.

            :param queries: Tensor with shape [batch_size, heads (optional), n_queries, depth_k]
            :param keys:    Tensor with shape [batch_size, heads (optional), n_keyval, depth_k]
            :param values:  Tensor with shape [batch_size, heads (optional), n_keyval, depth_v]
            :param mask:    Tensor with shape [batch_size, n_queries, n_queries]

            :return: output: Tensor with shape [batch_size, heads (optional), n_queries, depth_v]
        &quot;&quot;&quot;
        ####################################  YOUR CODE HERE  ####################################
        # n_queries corresponds to the sequence length on the query side @Brian
        # n_keyval corresponds to the sequence length on the key side (and value, as they are one and the same)
        # depth_k is the size of the projection that the key / query comparison is performed on.
        # depth_v is the size of the projection of the value projection. In a setting with one head, it is usually the dimension (dim) of the Transformer.
        # heads corresponds to the number of heads the attention is performed on.
        # If you are unfamiliar with attention heads, read section 3.2.2 of the Attention is all you need paper

        # PART 1: Implement Attention QKV

        # Use queries, keys and values to compute the output of the QKV attention

        # As defined is the Attention is all you need paper: https://arxiv.org/pdf/1706.03762.pdf
        key_dim = tf.cast(tf.shape(keys)[-1], tf.float32)
        similarity = 1/tf.sqrt(key_dim) # Compute the similarity according to the QKV formula

        masked_similarity = self.apply_mask(similarity, mask=mask) # We give you the mask to apply so that it is correct, you do not need to modify this.
        weights = None # Turn the similarity into a normalized output
        output = None # Obtain the output
        print('hello world!')
        ####################################  END OF YOUR CODE  ##################################

        return output, weights


class MultiHeadProjection(Model):

    def __init__(self, n_heads) -&gt; None:
        &quot;&quot;&quot;Map the multi-headed attention across the map

        Arguments:
            similarity_metric {[type]} -- The metric that should be used for the similarity
            n_heads {int} -- The number of heads in the attention map

        &quot;&quot;&quot;

        super().__init__()
        self.attention_map = AttentionQKV()
        self.n_heads = n_heads

    def build(self, input_shape):
        for shape in input_shape:
            assert shape[-1] % self.n_heads == 0, 'Shape of feature input must be divisible by n_heads'

    def call(self, inputs, mask=None):
        &quot;&quot;&quot;Fast multi-head attention.

        :param queries: Tensor with shape [batch_size, n_queries, depth_k]
        :param keys:    Tensor with shape [batch_size, n_keyval, depth_k]
        :param values:  Tensor with shape [batch_size, n_keyval, depth_v]

        :return: output: Tensor with shape [batch_size, n_queries, depth_v]
        &quot;&quot;&quot;
        queries, keys, values = inputs

        # Split each of the projection into its heads, by adding a new dimension
        # You must implement _split_heads, and _combine_heads
        queries_split = self._split_heads(queries)
        keys_split = self._split_heads(keys)
        values_split = self._split_heads(values)

        # Apply the attention map
        attention_output_split, _ = self.attention_map(queries_split, keys_split, values_split, mask=mask)

        # Re-combine the heads together, and return the output.
        output = self._combine_heads(attention_output_split)
        return output

    def _split_heads(self, tensor):
        tensor.shape.assert_has_rank(3)
        ####################################  YOUR CODE HERE  ####################################
        # PART 2: Implement the Multi-head attention.
        # You are given a Tensor which is one of the projections (K, Q or V)
        # and you must &quot;split it&quot; in self.n_heads. This splitting should add a dimension to the tensor,
        # so that each head acts independently

        batch_size, tensorlen = tf.shape(tensor)[0], tf.shape(tensor)[1]
        feature_size = tensor.shape.as_list()[2]

        new_feature_size = None # Compute what the feature size per head is.
        # Reshape this projection tensor so that it has n_heads, each of new_feature_size
        tensor = None
        # Transpose the matrix so the outer-dimensions are the batch-size and the number of heads
        tensor = None
        return tensor
        ##########################################################################################

    def _combine_heads(self, tensor):
        tensor.shape.assert_has_rank(4)
        ####################################  YOUR CODE HERE  ####################################
        # PART 2: Implement the Multi-head attention.
        # You are given the output from all the heads, and you must combine them back into 1 rank-3 matrix

        # Transpose back compared to the split, so that the outer dimensions are batch_size and sequence_length again
        tensor = None
        batch_size, tensorlen = tf.shape(tensor)[0], tf.shape(tensor)[1]
        feature_size = tensor.shape.as_list()[-1]

        new_feature_size = None # What is the new feature size, if we combine all the heads
        tensor = None # Reshape the Tensor to remove the heads dimension and come back to a Rank-3 tensor
        return tensor
        ##########################################################################################

class MultiHeadAttention(Model):
    &quot;&quot;&quot;
    Fast multi-head attention. Based on the Attention is All You Need paper.

    https://arxiv.org/pdf/1706.03762.pdf
    &quot;&quot;&quot;

    def __init__(self, n_heads) -&gt; None:
        super().__init__()

        self.n_heads = n_heads
        self.attention_layer = MultiHeadProjection(n_heads)

    def build(self, input_shapes):
        query_antecedent_shape, memory_antecedent_shape = input_shapes
        self.qa_channels = query_antecedent_shape[-1]
        self.ma_channels = memory_antecedent_shape[-1]
        assert self.qa_channels % self.n_heads == 0 and self.ma_channels % self.n_heads == 0, \
            'Feature size must be divisible by n_heads'
        assert self.qa_channels == self.ma_channels, 'Cannot combine tensors with different shapes'

        self.query_layer = Dense(self.qa_channels, use_bias=False)
        self.key_layer = Dense(self.qa_channels, use_bias=False)
        self.value_layer = Dense(self.ma_channels, use_bias=False)

        self.output_layer = Dense(self.qa_channels, use_bias=False)


    def call(self, inputs, mask=None):
        &quot;&quot;&quot;Fast multi-head self attention.

            :param inputs: tuple of (query_antecedent, memory_antecedent)
                query_antecedent -&gt; tensor w/ shape [batch_size, n_queries, channels]
                memory_antecedent -&gt; tensor w/ shape [batch_size, n_keyval, channels]
        &quot;&quot;&quot;
        assert isinstance(inputs, tuple) or isinstance(inputs, list) and len(inputs) == 2, \
            'Must pass query and memory'
        query_antecedent, memory_antecedent = inputs
        q = self.query_layer(query_antecedent)
        k = self.key_layer(memory_antecedent)
        v = self.value_layer(memory_antecedent)

        attention_output = self.attention_layer((q, k, v), mask=mask)
        output = self.output_layer(attention_output)
        return output
</code></pre>
<h1><a class="header" href="#final-project-model-distillation-low-precision-neural-networks" id="final-project-model-distillation-low-precision-neural-networks">Final Project: Model Distillation Low Precision Neural Networks</a></h1>
<h1><a class="header" href="#xnor-net" id="xnor-net">XNOR-net</a></h1>
<p><a href="https://github.com/jiecaoyu/XNOR-Net-PyTorch">Repo Link</a></p>
<h2><a class="header" href="#alexnet" id="alexnet">AlexNet</a></h2>
<pre><code class="language-python">import torch
import os
import torch.nn as nn
import torch.utils.model_zoo as model_zoo
import torch.nn.functional as F

__all__ = ['AlexNet', 'alexnet']


# @Brian Binary Activation
class BinActive(torch.autograd.Function):
    '''
    Binarize the input activations and calculate the mean across channel dimension.
    '''
    def forward(self, input):
        # @Brian save input for backwards
        self.save_for_backward(input)
        size = input.size()
        # @Brian return sign of input in binary activation
        input = input.sign()
        return input

    def backward(self, grad_output):
        # @Brian throw away something?
        input, = self.saved_tensors
        grad_input = grad_output.clone()
        # @Brian if greater or equal than 1, set to 0, if less or equal to -1 set to 0?
        grad_input[input.ge(1)] = 0
        grad_input[input.le(-1)] = 0
        return grad_input

class BinConv2d(nn.Module): # change the name of BinConv2d
    def __init__(self, input_channels, output_channels,
            kernel_size=-1, stride=-1, padding=-1, groups=1, dropout=0,
            Linear=False):
        super(BinConv2d, self).__init__()
        self.layer_type = 'BinConv2d'
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        self.dropout_ratio = dropout

        # @Brian adds a dropout layer
        if dropout!=0:
            self.dropout = nn.Dropout(dropout)
        # @Brian what is linear
        self.Linear = Linear
        if not self.Linear:
            self.bn = nn.BatchNorm2d(input_channels, eps=1e-4, momentum=0.1, affine=True)
            # @Brian this layer does a convolution
            self.conv = nn.Conv2d(input_channels, output_channels,
                    kernel_size=kernel_size, stride=stride, padding=padding, groups=groups)
        else:
            self.bn = nn.BatchNorm1d(input_channels, eps=1e-4, momentum=0.1, affine=True)
            # @Brian this layer does a linear
            self.linear = nn.Linear(input_channels, output_channels)
        self.relu = nn.ReLU(inplace=True)
    
    def forward(self, x):
        x = self.bn(x)
        # @Brian Creates Binary Activate Class
        x = BinActive()(x)
        if self.dropout_ratio!=0:
            x = self.dropout(x)
        if not self.Linear:
            x = self.conv(x)
        else:
            x = self.linear(x)
        # @Brian ReLU at the end
        x = self.relu(x)
        return x

class AlexNet(nn.Module):

    def __init__(self, num_classes=1000):
        super(AlexNet, self).__init__()
        self.num_classes = num_classes
        self.features = nn.Sequential(
            # @Brian Convolution at the front
            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0),
            nn.BatchNorm2d(96, eps=1e-4, momentum=0.1, affine=True),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
            # @Brian Binary Convolutions
            BinConv2d(96, 256, kernel_size=5, stride=1, padding=2, groups=1),
            nn.MaxPool2d(kernel_size=3, stride=2),
            BinConv2d(256, 384, kernel_size=3, stride=1, padding=1),
            BinConv2d(384, 384, kernel_size=3, stride=1, padding=1, groups=1),
            BinConv2d(384, 256, kernel_size=3, stride=1, padding=1, groups=1),
            nn.MaxPool2d(kernel_size=3, stride=2),
        )
        self.classifier = nn.Sequential(
            # @Brian Binary Convolutions with Linear
            BinConv2d(256 * 6 * 6, 4096, Linear=True),
            BinConv2d(4096, 4096, dropout=0.5, Linear=True),
            nn.BatchNorm1d(4096, eps=1e-3, momentum=0.1, affine=True),
            nn.Dropout(),
            # @Brian Full Linear 
            nn.Linear(4096, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        # @Brian squishes it into one vector
        x = x.view(x.size(0), 256 * 6 * 6)
        x = self.classifier(x)
        return x


def alexnet(pretrained=False, **kwargs):
    r&quot;&quot;&quot;AlexNet model architecture from the
    `&quot;One weird trick...&quot; &lt;https://arxiv.org/abs/1404.5997&gt;`_ paper.

    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    &quot;&quot;&quot;
    model = AlexNet(**kwargs)
    if pretrained:
        model_path = 'model_list/alexnet.pth.tar'
        pretrained_model = torch.load(model_path)
        model.load_state_dict(pretrained_model['state_dict'])
    return model
</code></pre>
<h2><a class="header" href="#main-to-train-on-imagenet" id="main-to-train-on-imagenet">Main to train on ImageNet</a></h2>
<p><code>main.py</code>:</p>
<pre><code class="language-python">import argparse
import os
import shutil
import time

import torch
import torch.nn as nn
import torch.nn.parallel
import torch.backends.cudnn as cudnn
import torch.distributed as dist
import torch.optim
import torch.utils.data
import torch.utils.data.distributed
# import torchvision.transforms as transforms
# import torchvision.datasets as datasets
import model_list
import util

# set the seed
torch.manual_seed(1)
torch.cuda.manual_seed(1)

import sys
import gc #@Brian what is this

parser = argparse.ArgumentParser(description='PyTorch ImageNet Training')
parser.add_argument('--arch', '-a', metavar='ARCH', default='alexnet',
                    help='model architecture (default: alexnet)')
parser.add_argument('--data', metavar='DATA_PATH', default='./data/',
                    help='path to imagenet data (default: ./data/)')
parser.add_argument('--caffe-data',  default=False, action='store_true',
                    help='whether use caffe-data')
parser.add_argument('-j', '--workers', default=8, type=int, metavar='N',
                    help='number of data loading workers (default: 8)')
parser.add_argument('--epochs', default=100, type=int, metavar='N',
                    help='number of total epochs to run')
parser.add_argument('--start-epoch', default=0, type=int, metavar='N',
                    help='manual epoch number (useful on restarts)')
parser.add_argument('-b', '--batch-size', default=256, type=int,
                    metavar='N', help='mini-batch size (default: 256)')
parser.add_argument('--lr', '--learning-rate', default=0.001, type=float,
                    metavar='LR', help='initial learning rate')
parser.add_argument('--momentum', default=0.90, type=float, metavar='M',
                    help='momentum')
parser.add_argument('--weight-decay', '--wd', default=1e-5, type=float,
                    metavar='W', help='weight decay (default: 1e-5)')
parser.add_argument('--print-freq', '-p', default=10, type=int,
                    metavar='N', help='print frequency (default: 10)')
parser.add_argument('--resume', default='', type=str, metavar='PATH',
                    help='path to latest checkpoint (default: none)')
parser.add_argument('-e', '--evaluate', dest='evaluate', action='store_true',
                    help='evaluate model on validation set')
parser.add_argument('--pretrained', dest='pretrained', action='store_true',
                    default=False, help='use pre-trained model')
parser.add_argument('--world-size', default=1, type=int,
                    help='number of distributed processes')
parser.add_argument('--dist-url', default='tcp://224.66.41.62:23456', type=str,
                    help='url used to set up distributed training')
parser.add_argument('--dist-backend', default='gloo', type=str,
                    help='distributed backend')

best_prec1 = 0

# define global bin_op
bin_op = None

def main():
    global args, best_prec1
    args = parser.parse_args()

    # create model
    if args.arch=='alexnet':
        model = model_list.alexnet(pretrained=args.pretrained) # @Brian get AlexNet
        input_size = 227
    else:
        raise Exception('Model not supported yet')

    if args.arch.startswith('alexnet') or args.arch.startswith('vgg'):
        # @Brian only data parallel the features?
        model.features = torch.nn.DataParallel(model.features)
        model.cuda()
    else:
        model = torch.nn.DataParallel(model).cuda()

    # define loss function (criterion) and optimizer
    criterion = nn.CrossEntropyLoss().cuda()

    optimizer = torch.optim.Adam(model.parameters(), args.lr,
                                weight_decay=args.weight_decay)

    for m in model.modules():
        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
            c = float(m.weight.data[0].nelement()) # @Brian initial weights of layer
            m.weight.data = m.weight.data.normal_(0, 2.0/c)
        elif isinstance(m, nn.BatchNorm2d):
            m.weight.data = m.weight.data.zero_().add(1.0) # @Brian initial weight batch norm
            m.bias.data = m.bias.data.zero_()

    # optionally resume from a checkpoint
    if args.resume:
        if os.path.isfile(args.resume):
            print(&quot;=&gt; loading checkpoint '{}'&quot;.format(args.resume))
            checkpoint = torch.load(args.resume)
            args.start_epoch = checkpoint['epoch']
            best_prec1 = checkpoint['best_prec1']
            model.load_state_dict(checkpoint['state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer'])
            print(&quot;=&gt; loaded checkpoint '{}' (epoch {})&quot;
                  .format(args.resume, checkpoint['epoch']))
            del checkpoint
        else:
            print(&quot;=&gt; no checkpoint found at '{}'&quot;.format(args.resume))

    cudnn.benchmark = True

    # Data loading code

    if args.caffe_data:
        print('==&gt; Using Caffe Dataset')
        cwd = os.getcwd()
        sys.path.append(cwd+'/../')
        import datasets as datasets
        import datasets.transforms as transforms
        if not os.path.exists(args.data+'/imagenet_mean.binaryproto'):
            print(&quot;==&gt; Data directory&quot;+args.data+&quot;does not exits&quot;)
            print(&quot;==&gt; Please specify the correct data path by&quot;)
            print(&quot;==&gt;     --data &lt;DATA_PATH&gt;&quot;)
            return

        normalize = transforms.Normalize(
                meanfile=args.data+'/imagenet_mean.binaryproto')


        train_dataset = datasets.ImageFolder(
            args.data,
            transforms.Compose([
                transforms.RandomHorizontalFlip(),
                transforms.ToTensor(),
                normalize,
                transforms.RandomSizedCrop(input_size),
            ]),
            Train=True)

        train_sampler = None

        train_loader = torch.utils.data.DataLoader(
            train_dataset, batch_size=args.batch_size, shuffle=False,
            num_workers=args.workers, pin_memory=True, sampler=train_sampler)

        val_loader = torch.utils.data.DataLoader(
            datasets.ImageFolder(args.data, transforms.Compose([
                transforms.ToTensor(),
                normalize,
                transforms.CenterCrop(input_size),
            ]),
            Train=False),
            batch_size=args.batch_size, shuffle=False,
            num_workers=args.workers, pin_memory=True)
    else:
        print('==&gt; Using Pytorch Dataset')
        import torchvision
        import torchvision.transforms as transforms
        import torchvision.datasets as datasets
        traindir = os.path.join(args.data, 'train')
        valdir = os.path.join(args.data, 'val')
        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                std=[1./255., 1./255., 1./255.])

        torchvision.set_image_backend('accimage')

        train_dataset = datasets.ImageFolder(
                traindir,
                transforms.Compose([
                    transforms.Resize((256, 256)),
                    transforms.RandomCrop(input_size),
                    transforms.RandomHorizontalFlip(),
                    transforms.ToTensor(),
                    normalize,
                    ]))

        train_loader = torch.utils.data.DataLoader(
                train_dataset, batch_size=args.batch_size, shuffle=True,
                num_workers=args.workers, pin_memory=True)
        val_loader = torch.utils.data.DataLoader(
                datasets.ImageFolder(valdir, transforms.Compose([
                    transforms.Resize((256, 256)),
                    transforms.CenterCrop(input_size),
                    transforms.ToTensor(),
                    normalize,
                    ])),
                batch_size=args.batch_size, shuffle=False,
                num_workers=args.workers, pin_memory=True)

    print model

    # define the binarization operator
    global bin_op
    bin_op = util.BinOp(model) # @Brian Binarization @TODO look at this

    if args.evaluate:
        validate(val_loader, model, criterion)
        return

    for epoch in range(args.start_epoch, args.epochs):
        adjust_learning_rate(optimizer, epoch)

        # train for one epoch
        train(train_loader, model, criterion, optimizer, epoch)

        # evaluate on validation set
        prec1 = validate(val_loader, model, criterion)

        # remember best prec@1 and save checkpoint
        is_best = prec1 &gt; best_prec1
        best_prec1 = max(prec1, best_prec1)
        save_checkpoint({
            'epoch': epoch + 1,
            'arch': args.arch,
            'state_dict': model.state_dict(),
            'best_prec1': best_prec1,
            'optimizer' : optimizer.state_dict(),
        }, is_best)


def train(train_loader, model, criterion, optimizer, epoch):
    batch_time = AverageMeter()
    data_time = AverageMeter()
    losses = AverageMeter()
    top1 = AverageMeter()
    top5 = AverageMeter()

    # switch to train mode
    model.train()

    end = time.time()
    for i, (input, target) in enumerate(train_loader):
        # measure data loading time
        data_time.update(time.time() - end)

        target = target.cuda(async=True)
        input_var = torch.autograd.Variable(input)
        target_var = torch.autograd.Variable(target)

        # process the weights including binarization
        bin_op.binarization() # @Brian global, binarization
        
        # compute output
        output = model(input_var)
        loss = criterion(output, target_var)

        # measure accuracy and record loss
        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))
        losses.update(loss.data.item(), input.size(0)) # @Brian the type is AverageMeter what is a AverageMeter
        top1.update(prec1[0], input.size(0))
        top5.update(prec5[0], input.size(0))

        # compute gradient and do SGD step
        optimizer.zero_grad()
        loss.backward() # @Brian computes full precision gradient

        # restore weights
        bin_op.restore() # @Brian does it requantize it?
        bin_op.updateBinaryGradWeight()

        optimizer.step()

        # measure elapsed time
        batch_time.update(time.time() - end)
        end = time.time()

        if i % args.print_freq == 0:
            print('Epoch: [{0}][{1}/{2}]\t'
                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\t'
                  'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\t'
                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(
                   epoch, i, len(train_loader), batch_time=batch_time,
                   data_time=data_time, loss=losses, top1=top1, top5=top5))
        gc.collect()


def validate(val_loader, model, criterion):
    batch_time = AverageMeter()
    losses = AverageMeter()
    top1 = AverageMeter()
    top5 = AverageMeter()

    # switch to evaluate mode
    model.eval()

    end = time.time()
    bin_op.binarization() # @Brian what does this do?
    for i, (input, target) in enumerate(val_loader):
        target = target.cuda(async=True)
        with torch.no_grad():
            input_var = torch.autograd.Variable(input)
            target_var = torch.autograd.Variable(target)

        # compute output
        output = model(input_var)
        loss = criterion(output, target_var)

        # measure accuracy and record loss
        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))
        losses.update(loss.data.item(), input.size(0))
        top1.update(prec1[0], input.size(0))
        top5.update(prec5[0], input.size(0))

        # measure elapsed time
        batch_time.update(time.time() - end)
        end = time.time()

        if i % args.print_freq == 0:
            print('Test: [{0}/{1}]\t'
                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
                  'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\t'
                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(
                   i, len(val_loader), batch_time=batch_time, loss=losses,
                   top1=top1, top5=top5))
    bin_op.restore()

    print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'
          .format(top1=top1, top5=top5))

    return top1.avg


def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):
    torch.save(state, filename)
    if is_best:
        shutil.copyfile(filename, 'model_best.pth.tar')


class AverageMeter(object):
    &quot;&quot;&quot;Computes and stores the average and current value&quot;&quot;&quot;
    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count # @Brian basically gets a validation


def adjust_learning_rate(optimizer, epoch):
    &quot;&quot;&quot;Sets the learning rate to the initial LR decayed by 10 every 30 epochs&quot;&quot;&quot;
    lr = args.lr * (0.1 ** (epoch // 30))
    print 'Learning rate:', lr
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr


def accuracy(output, target, topk=(1,)):
    &quot;&quot;&quot;Computes the precision@k for the specified values of k&quot;&quot;&quot;
    maxk = max(topk)
    batch_size = target.size(0)

    _, pred = output.topk(maxk, 1, True, True)
    pred = pred.t()
    correct = pred.eq(target.view(1, -1).expand_as(pred))

    res = []
    for k in topk:
        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)
        res.append(correct_k.mul_(100.0 / batch_size))
    return res


if __name__ == '__main__':
    main()
</code></pre>
<h2><a class="header" href="#utils" id="utils">Utils</a></h2>
<pre><code class="language-python">import torch.nn as nn
import numpy

class BinOp():
    def __init__(self, model): # @Brian takes in a model
        # count the number of Conv2d and Linear
        count_targets = 0
        for m in model.modules():
            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
                count_targets = count_targets + 1

        start_range = 1
        end_range = count_targets-2
        self.bin_range = numpy.linspace(start_range,
                end_range, end_range-start_range+1)\
                        .astype('int').tolist()
        self.num_of_params = len(self.bin_range)
        self.saved_params = []
        self.target_params = []
        self.target_modules = []
        index = -1
        for m in model.modules():
            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
                index = index + 1
                if index in self.bin_range:
                    tmp = m.weight.data.clone()
                    self.saved_params.append(tmp)
                    self.target_modules.append(m.weight)

    def binarization(self):
        self.meancenterConvParams() # @Brian what is binarization
        self.clampConvParams()
        self.save_params()
        self.binarizeConvParams()

    def meancenterConvParams(self):
        for index in range(self.num_of_params):
            s = self.target_modules[index].data.size()
            negMean = self.target_modules[index].data.mean(1, keepdim=True).\
                    mul(-1).expand_as(self.target_modules[index].data) # @Brian what is mul(-1) and expand_as
            self.target_modules[index].data = self.target_modules[index].data.add(negMean)

    def clampConvParams(self):
        for index in range(self.num_of_params):
            self.target_modules[index].data = \
                    self.target_modules[index].data.clamp(-1.0, 1.0) # @Brian clamp

    def save_params(self):
        for index in range(self.num_of_params):
            self.saved_params[index].copy_(self.target_modules[index].data) # @Brian then saves the params

    def binarizeConvParams(self):
        for index in range(self.num_of_params):
            n = self.target_modules[index].data[0].nelement()
            s = self.target_modules[index].data.size()
            if len(s) == 4:
                m = self.target_modules[index].data.norm(1, 3, keepdim=True)\
                        .sum(2, keepdim=True).sum(1, keepdim=True).div(n) # @Brian normalizes it
            elif len(s) == 2:
                m = self.target_modules[index].data.norm(1, 1, keepdim=True).div(n) # @Brian ?
            self.target_modules[index].data = \
                    self.target_modules[index].data.sign().mul(m.expand(s)) # @Brian ?

    def restore(self):
        for index in range(self.num_of_params):
            self.target_modules[index].data.copy_(self.saved_params[index]) # @Brian resets from the saved params

    def updateBinaryGradWeight(self):
        for index in range(self.num_of_params):
            weight = self.target_modules[index].data
            n = weight[0].nelement()
            s = weight.size()
            if len(s) == 4:
                m = weight.norm(1, 3, keepdim=True)\
                        .sum(2, keepdim=True).sum(1, keepdim=True).div(n).expand(s) # @Brian what is this?
            elif len(s) == 2:
                m = weight.norm(1, 1, keepdim=True).div(n).expand(s)
            m[weight.lt(-1.0)] = 0 
            m[weight.gt(1.0)] = 0
            m = m.mul(self.target_modules[index].grad.data)
            m_add = weight.sign().mul(self.target_modules[index].grad.data)
            if len(s) == 4:
                m_add = m_add.sum(3, keepdim=True)\
                        .sum(2, keepdim=True).sum(1, keepdim=True).div(n).expand(s)
            elif len(s) == 2:
                m_add = m_add.sum(1, keepdim=True).div(n).expand(s)
            m_add = m_add.mul(weight.sign())
            self.target_modules[index].grad.data = m.add(m_add).mul(1.0-1.0/s[1]).mul(n)
            self.target_modules[index].grad.data = self.target_modules[index].grad.data.mul(1e+9) # @Brian this does not make sense 

# @Brian what are expand, sum, norm, div, expand_as used for?
</code></pre>
<h1><a class="header" href="#knowledge-distillation" id="knowledge-distillation">Knowledge Distillation</a></h1>
<h1><a class="header" href="#dorefa-net" id="dorefa-net">DoReFa-Net</a></h1>
<h2><a class="header" href="#abstract" id="abstract">Abstract</a></h2>
<ul>
<li>low bitwidth weights and activation
<ul>
<li>1 bit weights, 2 bit activations</li>
</ul>
</li>
<li>low bitwidth gradients</li>
</ul>
<p><img src="https://i.imgur.com/isbpqr4.png" alt="" /></p>
<ul>
<li>bitcount when both weight and input activations are binarized (XNOR-net 2016).</li>
<li>1 bit convolution kernels?</li>
<li>1 bit weights, 1 bit activations and 2 bit gradients do well</li>
</ul>
<p><img src="https://i.imgur.com/isbpqr4.png" alt="" /></p>
<ul>
<li>x and y are arrays of bits, the floating point can be calculated by those sums.</li>
</ul>
<p><img src="https://i.imgur.com/UKMw0Vt.png" alt="" /></p>
<ul>
<li>dot product</li>
</ul>
<p><img src="https://i.imgur.com/aagv0oS.png" alt="" /></p>
<ul>
<li>Straight through estimator through sampling and quantization</li>
</ul>
<p><img src="https://i.imgur.com/77wxwFU.png" alt="" /></p>
<ul>
<li>k bit representation with tanh to constraint to [-1, 1]</li>
</ul>
<p><img src="https://i.imgur.com/OuygHvF.png" alt="" /></p>
<ul>
<li>.975 with all accurcay</li>
<li>.934 with 1W, 1A, 1G</li>
<li>.971 with 1W, 1A, 32G</li>
<li>W,A =(1,2) and G &gt;= 4 is best</li>
</ul>
<p>Quantizing the first and last layers leads to significant degradation </p>
<p>FPGAs with B-bit arithmetic is good at low bitwidth convolutions</p>
<h1><a class="header" href="#apprentice-knowledge-distillation-with-low-precision-networks" id="apprentice-knowledge-distillation-with-low-precision-networks">Apprentice: Knowledge Distillation with Low-Precision Networks</a></h1>
<p><a href="https://arxiv.org/pdf/1711.05852.pdf">Paper</a></p>
<h2><a class="header" href="#abstract-1" id="abstract-1">Abstract</a></h2>
<p>Low-precision numerics and model compression using knowledge distillation are popular techniques to lower both the compute requirements and memory footprint of these deployed model</p>
<h1><a class="header" href="#exam-practice" id="exam-practice">Exam Practice</a></h1>
<h1><a class="header" href="#midterm-1" id="midterm-1">Midterm 1</a></h1>
<h1><a class="header" href="#midterm-1-spring-2019" id="midterm-1-spring-2019">Midterm 1 Spring 2019</a></h1>
<p><img src="https://i.imgur.com/0WqLprF.png" alt="" /></p>
<ol>
<li>
<p>a.</p>
<ul>
<li><strong>Ans:</strong> AlexNet is deeper. AlexNet is split on GPUs.</li>
<li><strong>Corrections:</strong> AlexNet uses ReLU, LeNet uses Sigmoid</li>
<li><strong>Review:</strong> <code>AlexNet and LeNet</code></li>
</ul>
<p>b. </p>
<ul>
<li><strong>Ans:</strong> Multi-task learning is you replace the end layers of a network including the classifier, and can connect it to other tasks like question and answering. This makes it more robust to learn representations in images.</li>
<li><strong>Corrections:</strong> Layers are shared in a network (shared representation). The network is applied to multiple tasks. Benefits are: the network has extra information to capture the essence of a task, and avoids overfitting by learning more robust features.</li>
<li><strong>Review:</strong> <code>multi-task learning</code></li>
</ul>
<p>c.</p>
<ul>
<li><strong>Ans:</strong> Generative Models try to generate the joint distribution \( P(B,A) \). Discriminative models try to model only the decision boundary.</li>
<li><strong>Corrections:</strong> generate is &quot;sample&quot; data from distribution. Can't say it models \( P(X, Y) \). Discriminative doesn't need assumptions aboutd data, can model more complex distributions.</li>
<li><strong>Review:</strong> <code>Generative vs Discriminative Models</code></li>
</ul>
</li>
</ol>
<p><img src="https://i.imgur.com/mZLgVpH.png" alt="" /></p>
<ol>
<li>
<p>d.</p>
<ul>
<li><strong>Ans:</strong> Cross Entropy is used for discrete values</li>
<li><strong>Corrections:</strong> Cross entropy loss (which is log loss in the binary case)</li>
<li><strong>Review:</strong> <code>Squared Error Loss and Cross Entropy Loss. Review Loss vs. Risk</code></li>
</ul>
<p>e.</p>
<ul>
<li><strong>Ans:</strong> (rip)</li>
<li><strong>Corrections:</strong> Loss gradients push the boundary to move points outside the margin. Close points have very large gradients.</li>
<li><strong>Review:</strong> <code>Logistic Regression, its training algorithm</code></li>
</ul>
</li>
</ol>
<p><img src="https://i.imgur.com/ZoKIsmv.png" alt="" /></p>
<ol>
<li>
<p>f.</p>
<ul>
<li><strong>Ans:</strong> Bias is how accurate the model is on learning the relationship between x and y. Variance is how accurate the model is to data it has not seen. Deep neural networks have high variance and low bias.</li>
<li><strong>Corrections:</strong> Complex models can fit data better (low bias) but are more sensitive to data (high variance) Regularization reduces variance at expense for bias. Deep neural networks are low-bias, high variance</li>
<li><strong>Review</strong>: <code>bias variance tradeoff, regularization, deep networks are low bias high variance</code></li>
</ul>
<p>g.</p>
<ul>
<li><strong>Ans:</strong> When each variable is independent. Logistic regression is more accurate when this is not the case and there is more data.</li>
<li><strong>Corrections:</strong> Naive Bayes assumes feature values are conditionally independent based on class label. Logistic regression doesn't make that assumption and is more accurate when that is not the case.</li>
<li><strong>Review</strong>: <code>Naive Bayes, Conditional Independence on class label, Logistic Regression.</code></li>
</ul>
</li>
</ol>
<p><img src="https://i.imgur.com/h77LjNU.png" alt="" /></p>
<ol>
<li>
<p>h.</p>
<ul>
<li><strong>Review:</strong> Gradients vanishes at local minima, maxima and saddle points</li>
<li><strong>Corrections:</strong></li>
<li><strong>Review:</strong> <code>Vanishing Gradients</code></li>
</ul>
<p>i.</p>
<ul>
<li><strong>Review:</strong> \( 128 \times 64 \times 64 \)</li>
<li><strong>Corrections:</strong></li>
<li><strong>Review:</strong> <code>Convolution Equation</code></li>
</ul>
<p>j.</p>
<ul>
<li><strong>Review:</strong> Split dataset into <code>k+1</code> subsets taking one out for testing. On round <code>i</code>, hold <code>i</code> for validation and and the rest of the <code>k</code> for training. Repeat with different holdout sets.</li>
<li><strong>Corrections:</strong> Check on fold <code>i</code> (validation data)</li>
<li><strong>Review:</strong> <code>Cross Validation</code></li>
</ul>
</li>
</ol>
<h1><a class="header" href="#midterm-1-practice-1-spring-2018" id="midterm-1-practice-1-spring-2018">Midterm 1 Practice 1 Spring 2018</a></h1>
<p><img src="https://i.imgur.com/v1k6bSW.png" alt="" /></p>
<ol>
<li>
<p>a. Objects can be seen from different perspectives. Lighting can be different. Objects can be blocked; object partially visible.</p>
<ul>
<li><strong>Review:</strong> <code>Lecture 1.</code></li>
</ul>
<p>b. When doing transfer learning, the network has already extracted high level like edges, mid level like shapes and lines, and low level like faces. These generalize and don't have to be updated too much in being fine tuned. <strong>The already trained low-dimensional features capture the essence of the data, can have faster fine-tuning.</strong></p>
<ul>
<li><strong>Review:</strong> <code>Lecture 1, Transfer Learning.</code></li>
</ul>
<p>c. Expected Risk is <strong>expectation</strong> over all datasets of model loss (prediction – actual result). Empirical risk is difference over a fixed dataset sample. Machine Learning minimizes empirical risk.</p>
<ul>
<li><strong>Review:</strong> <code>Expected Risk, Empirical Risk, Loss vs Risk.</code></li>
</ul>
</li>
</ol>
<p><img src="https://i.imgur.com/Z5g6JuI.png" alt="" /></p>
<ol>
<li>
<p>d. L2 is designed for loss of continuous value functions. Logistic regression is for binary classification (0 or 1). We use binary cross-entropy loss. (It encourages outputs to be 0 or 1?) The loss becomes a probability?</p>
<ul>
<li><strong>Review:</strong> <code>L2 Loss vs Cross-Entropy Loss</code></li>
<li>Why does it work for binary/discrete classification vs L2 for continuous? What is the relationship of it to probability?</li>
</ul>
<p>e. Newton's second-order method converge to local minima and saddle points. Where second derivative gradients (is this correct?) are zero</p>
<ul>
<li><strong>Review:</strong> <code>Netwon Second-Order Methods</code></li>
</ul>
<p>f. Using a max-margin classifier is the most robust (decreases overfitting) classifier to unseen data. It maximizes distance of the nearest points to the margin/decision boundary. Diagram shows decision boundary with max distance to nearest points</p>
<ul>
<li><strong>Review:</strong> <code>SVM, Max-Margin, Hinge Loss</code></li>
</ul>
</li>
</ol>
<p><img src="https://i.imgur.com/ray5suc.png" alt="" /></p>
<ol>
<li>g. for each data point \( (x_i, y_i) \), we have \( f_{y_i}(x_i) - f_j(x_i) \) (the difference) for all classes \( j \neq y \). The original SVM loss is \( max(0, 1 - yw^Tx) \). Max margins for all classes not \( j \) is \( max(0, 1 - f_{y_i}(x_i) + f_j(x_i) \). Loss is sum/averaged. OvA classifer? What the heck is this?
<ul>
<li><strong>Review:</strong> <code>SVM</code></li>
</ul>
</li>
</ol>
<p><img src="https://i.imgur.com/yCRut4F.png" alt="" /></p>
<ol>
<li>
<p>h. Multiclass logistic regression can learn the multiclass naive Bayes</p>
<ul>
<li><strong>Review:</strong> <code>Multiclass Logistic Regression, Multiclass Naive Bayes, their relationship</code></li>
</ul>
<p>i. Gradient decreases proportionally to accumulating value at \( \approx \frac{1}{\sqrt{t}}\). This is because the denominator of the update contains sum of squares of past gradients.</p>
</li>
</ol>
<p><img src="https://i.imgur.com/0rGE4yp.png" alt="" /></p>
<ol>
<li>
<p>j. </p>
<ul>
<li><strong>Ans:</strong> Local optima were usually saddle points. SGD performs well because it does not just follow the gradient, so it is likely to fall off a saddle point.</li>
<li><strong>Corrections:</strong> </li>
<li><strong>Review:</strong> <code>loss landscape and convexity, SGD</code> </li>
</ul>
<p>k.</p>
<ul>
<li><strong>Ans:</strong> Depends on the stride and padding. if stride 1 padding 0,  \( 194 \times 194 \times 1\)</li>
<li><strong>Corrections:</strong> </li>
<li><strong>Review:</strong> <code>Convolution reshape equation</code> </li>
</ul>
<p>l.</p>
<ul>
<li><strong>Ans:</strong> They extract features in images. These features can be used for more robust representations of images.</li>
<li><strong>Corrections:</strong> </li>
<li><strong>Review:</strong> <code>Convolutions, Feature Detection</code> </li>
</ul>
<p>m. </p>
<ul>
<li><strong>Ans:</strong> Expectation of dropout is <code>output * p</code>. We can use the expectation of dropout in inference then, but also just use the <code>output</code> by doing dropout at training time (with a mask where each index has probability <code>p</code> being 1 (retained) and dividing by <code>p</code>.</li>
<li><strong>Corrections:</strong> </li>
<li><strong>Review:</strong> <code>Dropout</code></li>
</ul>
<p>n. </p>
<ul>
<li><strong>Ans:</strong> Prediction Averaging reduces variance. Each model is robust to different relationships (learn different things). Parameter averaging canceling out the relationships each model learns. Snapshot parameter ensembling works because the relationships the model was learning are close to the same, while still reducing variance.</li>
<li><strong>Corrections:</strong> </li>
<li><strong>Review:</strong> <code>Prediction Averaging, Parameter Averaging, Snapshot Ensembling</code></li>
</ul>
<p>o. </p>
<ul>
<li><strong>Ans:</strong> x -&gt; [] (array with h wraps into block) -&gt; y</li>
<li><strong>Corrections:</strong> </li>
<li><strong>Review:</strong> <code>RNN</code></li>
</ul>
<p>p. </p>
<ul>
<li><strong>Ans:</strong> <code>y = A_yh * tanh(A_hx * x + A_hh * h)</code></li>
<li><strong>Corrections:</strong> </li>
<li><strong>Review:</strong> <code>RNN equation</code></li>
</ul>
<p>q. </p>
<ul>
<li><strong>Ans:</strong> Remember and Forget?</li>
<li><strong>Corrections:</strong> </li>
<li><strong>Review:</strong> <code>LSTM equation and interpretation</code></li>
</ul>
</li>
</ol>
<h1><a class="header" href="#midterm-1-cheat-sheet" id="midterm-1-cheat-sheet">Midterm 1 Cheat Sheet</a></h1>
<p><img src="https://i.imgur.com/5fKIV3n.jpg" alt="" /></p>
<p><img src="https://i.imgur.com/Ch56t9P.jpg" alt="" /></p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        

        
        
        
        <script type="text/javascript">
            window.playpen_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>
        
        

    </body>
</html>
